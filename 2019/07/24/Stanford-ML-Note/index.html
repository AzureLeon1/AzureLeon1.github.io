<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/icon-180.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/icon-32.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/icon-16.png?v=5.1.4">


  <link rel="mask-icon" href="/images/icon.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="机器学习,吴恩达,">










<meta name="description" content="讲师：Andrew Ng     Date Content     2019-3-16 [L1 : L4]   2019-3-20 [L6 : L12]   2019-4-7 [L14 : L27]   2019-6-6 [L28 : L35]   2019-7-24 [L46 : L51]    第1章 绪论：初识机器学习L2 什么是机器学习任务T 经验E 性能度量P L3 监督学习Super">
<meta name="keywords" content="机器学习,吴恩达">
<meta property="og:type" content="article">
<meta property="og:title" content="Stanford ML Note">
<meta property="og:url" content="http://www.leonwang.top/2019/07/24/Stanford-ML-Note/index.html">
<meta property="og:site_name" content="Leon Wang">
<meta property="og:description" content="讲师：Andrew Ng     Date Content     2019-3-16 [L1 : L4]   2019-3-20 [L6 : L12]   2019-4-7 [L14 : L27]   2019-6-6 [L28 : L35]   2019-7-24 [L46 : L51]    第1章 绪论：初识机器学习L2 什么是机器学习任务T 经验E 性能度量P L3 监督学习Super">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://img.cdn.leonwang.top/006tKfTcgy1g16s7db4zwj31uh0u0dk0.jpg">
<meta property="og:image" content="http://img.cdn.leonwang.top/006tKfTcgy1g16s8xvhhgj314b0u0qcy.jpg">
<meta property="og:image" content="http://img.cdn.leonwang.top/006tKfTcgy1g191alaixrj30rk0cw0t9.jpg">
<meta property="og:image" content="http://img.cdn.leonwang.top/006tKfTcgy1g191nvyu1xj30mu0fodg7.jpg">
<meta property="og:image" content="http://img.cdn.leonwang.top/006tKfTcgy1g191yug4nuj30tu0jc409.jpg">
<meta property="og:image" content="http://img.cdn.leonwang.top/006tKfTcgy1g1955i32adj30ry088dg8.jpg">
<meta property="og:image" content="http://img.cdn.leonwang.top/006tKfTcgy1g1955zzel9j30n607qt8x.jpg">
<meta property="og:image" content="http://img.cdn.leonwang.top/006tKfTcgy1g1952enn4fj30sw082wf4.jpg">
<meta property="og:image" content="http://img.cdn.leonwang.top/006tNc79gy1g1thtzm8tkj30qc0fcq6c.jpg">
<meta property="og:image" content="http://img.cdn.leonwang.top/006tNc79gy1g1thtdplhrj30wa0hsn77.jpg">
<meta property="og:image" content="http://img.cdn.leonwang.top/006tNc79gy1g1thwi0rmnj30sg0ean19.jpg">
<meta property="og:image" content="http://img.cdn.leonwang.top/006tNc79gy1g1ti44z0j9j30j803uwgk.jpg">
<meta property="og:image" content="http://img.cdn.leonwang.top/006tNc79gy1g3qn70elnqj30p40cq7a0.jpg">
<meta property="og:image" content="http://img.cdn.leonwang.top/006tNc79gy1g3qn9fvsp2j30nk0bm0z1.jpg">
<meta property="og:image" content="http://img.cdn.leonwang.top/006tNc79gy1g3qneduvi7j30oy0dmai4.jpg">
<meta property="og:image" content="http://img.cdn.leonwang.top/image-20190606010011784.png">
<meta property="og:image" content="http://img.cdn.leonwang.top/image-20190606010355412.png">
<meta property="og:image" content="http://img.cdn.leonwang.top/006tNc79gy1g3qnrbbzcqj30j20d8adt.jpg">
<meta property="og:image" content="http://img.cdn.leonwang.top/006tNc79gy1g3qnrtw4e2j30cq09cdhj.jpg">
<meta property="og:image" content="http://img.cdn.leonwang.top/006tNc79gy1g3qnu4qgjpj30m205itag.jpg">
<meta property="og:image" content="http://img.cdn.leonwang.top/image-20190606011858017.png">
<meta property="og:image" content="http://img.cdn.leonwang.top/image-20190606012301141.png">
<meta property="og:image" content="http://img.cdn.leonwang.top/20190724150741.png">
<meta property="og:image" content="http://img.cdn.leonwang.top/20190724153422.png">
<meta property="og:image" content="http://img.cdn.leonwang.top/20190724155356.png">
<meta property="og:image" content="http://img.cdn.leonwang.top/20190724155414.png">
<meta property="og:image" content="http://img.cdn.leonwang.top/20190725165626.png">
<meta property="og:image" content="http://img.cdn.leonwang.top/20190725170109.png">
<meta property="og:image" content="http://img.cdn.leonwang.top/20190725205823.png">
<meta property="og:image" content="http://img.cdn.leonwang.top/20190725214908.png">
<meta property="og:image" content="http://img.cdn.leonwang.top/20190725214854.png">
<meta property="og:image" content="http://img.cdn.leonwang.top/20190725215818.png">
<meta property="og:image" content="http://img.cdn.leonwang.top/20190809221747.png">
<meta property="og:updated_time" content="2019-08-11T06:25:24.133Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Stanford ML Note">
<meta name="twitter:description" content="讲师：Andrew Ng     Date Content     2019-3-16 [L1 : L4]   2019-3-20 [L6 : L12]   2019-4-7 [L14 : L27]   2019-6-6 [L28 : L35]   2019-7-24 [L46 : L51]    第1章 绪论：初识机器学习L2 什么是机器学习任务T 经验E 性能度量P L3 监督学习Super">
<meta name="twitter:image" content="http://img.cdn.leonwang.top/006tKfTcgy1g16s7db4zwj31uh0u0dk0.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.leonwang.top/2019/07/24/Stanford-ML-Note/">





  <title>Stanford ML Note | Leon Wang</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?2e891af0fec421b141ef1add26813124";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>





  <script src="https://cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Leon Wang</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.leonwang.top/2019/07/24/Stanford-ML-Note/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Leon Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Leon Wang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Stanford ML Note</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-24T12:52:02+08:00">
                2019-07-24
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2019-08-11T14:25:24+08:00">
                2019-08-11
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <blockquote>
<p>讲师：Andrew Ng</p>
</blockquote>
<table>
<thead>
<tr>
<th>Date</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr>
<td>2019-3-16</td>
<td>[L1 : L4]</td>
</tr>
<tr>
<td>2019-3-20</td>
<td>[L6 : L12]</td>
</tr>
<tr>
<td>2019-4-7</td>
<td>[L14 : L27]</td>
</tr>
<tr>
<td>2019-6-6</td>
<td>[L28 : L35]</td>
</tr>
<tr>
<td>2019-7-24</td>
<td>[L46 : L51]</td>
</tr>
</tbody>
</table>
<h1 id="第1章-绪论：初识机器学习"><a href="#第1章-绪论：初识机器学习" class="headerlink" title="第1章 绪论：初识机器学习"></a>第1章 绪论：初识机器学习</h1><h2 id="L2-什么是机器学习"><a href="#L2-什么是机器学习" class="headerlink" title="L2 什么是机器学习"></a>L2 什么是机器学习</h2><p>任务T 经验E 性能度量P</p>
<h2 id="L3-监督学习"><a href="#L3-监督学习" class="headerlink" title="L3 监督学习"></a>L3 监督学习</h2><p>Supervised Learning: “right answers” given</p>
<ul>
<li><p>回归(regression)：预测值是连续的</p>
</li>
<li><p>分类(classification)：预测值是离散的</p>
<ul>
<li><p>一个特征：</p>
<p>  可以把样本点投射到特征轴上</p>
<p>  <img src="http://img.cdn.leonwang.top/006tKfTcgy1g16s7db4zwj31uh0u0dk0.jpg" alt=""></p>
</li>
<li><p>两个特征：</p>
<p>  用线将样本点分割成不同的区域</p>
<p>  <img src="http://img.cdn.leonwang.top/006tKfTcgy1g16s8xvhhgj314b0u0qcy.jpg" alt=""></p>
</li>
<li><p>更多特征：</p>
<p>  ……    </p>
</li>
</ul>
</li>
</ul>
<h2 id="L4-无监督学习"><a href="#L4-无监督学习" class="headerlink" title="L4 无监督学习"></a>L4 无监督学习</h2><p>Unsupervised Learning</p>
<p>聚类算法(clustering algorithms)</p>
<p>例子：给新闻自动划分主题、根据基因表达程度把人划分为不同群体、判断哪些计算机协同工作而提高数据中心的效率、社交软件分析自动划分社交圈子、根据客户数据划分客户圈、天文分析星系形成理论、处理分析音频</p>
<h1 id="第2章-单变量线性回归"><a href="#第2章-单变量线性回归" class="headerlink" title="第2章 单变量线性回归"></a>第2章 单变量线性回归</h1><h2 id="L6-模型描述"><a href="#L6-模型描述" class="headerlink" title="L6 模型描述"></a>L6 模型描述</h2><p>(x^(i), y^(i))表示第i个训练样本</p>
<p>训练出的模型是一个函数，称为hypothesis函数</p>
<p>简单的例子：单变量线性回归</p>
<p><img src="http://img.cdn.leonwang.top/006tKfTcgy1g191alaixrj30rk0cw0t9.jpg" alt=""></p>
<h2 id="L7-代价函数"><a href="#L7-代价函数" class="headerlink" title="L7 代价函数"></a>L7 代价函数</h2><p>“#”是训练样本个数的缩写</p>
<p>回归问题常用的代价函数：平方误差代价函数</p>
<p>分母中的2是为了便于求导</p>
<p><img src="http://img.cdn.leonwang.top/006tKfTcgy1g191nvyu1xj30mu0fodg7.jpg" alt=""></p>
<h2 id="L8-代价函数（一）"><a href="#L8-代价函数（一）" class="headerlink" title="L8 代价函数（一）"></a>L8 代价函数（一）</h2><p>简化上节课的hypothesis（使其只有一个一次项系数参数），然后借助图像，理解hypothesis与代价函数的关系：</p>
<p><img src="http://img.cdn.leonwang.top/006tKfTcgy1g191yug4nuj30tu0jc409.jpg" alt=""></p>
<h2 id="L9-代价函数（二）"><a href="#L9-代价函数（二）" class="headerlink" title="L9 代价函数（二）"></a>L9 代价函数（二）</h2><p>取消上节课中的简化操作，讨论两个参数的hypothesis。其代价函数图像不再是二维的曲线，而是一个三维的曲面，但可以用二维的等高线图来表示。</p>
<h2 id="L10-梯度下降"><a href="#L10-梯度下降" class="headerlink" title="L10 梯度下降"></a>L10 梯度下降</h2><p>梯度下降算法：求最小化代价函数，不只可以用在线性回归问题</p>
<p>得到的是局部最优</p>
<p>过程：</p>
<p><img src="http://img.cdn.leonwang.top/006tKfTcgy1g1955i32adj30ry088dg8.jpg" alt=""></p>
<p>定义：</p>
<p><img src="http://img.cdn.leonwang.top/006tKfTcgy1g1955zzel9j30n607qt8x.jpg" alt=""></p>
<p>:= 赋值</p>
<p>alpha 学习率（梯度下降时迈出多大的步子）</p>
<p>theta0 和 theta1 是<strong>同时更新</strong>的</p>
<p><img src="http://img.cdn.leonwang.top/006tKfTcgy1g1952enn4fj30sw082wf4.jpg" alt=""></p>
<p>右边的方法是错误的，因为在计算 theta1 的过程中使用了新的 theta0</p>
<h2 id="L11-梯度下降知识点总结"><a href="#L11-梯度下降知识点总结" class="headerlink" title="L11 梯度下降知识点总结"></a>L11 梯度下降知识点总结</h2><p>以单变量函数的梯度下降法为例，理解梯度下降法中导数部分和学习率alpha的意义。</p>
<h2 id="L12-线性回归的梯度下降"><a href="#L12-线性回归的梯度下降" class="headerlink" title="L12 线性回归的梯度下降"></a>L12 线性回归的梯度下降</h2><p>平方代价函数 + 梯度下降法</p>
<p>Batch梯度下降算法：每一步梯度下降都遍历了<strong>整个</strong>训练集的样本。</p>
<h1 id="第3章-线性代数回顾"><a href="#第3章-线性代数回顾" class="headerlink" title="第3章 线性代数回顾"></a>第3章 线性代数回顾</h1><h2 id="L14-矩阵和向量"><a href="#L14-矩阵和向量" class="headerlink" title="L14 矩阵和向量"></a>L14 矩阵和向量</h2><p>维数：</p>
<p>​    矩阵 m行n列 （二维）</p>
<p>​    向量 n行1列 （一维）</p>
<p>没有特殊说明的情况下，默认向量的下标从1开始，而不是从0。</p>
<h2 id="L15-加法和标量乘法"><a href="#L15-加法和标量乘法" class="headerlink" title="L15 加法和标量乘法"></a>L15 加法和标量乘法</h2><h2 id="L16-矩阵向量乘法"><a href="#L16-矩阵向量乘法" class="headerlink" title="L16 矩阵向量乘法"></a>L16 矩阵向量乘法</h2><p>形式：</p>
<p><img src="http://img.cdn.leonwang.top/006tNc79gy1g1thtzm8tkj30qc0fcq6c.jpg" alt=""></p>
<p>实际应用：</p>
<p><img src="http://img.cdn.leonwang.top/006tNc79gy1g1thtdplhrj30wa0hsn77.jpg" alt=""></p>
<p>代码用矩阵向量乘法表示而不是用for循环的优点：</p>
<ol>
<li>代码简洁</li>
<li>计算效率更高</li>
</ol>
<h2 id="L17-矩阵乘法"><a href="#L17-矩阵乘法" class="headerlink" title="L17 矩阵乘法"></a>L17 矩阵乘法</h2><p>用于多个hypotheses函数的情形：</p>
<p><img src="http://img.cdn.leonwang.top/006tNc79gy1g1thwi0rmnj30sg0ean19.jpg" alt=""></p>
<p>应用：高效地进行多个假设的计算</p>
<h2 id="L18-矩阵乘法特征"><a href="#L18-矩阵乘法特征" class="headerlink" title="L18 矩阵乘法特征"></a>L18 矩阵乘法特征</h2><p>矩阵乘法</p>
<ul>
<li>不服从交换律</li>
<li>服从结合律</li>
<li>单位矩阵的概念</li>
</ul>
<h2 id="L19-逆和转置"><a href="#L19-逆和转置" class="headerlink" title="L19 逆和转置"></a>L19 逆和转置</h2><p><img src="http://img.cdn.leonwang.top/006tNc79gy1g1ti44z0j9j30j803uwgk.jpg" alt=""></p>
<p>奇异矩阵/退化矩阵：没有逆矩阵的矩阵，比如全零矩阵</p>
<h1 id="第5章-多变量线性回归"><a href="#第5章-多变量线性回归" class="headerlink" title="第5章 多变量线性回归"></a>第5章 多变量线性回归</h1><h2 id="L28-多功能"><a href="#L28-多功能" class="headerlink" title="L28 多功能"></a>L28 多功能</h2><p>例：多变量预测房屋价格</p>
<p><img src="http://img.cdn.leonwang.top/006tNc79gy1g3qn70elnqj30p40cq7a0.jpg" alt=""></p>
<p>假设形式：</p>
<p><img src="http://img.cdn.leonwang.top/006tNc79gy1g3qn9fvsp2j30nk0bm0z1.jpg" alt=""></p>
<p>（惯例：x_0 = 1）</p>
<h2 id="L29-多元梯度下降法"><a href="#L29-多元梯度下降法" class="headerlink" title="L29 多元梯度下降法"></a>L29 多元梯度下降法</h2><p>多元梯度下降法的更新规则：单变量vs多变量</p>
<p><img src="http://img.cdn.leonwang.top/006tNc79gy1g3qneduvi7j30oy0dmai4.jpg" alt=""></p>
<h2 id="L30-多元梯度下降法演练1——特征缩放"><a href="#L30-多元梯度下降法演练1——特征缩放" class="headerlink" title="L30 多元梯度下降法演练1——特征缩放"></a>L30 多元梯度下降法演练1——特征缩放</h2><p><strong>特征缩放：</strong></p>
<p><img src="http://img.cdn.leonwang.top/image-20190606010011784.png" alt=""></p>
<p>如果不同特征的取值范围相差较大，则代价函数图像可能不理想（例如上图太细长），导致梯度下降时不断振荡才能收敛到全局最优。</p>
<p>通过特征缩放，可以让梯度下降找到一条更直接的路径。</p>
<p>不要求特征的范围完全相同，但应该接近。</p>
<p><strong>均值归一化：</strong></p>
<p><img src="http://img.cdn.leonwang.top/image-20190606010355412.png" alt=""></p>
<p>通过减去一个常数，使特征的均值在0附近。</p>
<p>通常的做法： x减去均值，再除以范围的长度。</p>
<p>目的：特征缩放不需要太精确，只是为了让梯度下降更快速。</p>
<h2 id="L31-多元梯度下降法2——学习率"><a href="#L31-多元梯度下降法2——学习率" class="headerlink" title="L31 多元梯度下降法2——学习率"></a>L31 多元梯度下降法2——学习率</h2><p>通过观察”代价函数-迭代步数”图来判断和调整学习率</p>
<p>理想学习率的情况下：</p>
<p><img src="http://img.cdn.leonwang.top/006tNc79gy1g3qnrbbzcqj30j20d8adt.jpg" alt=""></p>
<p>学习率过大的情况：</p>
<p><img src="http://img.cdn.leonwang.top/006tNc79gy1g3qnrtw4e2j30cq09cdhj.jpg" alt=""></p>
<p>对于上面的两种情况，通常调小学习率可以解决。</p>
<p>但是学习率不能太小，否则收敛很慢。</p>
<p>选择学习率的策略：</p>
<p><img src="http://img.cdn.leonwang.top/006tNc79gy1g3qnu4qgjpj30m205itag.jpg" alt=""></p>
<p>每隔3倍取一个，找到过大的和过小的学习率。通常去尝试比 过大的学习率 稍小一点的数作为学习率。</p>
<h2 id="L32-特征和多项式回归"><a href="#L32-特征和多项式回归" class="headerlink" title="L32 特征和多项式回归"></a>L32 特征和多项式回归</h2><p>不只用直线进行拟合，可以用多项式函数。</p>
<p>假设函数是多项式时，特征缩放尤其重要。</p>
<h2 id="L33-正规方程（区别于迭代方法的直接解法）"><a href="#L33-正规方程（区别于迭代方法的直接解法）" class="headerlink" title="L33 正规方程（区别于迭代方法的直接解法）"></a>L33 正规方程（区别于迭代方法的直接解法）</h2><p><img src="http://img.cdn.leonwang.top/image-20190606011858017.png" alt=""></p>
<p>特征方程法给出了直接求解令代价函数最小化的参数向量的计算方法。</p>
<p>正规方程法不需要做特征缩放。</p>
<p><strong>梯度下降法和正规方程法的比较：</strong></p>
<p><img src="http://img.cdn.leonwang.top/image-20190606012301141.png" alt=""></p>
<p>当n的规模较大时，梯度下降法的表现比正规方程法好。</p>
<h2 id="L34-正规方程在矩阵X‘X不可逆情况下的解决方法"><a href="#L34-正规方程在矩阵X‘X不可逆情况下的解决方法" class="headerlink" title="L34 正规方程在矩阵X‘X不可逆情况下的解决方法"></a>L34 正规方程在矩阵X‘X不可逆情况下的解决方法</h2><p>没有逆的矩阵：奇异或退化矩阵</p>
<p>可以使用编程包中计算 <strong>伪逆</strong> 的方法来计算。</p>
<p>机器学习中矩阵X’X不可逆的<strong>原因</strong>通常考虑：</p>
<ul>
<li>包含了多余的特征（比如米和英尺存在定值转换的关系）</li>
<li>特征太多（比如样本数小于特征数） -&gt; 尝试 删掉部分特征 或 正则化</li>
</ul>
<h1 id="第7章-Logistic回归"><a href="#第7章-Logistic回归" class="headerlink" title="第7章 Logistic回归"></a>第7章 Logistic回归</h1><h2 id="L46-分类"><a href="#L46-分类" class="headerlink" title="L46 分类"></a>L46 分类</h2><p>分类问题：y 是离散值</p>
<p>把线性回归应用于分类问题通常不是个好主意。</p>
<p>Logistic函数：值在0-1之间</p>
<p>Logistic回归被视为一种分类算法（虽然名字里带回归，但实际上是分类算法）</p>
<h2 id="L47-假设函数"><a href="#L47-假设函数" class="headerlink" title="L47 假设函数"></a>L47 假设函数</h2><p>假设函数：<br>$$<br>\mathbf{h}_{\theta}(x)=g\left(\mathbf{\theta}^{T} \mathbf{x}\right)<br>$$<br>其中，g 是 Sigmoid 函数（或称 Logistic 函数，二者几乎是等价的）：<br>$$<br>g(z)=\frac{1}{1+e^{-z}}<br>$$<br><img src="http://img.cdn.leonwang.top/20190724150741.png" alt=""></p>
<p>对 $\mathbf{h}_{\theta}(x)$ 的解释：表示y=1的概率，即 $\mathbf{h}_{\theta}(x)=P(y=1 | x ; \theta)=1-P(y=0 | x ; \theta)$   （y的值只能是0或1）</p>
<h2 id="L48-决策界限"><a href="#L48-决策界限" class="headerlink" title="L48 决策界限"></a>L48 决策界限</h2><p>决策边界（decision boundary）</p>
<p>可以帮助我们理解上面的假设函数是如何做出预测的</p>
<p>即指定判定输出y=0还是y=1的阈值（在假设函数中对应的线/面/…）</p>
<p>决策边界是假设函数的属性，而不是数据集的属性</p>
<h2 id="L49-代价函数"><a href="#L49-代价函数" class="headerlink" title="L49 代价函数"></a>L49 代价函数</h2><p>如果使用线性回归中的代价函数计算方式，由于$ \mathbf{h}_{\theta}(x)$ 是非线性的，所以最终计算的代价函数 $ J(\theta)$ 是非凸函数，导致无法很好地应用梯度下降法来寻找全局最优解</p>
<blockquote>
<p>非凸函数有很多的局部最优值，例如<img src="http://img.cdn.leonwang.top/20190724153422.png" alt=""></p>
</blockquote>
<p>因此不采用上述代价函数，而采用：<br>$$<br>\begin{array}{ll}{J(\theta)=\frac{1}{m} \sum_{i=1}^{m} \operatorname{cost}\left(h_{\theta}\left(x^{(i)}\right), y^{(i)}\right)} &amp; {} \ {\operatorname{cost}\left(h_{\theta}(x), y\right)=-\log \left(h_{\theta}(x)\right)} &amp; {\text { if } y=1} \ {\operatorname{cost}\left(h_{\theta}(x), y\right)=-\log \left(1-h_{\theta}(x)\right)} &amp; {\text { if } y=0}\end{array}<br>$$<br>这样的 $J(\theta)$ 是凸函数且没有局部最优</p>
<p><img src="http://img.cdn.leonwang.top/20190724155356.png" alt=""></p>
<p><img src="http://img.cdn.leonwang.top/20190724155414.png" alt=""></p>
<p>性质：<br>$$<br>\begin{array}{l}{\operatorname{cost}\left(h_{\theta}(x), y\right)=0 \text { if } h_{\theta}(x)=y} \ {\operatorname{cost}(h \theta(x), y) \rightarrow \infty \text { if } y=0 \text { and } h_{\theta}(x) \rightarrow 1} \ {\operatorname{cost}\left(h_{\theta}(x), y\right) \rightarrow \infty \text { if } y=1 \text { and } h_{\theta}(x) \rightarrow 0}\end{array}<br>$$</p>
<h2 id="L50-简化代价函数与梯度下降"><a href="#L50-简化代价函数与梯度下降" class="headerlink" title="L50 简化代价函数与梯度下降"></a>L50 简化代价函数与梯度下降</h2><p>把上面的代价函数改写成如下形式：<br>$$<br>J(\theta)=-\frac{1}{m} \sum_{i=1}^{m}\left[y^{(i)} \log \left(h_{\theta}\left(x^{(i)}\right)\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]<br>$$</p>
<blockquote>
<p>与之前等价，但更加紧凑。合并成一个式子便于梯度下降。</p>
</blockquote>
<p>然后可以应用梯度下降法最小化代价函数来更新参数</p>
<p>线性回归中的特征缩放同样适用于逻辑回归</p>
<h2 id="L51-高级优化"><a href="#L51-高级优化" class="headerlink" title="L51 高级优化"></a>L51 高级优化</h2><p>梯度下降法以外的优化算法：</p>
<p>共轭梯度，bfgs，l-bfgs：能自动选择学习率，但很复杂，没必要理解和自己实现</p>
<p>本节主要介绍了以上高级优化算法在Octave函数库中的调用，没仔细看…</p>
<h2 id="L52-多元分类：一对多"><a href="#L52-多元分类：一对多" class="headerlink" title="L52 多元分类：一对多"></a>L52 多元分类：一对多</h2><p>多元分类（相比于二分类）：分类的结果超过两种</p>
<p><img src="http://img.cdn.leonwang.top/20190725165626.png" alt=""></p>
<p>方法：三分类 =&gt; 拟合3个二分类的分类器</p>
<p><img src="http://img.cdn.leonwang.top/20190725170109.png" alt=""></p>
<p>预测（以上述三分类为例）：把 x 分别输入3个分类器，以输出 $\mathbf{h}_{\theta}(x)$ 最高的一个分类器对应的类别作为预测结果 y</p>
<h1 id="第8章-正则化"><a href="#第8章-正则化" class="headerlink" title="第8章 正则化"></a>第8章 正则化</h1><h2 id="L55-过拟合问题"><a href="#L55-过拟合问题" class="headerlink" title="L55 过拟合问题"></a>L55 过拟合问题</h2><blockquote>
<p>本节为了引出正则化</p>
</blockquote>
<p>正则化 可以改善或减少 过拟合 问题</p>
<p>欠拟合（underfitting）: has high bias（具有高偏差）</p>
<p>过拟合（overfitting）：has high variance（具有高方差）</p>
<p>过拟合可能会在特征过多，但样本很少时出现</p>
<p>避免过拟合的方法：</p>
<ul>
<li>减少特征（选取变量）的数量</li>
<li><strong>正则化</strong></li>
</ul>
<h2 id="L56-代价函数"><a href="#L56-代价函数" class="headerlink" title="L56 代价函数"></a>L56 代价函数</h2><blockquote>
<p>介绍了正则化，同时给出代价函数</p>
</blockquote>
<p>正则化：修改代价函数，来缩小各个参数的值</p>
<ul>
<li>方法：在代价函数后面加上正则化项，正则化项的系数称为正则化系数</li>
</ul>
<p>$$<br>\begin{array}{l}{J(\theta)=\frac{1}{2 m}\left[\sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+\lambda \sum_{j=1}^{n} \theta_{j}^{2}\right]} \ {\min _{\theta} J(\theta)}\end{array}<br>$$</p>
<h2 id="L57-线性回归的正则化"><a href="#L57-线性回归的正则化" class="headerlink" title="L57 线性回归的正则化"></a>L57 线性回归的正则化</h2><p>前面学过两种方法：</p>
<ol>
<li>梯度下降法</li>
<li>正规方程法</li>
</ol>
<p>分别介绍这两种方法在代价函数中加入正则化项之后应该如何应用</p>
<h2 id="L61-Logistic回归的正则化"><a href="#L61-Logistic回归的正则化" class="headerlink" title="L61 Logistic回归的正则化"></a>L61 Logistic回归的正则化</h2><p>前面学过两种方法：</p>
<ol>
<li>梯度下降法</li>
<li>高级优化</li>
</ol>
<p>分别介绍这两种方法在代价函数中加入正则化项之后应该如何应用</p>
<h1 id="第9章-神经网络学习"><a href="#第9章-神经网络学习" class="headerlink" title="第9章 神经网络学习"></a>第9章 神经网络学习</h1><h2 id="L62-非线性假设"><a href="#L62-非线性假设" class="headerlink" title="L62 非线性假设"></a>L62 非线性假设</h2><p>传统的逻辑回归进行分类，在面对复杂的（特征空间较大）的非线性假设时，计算量太大</p>
<p>所以使用神经网络</p>
<h2 id="L63-神经元与大脑"><a href="#L63-神经元与大脑" class="headerlink" title="L63 神经元与大脑"></a>L63 神经元与大脑</h2><p>story</p>
<h2 id="L64-模型展示1"><a href="#L64-模型展示1" class="headerlink" title="L64 模型展示1"></a>L64 模型展示1</h2><p>神经元的激活函数（通常sigmoid函数）</p>
<p>weights == parameters</p>
<p>神经网络： input layer — hidden layer — output layer</p>
<p>隐藏层可以不止一个</p>
<p>神经网络的 <strong>假设函数</strong> 的数学定义：</p>
<p><img src="http://img.cdn.leonwang.top/20190725205823.png" alt=""></p>
<h2 id="L65-模型展示2"><a href="#L65-模型展示2" class="headerlink" title="L65 模型展示2"></a>L65 模型展示2</h2><p>计算过程：<strong>前向传播</strong> 及其向量化表示</p>
<h2 id="L68-例子与直觉理解1"><a href="#L68-例子与直觉理解1" class="headerlink" title="L68 例子与直觉理解1"></a>L68 例子与直觉理解1</h2><p>使用单个神经元实现逻辑运算 AND, OR</p>
<p><img src="http://img.cdn.leonwang.top/20190725214908.png" alt=""></p>
<h2 id="L70-例子与直觉理解2"><a href="#L70-例子与直觉理解2" class="headerlink" title="L70 例子与直觉理解2"></a>L70 例子与直觉理解2</h2><p>使用单个神经元实现逻辑运算 NOT</p>
<p><img src="http://img.cdn.leonwang.top/20190725214854.png" alt=""></p>
<p>使用多层神经元的神经网络实现逻辑运算 XNOR</p>
<p>手写数字识别的演示视频</p>
<h2 id="L71-多元分类"><a href="#L71-多元分类" class="headerlink" title="L71 多元分类"></a>L71 多元分类</h2><p>神经网络实现多元分类的原理：</p>
<p>输出层神经元的个数与类别的个数相同，假设函数的函数值是类别的 one-hot 编码形式</p>
<p><img src="http://img.cdn.leonwang.top/20190725215818.png" alt=""></p>
<h1 id="第10章-神经网络参数的反向传播算法"><a href="#第10章-神经网络参数的反向传播算法" class="headerlink" title="第10章 神经网络参数的反向传播算法"></a>第10章 神经网络参数的反向传播算法</h1><h2 id="L72-代价函数"><a href="#L72-代价函数" class="headerlink" title="L72 代价函数"></a>L72 代价函数</h2><p>L：神经网络总层数</p>
<p>$s_l$ ：第 $l$ 层的神经元个数（不包括偏差单元）</p>
<p>神经网络中的代价函数 是 Logistic回归的代价函数 的一般形式：</p>
<p><img src="http://img.cdn.leonwang.top/20190809221747.png" alt=""></p>
<h2 id="L73-反向传播算法"><a href="#L73-反向传播算法" class="headerlink" title="L73 反向传播算法"></a>L73 反向传播算法</h2><p>神经网络中让代价函数最小的方法</p>
<p>$\delta_{j}^{(l)}$：第 l 层的第 j 个神经元的激活值的<strong>误差</strong></p>
<p>计算误差项的过程，是从输出层开始，逐渐向前计算</p>
<h2 id="L74-理解反向传播"><a href="#L74-理解反向传播" class="headerlink" title="L74 理解反向传播"></a>L74 理解反向传播</h2><h2 id="L75-使用注意：展开参数"><a href="#L75-使用注意：展开参数" class="headerlink" title="L75 使用注意：展开参数"></a>L75 使用注意：展开参数</h2><h2 id="L76-梯度检测"><a href="#L76-梯度检测" class="headerlink" title="L76 梯度检测"></a>L76 梯度检测</h2><h2 id="L77-随机初始化"><a href="#L77-随机初始化" class="headerlink" title="L77 随机初始化"></a>L77 随机初始化</h2><h2 id="L78-组合到一起"><a href="#L78-组合到一起" class="headerlink" title="L78 组合到一起"></a>L78 组合到一起</h2><h2 id="L80-无人驾驶"><a href="#L80-无人驾驶" class="headerlink" title="L80 无人驾驶"></a>L80 无人驾驶</h2>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
            <a href="/tags/吴恩达/" rel="tag"># 吴恩达</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/07/23/Global-Average-Pooling/" rel="next" title="Global Average Pooling">
                <i class="fa fa-chevron-left"></i> Global Average Pooling
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/09/09/yolov3-Keras实现解读/" rel="prev" title="yolov3 Keras实现解读">
                yolov3 Keras实现解读 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpeg" alt="Leon Wang">
            
              <p class="site-author-name" itemprop="name">Leon Wang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">34</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">12</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">38</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#第1章-绪论：初识机器学习"><span class="nav-number">1.</span> <span class="nav-text">第1章 绪论：初识机器学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#L2-什么是机器学习"><span class="nav-number">1.1.</span> <span class="nav-text">L2 什么是机器学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L3-监督学习"><span class="nav-number">1.2.</span> <span class="nav-text">L3 监督学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L4-无监督学习"><span class="nav-number">1.3.</span> <span class="nav-text">L4 无监督学习</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第2章-单变量线性回归"><span class="nav-number">2.</span> <span class="nav-text">第2章 单变量线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#L6-模型描述"><span class="nav-number">2.1.</span> <span class="nav-text">L6 模型描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L7-代价函数"><span class="nav-number">2.2.</span> <span class="nav-text">L7 代价函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L8-代价函数（一）"><span class="nav-number">2.3.</span> <span class="nav-text">L8 代价函数（一）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L9-代价函数（二）"><span class="nav-number">2.4.</span> <span class="nav-text">L9 代价函数（二）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L10-梯度下降"><span class="nav-number">2.5.</span> <span class="nav-text">L10 梯度下降</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L11-梯度下降知识点总结"><span class="nav-number">2.6.</span> <span class="nav-text">L11 梯度下降知识点总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L12-线性回归的梯度下降"><span class="nav-number">2.7.</span> <span class="nav-text">L12 线性回归的梯度下降</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第3章-线性代数回顾"><span class="nav-number">3.</span> <span class="nav-text">第3章 线性代数回顾</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#L14-矩阵和向量"><span class="nav-number">3.1.</span> <span class="nav-text">L14 矩阵和向量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L15-加法和标量乘法"><span class="nav-number">3.2.</span> <span class="nav-text">L15 加法和标量乘法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L16-矩阵向量乘法"><span class="nav-number">3.3.</span> <span class="nav-text">L16 矩阵向量乘法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L17-矩阵乘法"><span class="nav-number">3.4.</span> <span class="nav-text">L17 矩阵乘法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L18-矩阵乘法特征"><span class="nav-number">3.5.</span> <span class="nav-text">L18 矩阵乘法特征</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L19-逆和转置"><span class="nav-number">3.6.</span> <span class="nav-text">L19 逆和转置</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第5章-多变量线性回归"><span class="nav-number">4.</span> <span class="nav-text">第5章 多变量线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#L28-多功能"><span class="nav-number">4.1.</span> <span class="nav-text">L28 多功能</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L29-多元梯度下降法"><span class="nav-number">4.2.</span> <span class="nav-text">L29 多元梯度下降法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L30-多元梯度下降法演练1——特征缩放"><span class="nav-number">4.3.</span> <span class="nav-text">L30 多元梯度下降法演练1——特征缩放</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L31-多元梯度下降法2——学习率"><span class="nav-number">4.4.</span> <span class="nav-text">L31 多元梯度下降法2——学习率</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L32-特征和多项式回归"><span class="nav-number">4.5.</span> <span class="nav-text">L32 特征和多项式回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L33-正规方程（区别于迭代方法的直接解法）"><span class="nav-number">4.6.</span> <span class="nav-text">L33 正规方程（区别于迭代方法的直接解法）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L34-正规方程在矩阵X‘X不可逆情况下的解决方法"><span class="nav-number">4.7.</span> <span class="nav-text">L34 正规方程在矩阵X‘X不可逆情况下的解决方法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第7章-Logistic回归"><span class="nav-number">5.</span> <span class="nav-text">第7章 Logistic回归</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#L46-分类"><span class="nav-number">5.1.</span> <span class="nav-text">L46 分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L47-假设函数"><span class="nav-number">5.2.</span> <span class="nav-text">L47 假设函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L48-决策界限"><span class="nav-number">5.3.</span> <span class="nav-text">L48 决策界限</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L49-代价函数"><span class="nav-number">5.4.</span> <span class="nav-text">L49 代价函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L50-简化代价函数与梯度下降"><span class="nav-number">5.5.</span> <span class="nav-text">L50 简化代价函数与梯度下降</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L51-高级优化"><span class="nav-number">5.6.</span> <span class="nav-text">L51 高级优化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L52-多元分类：一对多"><span class="nav-number">5.7.</span> <span class="nav-text">L52 多元分类：一对多</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第8章-正则化"><span class="nav-number">6.</span> <span class="nav-text">第8章 正则化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#L55-过拟合问题"><span class="nav-number">6.1.</span> <span class="nav-text">L55 过拟合问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L56-代价函数"><span class="nav-number">6.2.</span> <span class="nav-text">L56 代价函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L57-线性回归的正则化"><span class="nav-number">6.3.</span> <span class="nav-text">L57 线性回归的正则化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L61-Logistic回归的正则化"><span class="nav-number">6.4.</span> <span class="nav-text">L61 Logistic回归的正则化</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第9章-神经网络学习"><span class="nav-number">7.</span> <span class="nav-text">第9章 神经网络学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#L62-非线性假设"><span class="nav-number">7.1.</span> <span class="nav-text">L62 非线性假设</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L63-神经元与大脑"><span class="nav-number">7.2.</span> <span class="nav-text">L63 神经元与大脑</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L64-模型展示1"><span class="nav-number">7.3.</span> <span class="nav-text">L64 模型展示1</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L65-模型展示2"><span class="nav-number">7.4.</span> <span class="nav-text">L65 模型展示2</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L68-例子与直觉理解1"><span class="nav-number">7.5.</span> <span class="nav-text">L68 例子与直觉理解1</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L70-例子与直觉理解2"><span class="nav-number">7.6.</span> <span class="nav-text">L70 例子与直觉理解2</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L71-多元分类"><span class="nav-number">7.7.</span> <span class="nav-text">L71 多元分类</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第10章-神经网络参数的反向传播算法"><span class="nav-number">8.</span> <span class="nav-text">第10章 神经网络参数的反向传播算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#L72-代价函数"><span class="nav-number">8.1.</span> <span class="nav-text">L72 代价函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L73-反向传播算法"><span class="nav-number">8.2.</span> <span class="nav-text">L73 反向传播算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L74-理解反向传播"><span class="nav-number">8.3.</span> <span class="nav-text">L74 理解反向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L75-使用注意：展开参数"><span class="nav-number">8.4.</span> <span class="nav-text">L75 使用注意：展开参数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L76-梯度检测"><span class="nav-number">8.5.</span> <span class="nav-text">L76 梯度检测</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L77-随机初始化"><span class="nav-number">8.6.</span> <span class="nav-text">L77 随机初始化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L78-组合到一起"><span class="nav-number">8.7.</span> <span class="nav-text">L78 组合到一起</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L80-无人驾驶"><span class="nav-number">8.8.</span> <span class="nav-text">L80 无人驾驶</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>

<span>鲁ICP备 -
  <a href="http://www.miitbeian.gov.cn/">18054179号</a></span>

  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Leon Wang</span>

  
</div>











    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人</span>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












   <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
   <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
   <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
   <script type="text/javascript">
        var gitalk = new Gitalk({
          clientID: 'e25cb882a9cfd69b0fcf',
          clientSecret: 'd5aa7f1f939ecb26efd4c1279013d10158e19c7f',
          repo: 'comments',
          owner: 'AzureLeon1',
          admin: ['AzureLeon1'],
          id: md5(window.location.pathname),
          distractionFreeMode: 'true'
        })
        gitalk.render('gitalk-container')
       </script>



  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  
  <script type="text/javascript" src="/js/src/js.cookie.js?v=5.1.4"></script>
  <script type="text/javascript" src="/js/src/scroll-cookie.js?v=5.1.4"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


  





</body>
</html>
