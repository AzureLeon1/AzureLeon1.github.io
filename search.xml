<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>字节跳动实习记录</title>
      <link href="2021/01/03/%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%AE%9E%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
      <url>2021/01/03/%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%AE%9E%E4%B9%A0%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<p>从2020年7月27日到2020年12月23日，在字节总共实习了接近5个月。</p><p>把实习的工作内容做个简单的总结：</p><p>工程部分的工作主要和RTA广告相关，从最开始对广告投放一窍不通，到最后成为RTA广告的半个owner，多次向别人介绍RTA业务和技术架构。通过围绕RTA做的工程开发，我对大厂如何快速迭代、DevOps、IDL、rpc服务、go语言、开发机使用、项目编译和架构、如何设计缓存、如何理解离线侧与实时侧、动态配置、监控埋点、如何压测/小流量后规范上线、排查事故都有了一定的了解。最终功能上线并且有多个广告主接入服务。</p><p>算法部分做了两件事：</p><p>一个是负责商品召回通路里模型侧的工作。从cpa_bid数据流到sorted_ecpm数据流，从 multi_head LTR 模型架构到 logits_sum LTR 模型架构，以及还没来得及尝试的多head cpa_bid 回归模型。模型如何在训练器、training ps、online ps、pilot、p2p分发、预估服务中一步步部署。如何看离线指标和通过AB实验观察实时指标，如何分析实验结果是否置信。最终模型NDCG离线指标上涨、在鲁班和SDPA广告场景下cost、adv value、send等实时指标都拿到了1到2个点的收益。</p><p>另一个是广告场景下Graph Embedding调研。GE的收益点、可尝试的方向（与推荐侧结合、与NLP结合），临近离职，把自己的调研和总结给部门里的Leader和几十个同事做了一次分享。</p><p>2次北京中关村E世界出差、1次秦皇岛京条共建、双十一值班、几次团建（卡丁车、海鲜自助、真人CS、轰趴、海底捞、烧烤）也都是比较有趣的经历。</p><p>体会：</p><ol><li>感谢xuchen和所有同事们的指导和帮助，即便是实习生也能够去参与和承担重要的、有意义的工作，而且自由度很高。</li><li>不论是工程开发还是模型算法，学校里能接触到的和企业里在做的都是大相径庭。就模型算法而言，学术界强调科研创新，企业界更强调能否落地和带来收益，并且和业务强相关。</li><li>字节每个人都是项目owner，要负责去推进自己的项目，很不错。</li><li>字节开会有点太多了，吐槽一下。</li><li>字节不讲title的氛围真的很好，员工之间直呼其名，沟通交流很舒服。</li><li>福利很好，伙食很好。</li><li>加班看部门，不过我的部门确实加班挺多的。</li><li>出差了几次，感觉北京的生活环境和办公环境真的不如上海。</li></ol><p><img src="/2021/01/03/字节跳动实习记录/badge.jpeg" alt="img" style="zoom: 25%;"></p>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实习 </tag>
            
            <tag> 字节 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>保研经历记录</title>
      <link href="2021/01/03/%E4%BF%9D%E7%A0%94%E7%BB%8F%E5%8E%86%E8%AE%B0%E5%BD%95/"/>
      <url>2021/01/03/%E4%BF%9D%E7%A0%94%E7%BB%8F%E5%8E%86%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<p>保研也算是2020年完成的比较重要的事情之一，虽然我的保研在八月上旬的时候就有了结论，但因为保研和之后的实习无缝衔接上了，一直没抽出时间来好好整理一下自己的保研经历。恰逢最近离职，总结一下自己的保研过程。</p><h2 id="起源"><a href="#起源" class="headerlink" title="起源"></a>起源</h2><p>大二大三肝课程项目太多，对开发失去热情，转而研究机器学习。既然要搞算法，读研还是有必要的。</p><p>有了保研的想法之后，大三上的时候开始去搜集相关的信息，包括热门院校、前届学长学姐的去向和经验等等。最终敲定了把中科院自动化所作为目标院校。</p><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>正式准备工作是从大三寒假开始的，分以下几个方面：</p><ol><li><p>专业课</p><p> 计算机专业的保研，不论是什么方向，专业课基本都是必须准备的科目。数据结构、算法、操作系统、计算机网络、编译原理、离散数学等等。</p></li><li><p>数学 &amp; 机器学习</p><p> 了解到自动化所的夏令营面试很喜欢考察数学，所以专门花了一些时间来复习数学，包括高数、线代、概率论与数理统计、统计学习方法等等，还有奇异值分解这类常考的问题也要看一下。关于概率论和数理统计这门课，墙裂推荐浙大出版的教材，比同济那本要好不少。</p></li><li><p>英语</p><p> 练一下口语，夏令营一般都会有英语面试环节。</p></li><li><p>OJ机试</p><p> 王道机试指南的书、牛客网、LeetCode。因为没考虑浙大所以没怎么准备PAT。</p></li></ol><p>从大三寒假到各个院校开始举办夏令营的几个月，基本上就是在一边忙课程，一边按计划复习。随时关注各个院校发布的消息，及时上传、邮寄报名材料。</p><h2 id="夏令营"><a href="#夏令营" class="headerlink" title="夏令营"></a>夏令营</h2><p><img src="/2021/01/03/保研经历记录/image-20210331203221409.png" alt="Figure 1" style="zoom: 40%;"></p><p>夏令营尝试的院校包括：</p><ul><li>清北（摸奖）</li><li>中科院在京三所</li><li>南大计算机</li><li>复旦计算机</li><li>上交计算机</li></ul><p>申请的学校里：</p><ul><li>清北没过初审，符合预期</li><li>计算所没过初审，计算所联系导师的环节比较重要，自己当时联系导师的环节比较随意，没过初审也算符合预期（如果对计算所有意愿的话，联系导师是很重要的，计算所是导师捞简历，而且还有霸面的机会，这点和其他学校不太一样）</li><li>软件所没过初审，不太理解</li><li>自动化所、上交计算机、南大计算机、复旦计算机都顺利入营并且最终拿到了offer，最后选择了自动化所</li></ul><p><strong>面经部分</strong></p><p>（按照夏令营的时间顺序）</p><p><strong>一、南大计算机</strong></p><p>offer：学硕，MCG组(Multimedia Computing Group，媒体计算研究组)</p><p>南大计算机系会统一组织考核，而有的实验室/导师还会再单独组织一轮内部考核，所以要两边都通过才算真正拿到offer。</p><p>当时自己还对 CV 比较感兴趣，所以选择了南大 CV 方向的 MCG 组。</p><p>组内考核：面试官是wlm老师，10min左右，更像是聊天。问的问题包括如何看待计算机视觉这个领域、我之前在CV领域的经历、我未来的研究计划和对自己的期望等等。和w老师交流的过程非常愉快，老师很平易近人，最终也是顺利通过了MCG组的组内考核。</p><p>计算机系考核：笔试10mins，面试10mins（包括英语面试和专业面试）</p><blockquote><p><strong>笔试</strong></p><p>每道题给1分钟或2分钟浏览，然后5秒内说出答案，有几道题忘记了，记得的有下面几道：</p><ol><li>不属于TCP可靠性控制的是？</li><li>两个矩阵，分别是否符合群的定义？</li><li>给一段代码和输入的数据，运行结果是什么？</li><li>最后是一个开放性问题：你认为本科期间最成功的的一件事是什么？不限于学习</li></ol><p><strong>英语面试</strong></p><ol><li><p>your favorite computer science related course？</p></li><li><p>具体学过哪些数据结构？</p></li><li><p>数组和链表的区别？</p></li><li><p>Merge sort算法你会选择用数组实现还是链表实现？为什么？</p></li></ol><p><strong>专业面试</strong></p><ol><li>课程之间有先修的关系，如何安排课程的学习顺序（拓扑排序）</li><li><p>如果内存无法载入全部数据，如何排序？（外部排序）</p></li><li><p>课程项目中用过哪些编程语言？</p></li><li><p>c、java、python之间的区别？</p></li><li><p>编译的过程分哪几步？</p></li><li><p>语法分析和语义分析的区别？举例详细分析一下？</p></li><li><p>操作系统中虚拟内存的作用？详细解释一下？</p></li><li><p>回到归并排序上来，如何把归并排序的递归版本变成迭代版本？</p></li></ol></blockquote><p>总结：</p><p>南大计算机的夏令营考核很看重计算机专业课，对专业课的考核涵盖了计网、离散数学、数据结构、操作系统、编译原理等，算是比较全面的了。往年南大对编程能力的考查也是很重视的，今年受疫情影响只能线上举办夏令营，所以没有了机试，但笔试、面试里都有对算法和编程能力能力的考查。</p><p>因为南大是我参加的第一场夏令营，当时准备还不充分。之前一直都是以自动化所的标准来准备，比较侧重数学，专业课当时只复习了数据结构，所以面试时数据结构的问题都回答得不错，但是问到编译原理和操作系统的问题就完全gg了。_(:3」∠)_</p><p>还记得当时面试老师对我编译原理和操作系统的回答都挺不满意的，但最后出来竟然也是优营，所以夏令营笔试和面试表现得不完美也还是有可能拿到offer~</p><p>offer：</p><p><img src="/2021/01/03/保研经历记录/image-20210104022137396-7193983.png" alt="Figure 2" style="zoom: 33%;"></p><p><img src="/2021/01/03/保研经历记录/image-20210104022215233-7194060.png" alt="Figure 3" style="zoom:33%;"></p><p><strong>二、复旦计算机</strong></p><p>offer：专硕，推荐系统方向</p><p>复旦计算机看报录比的话竞争程度还是挺激烈的，不过其实看生源的话可能热门方向神仙打架，冷门方向的生源质量相对一般：</p><p><img src="/2021/01/03/保研经历记录/image-20210104022531104-7194071.png" alt="Figure 4" style="zoom: 25%;"></p><p>机试：</p><ol><li>根据课程之间的先修关系输出修读课程的顺序，经典拓扑排序问题</li><li>0-1矩阵中的最大正方形，经典动态规划问题</li><li>树中部分节点染色后查询指定节点到染色节点的距离，树的距离问题</li></ol><p>英文面试：</p><ol><li>同济的计算机也很不错，你为什么要申请复旦计算机的夏令营呢？</li><li>你参与过一些项目、参加过数学建模的比赛，从中有什么收获？</li></ol><p>专业面试：</p><ol><li>机试中的第三题还能不能继续优化？第一题中的特殊情况是如何考虑的？</li><li>面向简历提问，包括参与过的项目、科研经历、论文的详细情况</li><li>推荐系统相关问题</li></ol><p>offer：</p><p><img src="/2021/01/03/保研经历记录/image-20210104024207768-7194085.png" alt="Figure 5" style="zoom:33%;"></p><p><strong>三、中科院自动化所</strong></p><p>offer：学硕，智能感知与计算研究中心，数据挖掘</p><p>面试内容（也有点记不太清了，还记得的有这些）：</p><ol><li>英文面试：自我介绍、对比家乡城市和上海</li><li>矩阵的秩</li><li>方程组有解/无解/无穷多解的条件</li><li>数据结构中图结构相关的问题</li><li>协同过滤算法</li><li>还有没有参加其他夏令营</li></ol><p>offer：</p><p><img src="/2021/01/03/保研经历记录/image-20210104024639914-7194098.png" alt="Figure 6" style="zoom:33%;"></p><p><strong>四、上交计算机</strong></p><p>offer：专硕</p><p>上交的夏令营举办是在自动化所之前，不过出结果比较晚</p><p>只有一场面试，面试内容：</p><p>基础知识三道题：</p><ol><li>Python不是用指针来寻址，而是用<strong>____</strong>来寻址？</li><li>TCP和UDP的特点和比较？</li><li>什么是霍夫曼树？怎么构造一颗霍夫曼树？</li></ol><p>英文问题：</p><ol><li>选择一门课程或者一个项目，对其中的一个技术细节进行介绍？</li></ol><p>专业面试：</p><p>围绕自我介绍展开提问。包括参与过的科研项目、对推荐系统方向的了解、“推荐系统中的邻域是如何确定的？”。</p><p>offer：</p><p><img src="/2021/01/03/保研经历记录/image-20210104025046957-7194152.png" alt="Figure 7" style="zoom:33%;"></p><h2 id="插曲"><a href="#插曲" class="headerlink" title="插曲"></a>插曲</h2><p><strong>金融科技</strong></p><p>因为以前接触过一点金融，所以专门看过一些金融科技方向的信息。包括北大软微、人大财政金融学院、上财金融学院。最后还是没有选择去做FinTech。</p><p><strong>从CV到推荐</strong></p><p>大三一年都在做CV的项目，后来感觉CV实在难落地，所以转投了推荐/广告/搜索的怀抱。</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>在我看来，保研的过程要保持一个自信、坦然、平常心的心态。即使笔试面试发挥得不够完美，仍然有可能收获offer，只要把自己最真实的水平表现出来就好。</p><p>因为夏令营阶段就拿到了自己满意的offer，后面就没有再参加预推免和九推的环节。七月初还在为了期末课程项目而焦头烂额，七月中下旬拿到了南大和复旦的保底offer，然后以一个比较轻松的心态继续参与了上交和自动化所的夏令营，中间还穿插了字节算法岗的实习面试。事情安排得比较紧凑，最后也都顺顺利利地结束。</p><p>现在回想起来，当时也没有过多的焦虑，只要把自己该做的事情做好，剩下的就交给时间。</p><p>但行好事，莫问前程~</p>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 保研 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>注意力机制</title>
      <link href="2020/12/25/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
      <url>2020/12/25/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><h4 id="Seq2Seq模型的提出"><a href="#Seq2Seq模型的提出" class="headerlink" title="Seq2Seq模型的提出"></a>Seq2Seq模型的提出</h4><p>为了解决<strong>机器翻译</strong>问题提出的 Seq2Seq模型基于 RNN，最早由这两篇文章提出：</p><ol><li><blockquote><p>Sutskever, I., Vinyals, O., &amp; Le, Q. V. (2014). <strong>Sequence to sequence learning with neural networks</strong>. <em>Advances in neural information processing systems</em>, <em>27</em>, 3104-3112.</p></blockquote><p> 使用 LSTM 来构建 Seq2Seq模型。其中前三个为 Encoder 层，后五个为 Decoder 层。</p><p> 这个最早提出的 Seq2Seq 模型还有一个缺点，只支持固定长度的输入。</p><p> <img src="http://img.cdn.leonwang.top/image-20201225142512450.png" style="zoom:50%;"></p></li><li><blockquote><p>Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., &amp; Bengio, Y. (2014). <strong>Learning phrase representations using RNN encoder-decoder for statistical machine translation</strong>. <em>arXiv preprint arXiv:1406.1078</em>.</p></blockquote><p> 提出了基于 GRU 的 Seq2Seq 模型</p><p><img src="http://img.cdn.leonwang.top/image-20201225142533287.png" style="zoom:33%;"></p></li></ol><h4 id="传统Seq2Seq模型的局限"><a href="#传统Seq2Seq模型的局限" class="headerlink" title="传统Seq2Seq模型的局限"></a>传统Seq2Seq模型的局限</h4><p>传统 Seq2Seq 模型主要由两个 RNN 模型组成，分别起到 Encoder 和 Decoder 的作用，通过“不定长序列 -&gt; 定长隐向量 -&gt; 不定长序列”的转换实现机器翻译。</p><p>传统 Seq2Seq 的局限在于：</p><ol><li>对输入序列缺乏区分度</li><li>对长序列有信息损失。因为会把任何序列都首先转化为定长的隐向量，对于比较长的输入序列会有信息损失。从另一个角度来看，RNN机制本身存在着长程梯度消失的问题。</li></ol><p>为了解决上述问题，引入了 Attention 机制。</p><h3 id="Attention-Mechanism"><a href="#Attention-Mechanism" class="headerlink" title="Attention Mechanism"></a>Attention Mechanism</h3><blockquote><p>Bahdanau, D., Cho, K., &amp; Bengio, Y. (2014). <strong>Neural machine translation by jointly learning to align and translate</strong>. <em>arXiv preprint arXiv:1409.0473</em>.</p></blockquote><h4 id="提出"><a href="#提出" class="headerlink" title="提出"></a>提出</h4><p><img src="http://img.cdn.leonwang.top/image-20201225144357457.png" style="zoom:33%;"></p><p>引入 Attention Mechanism 后，Seq2Seq中的encoder被替换为一个双向循环网络（bidirectional RNN），decoder部分则利用encoder部分的所有输入信息（加权求和），这使得对于<strong>每个输出y</strong>，都和之前的输出以及当前的<strong>整个句子</strong>有关。</p><p>同时，原来输出的梯度只能从最后一个时刻的隐含状态向更早时刻传导，有了attention机制之后，梯度能直接传导到输入部分，降低的梯度消失的风险。</p><p>因为不再需要将完整的输入序列编码为固定长度的向量，而是要让全部的原文序列参与到每一步的 decoder 中，那么就会面临一个核心问题，需要在每一步 decoder 中确定输入序列各个部分的权重（物理含义是每一步预测时输入序列的哪些部分对当前预测更加重要）。所以需要对序列的不同部分进行<strong>加权</strong>。</p><h4 id="计算过程"><a href="#计算过程" class="headerlink" title="计算过程"></a>计算过程</h4><ol><li><p>利用 BRNN 结构得到 hidden state： $s_{t-1}=(h_1, h_2, h_3, …, h_t)$</p></li><li><p>计算当前 state 中每个输入位置 $h_i$ 的权重，权重通过相关性算符和softmax计算得到，然后加权求和得到 context vector：$c_t$</p><p> <img src="http://img.cdn.leonwang.top/image-20201225144843587.png" style="zoom:33%;"></p><p> 其中 a(s,h) 表示相关性算符，常见的比如点乘、加权点乘</p></li><li><p>计算输出的条件概率：</p><p> <img src="http://img.cdn.leonwang.top/image-20201225151013313.png" style="zoom:33%;"></p></li></ol><p>通过可视化权重矩阵理解模型的工作机制：</p><p><img src="http://img.cdn.leonwang.top/image-20201225151345815.png" style="zoom:33%;"></p><h4 id="核心本质"><a href="#核心本质" class="headerlink" title="核心本质"></a>核心本质</h4><p>计算encoder与decoder state之间的关联性的权重，得到Attention分布，从而对于当前输出位置得到比较重要的输入位置的权重，在预测输出时相应的会占较大的比重。</p><h4 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h4><h5 id="Soft-Attention-和-Hard-Attention"><a href="#Soft-Attention-和-Hard-Attention" class="headerlink" title="Soft Attention 和 Hard Attention"></a>Soft Attention 和 Hard Attention</h5><blockquote><p>Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., … &amp; Bengio, Y. (2015, June). <strong>Show, attend and tell: Neural image caption generation with visual attention</strong>. In <em>International conference on machine learning</em> (pp. 2048-2057).</p></blockquote><p>任务：Image Caption</p><p>在Image Caption中引入了Attention，当生成第i个关于图片内容描述的词时，用Attention来关联与i个词相关的图片的区域。</p><p><img src="http://img.cdn.leonwang.top/image-20201225152850214.png" style="zoom:33%;"></p><p>论文中使用了两种Attention Mechanism，即Soft Attention和Hard Attention。</p><p>Soft Attention：之前所描述的传统的Attention Mechanism就是Soft Attention。Soft Attention是参数化的（Parameterization），因此可导，可以被嵌入到模型中去，直接训练。梯度可以经过Attention Mechanism模块，反向传播到模型其他部分。</p><p>Hard Attention：Hard Attention是一个随机的过程。Hard Attention不会选择整个encoder的输出做为其输入，Hard Attention会依概率Si来采样输入端的隐状态一部分来进行计算，而不是整个encoder的隐状态。为了实现梯度的反向传播，需要采用蒙特卡洛采样的方法来估计模块的梯度。</p><p>评价：两种Attention Mechanism都有各自的优势，但目前更多的研究和应用还是更倾向于使用Soft Attention，因为其可以直接求导，进行梯度反向传播。</p><h5 id="Global-Attention-和-Local-Attention"><a href="#Global-Attention-和-Local-Attention" class="headerlink" title="Global Attention 和 Local Attention"></a>Global Attention 和 Local Attention</h5><blockquote><p>Luong, M. T., Pham, H., &amp; Manning, C. D. (2015). <strong>Effective approaches to attention-based neural machine translation</strong>. <em>arXiv preprint arXiv:1508.04025</em>.</p></blockquote><p><strong>Global Attention：</strong>和传统的Attention model一样。所有的hidden state都被用于计算Context vector 的权重，即变长的权重向量$a_t$的其长度等于encoder端输入句子的长度。</p><p>Global Attention有一个明显的缺点，每一次encoder端的所有hidden state都要参与计算，这样做计算开销会比较大，特别是当encoder的句子偏长。</p><p><strong>Local Attention：</strong>Local Attention是一种介于Kelvin Xu所提出的Soft Attention和Hard Attention之间的一种Attention方式，即把两种方式结合起来。其结构如图6所示。</p><p>Local Attention首先会为decoder端当前的词，<strong>预测一个source端对齐位置（aligned position）$p_t$</strong>，然后基于$p_t$选择一个窗口，用于计算背景向量$c_t$。</p><p><img src="http://img.cdn.leonwang.top/image-20201225154657676.png" style="zoom: 33%;"></p><p>评价：Global Attention和Local Attention各有优劣，在实际应用中，Global Attention应用更普遍，因为local Attention需要预测位置向量$p_t$，这就带来两个问题：</p><ol><li>当encoder句子不是很长时，相对Global Attention，计算量并没有明显减小。</li><li>位置向量$p_t$的预测可能不准确。</li></ol><h5 id="Self-Attention-amp-Transformer"><a href="#Self-Attention-amp-Transformer" class="headerlink" title="Self Attention &amp; Transformer"></a>Self Attention &amp; Transformer</h5><blockquote><p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … &amp; Polosukhin, I. (2017). <strong>Attention is all you need.</strong> In <em>Advances in neural information processing systems</em> (pp. 5998-6008).</p></blockquote><h6 id="QKV"><a href="#QKV" class="headerlink" title="QKV"></a>QKV</h6><p>query：目标向量</p><p>key-value：X/X的矩阵变换</p><h6 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self Attention"></a>Self Attention</h6><p>QKV角度理解：</p><p>KQV分别通过不同的矩阵变换映射到不同的⼦空间中，区别在于以前共享⼀个ctx的Query，self-attention中的Query也不共享了。</p><p>Self Attention与传统的Attention机制的区别：</p><ol><li><p>传统的Attention是基于source端和target端的隐变量（hidden state）计算Attention的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。</p></li><li><p>Self Attention分别在source端和target端进行，仅与source input或者target input自身相关的Self Attention，捕捉source端或target端自身的词与词之间的依赖关系；然后再把source端的得到的self Attention加入到target端得到的Attention中，捕捉source端和target端词与词之间的依赖关系。</p><p> <img src="https://pic2.zhimg.com/v2-ee0044cda391d9a8f6775e74aee972bd_r.jpg" alt="preview" style="zoom: 50%;"></p><p> 因此，self Attention Attention比传统的Attention mechanism效果要好，主要原因之一是，传统的Attention机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系</p></li></ol><p>Self-attention 计算过程：</p><p><img src="http://img.cdn.leonwang.top/70d4600934a1a5513296c5561bc75962.jpg" alt=""></p><h6 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h6><p>不依赖基于RNN的Seq2Seq模型，而是直接用 attention 建模。</p><p>模型结构：</p><p><img src="http://img.cdn.leonwang.top/image-20201225160547407.png" style="zoom:33%;"></p><p>左边是encoder (N=6)，右边是decoder (N=6)。</p><p><strong>Encoder部分</strong></p><p>每个 Encoder Layer 包含两个 sub-layer：</p><p><strong>sub-layer-1：multi-head self-attention mechanism</strong>，用来进行 self-attention。</p><p><strong>sub-layer-2：Position-wise Feed-forward Networks</strong>，简单的全连接网络，对每个 position 的向量分别进行相同的操作，包括两个线性变换和一个 ReLU 激活输出（输入输出层的维度都为 512，中间层为 2048）：<br>$$<br>\mathrm{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}<br>$$<br>每个 sub-layer 都使用了残差连接</p><p><strong>Decoder部分</strong></p><p>每个 Decoder Layer 包含 3 个 sub-layer：</p><p><strong>sub-layer-1：Masked multi-head self-attention mechanism</strong>，用来进行self-attention，与Encoder不同：由于是序列生成过程，所以在时刻 i 的时候，大于 i 的时刻都没有结果，只有小于 i 的时刻有结果，因此需要做Mask。</p><p><strong>sub-layer-2：Position-wise Feed-forward Networks</strong>，同Encoder。</p><p><strong>sub-layer-3：Encoder-Decoder attention计算</strong>。</p><p><strong>Encoder和Decoder中的Multi-Head Attention单元</strong></p><p>每一个Multi-Head Attention单元由多个结构相似的Scaled Dot-Product Attention单元组成：</p><p><img src="http://img.cdn.leonwang.top/image-20201225163115469.png" style="zoom:33%;"></p><p>Transformer 中的 self-attention 就是在 Scaled Dot-Product Attention 单元中实现的：</p><p>如上图左图所示，首先把输入Input经过线性变换分别得到Q、K、V，注意，Q、K、V都来自于Input，只不过是线性变换的矩阵的权值不同而已。然后把Q和K做dot Product相乘，得到输入Input词与词之间的依赖关系，然后经过尺度变换（scale）、掩码（mask）和softmax操作，得到最终的Self Attention矩阵。尺度变换是为了防止输入值过大导致训练不稳定，mask则是为了保证时间的先后关系。</p><p>最后，把encoder端self Attention计算的结果加入到decoder做为K和V，结合decoder自身的输出做为Q，得到encoder端的attention与decoder端attention之间的依赖关系。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> Common </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Message Passing</title>
      <link href="2020/12/25/Message-Passing/"/>
      <url>2020/12/25/Message-Passing/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><blockquote><p>Gilmer, Justin, et al. “Neural message passing for quantum chemistry.” <em>arXiv preprint arXiv:1704.01212</em> (2017).</p></blockquote><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>本文对现有的模型抽象出它们的共性，得到了在图上进行监督学习的通用框架，称为 Message Passing Neural Networks (MPNNs) 的框架，同时利用 MPNN 框架，以量子化学为例，根据原子的性质（对应节点特征）和分子的结构（对应边特征）预测了13种物理化学性质。</p><p>MPNN 不是一个模型，而是一个框架。</p><p>用MPNN代替DFT来预测有机分子结构量子特性，比较二者的时间开销：</p><p><img src="http://img.cdn.leonwang.top/image-20201225112845116.png" alt="image-20201225112845116"></p><h2 id="MPNN-Framework"><a href="#MPNN-Framework" class="headerlink" title="MPNN Framework"></a>MPNN Framework</h2><p>本文定义了 MPNN 这一通用框架，并通过八篇文献来举例验证 MPNN 框架的通配性。</p><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>无向图 ，节点 v 的特征为 ，边的特征为 。</p><p>前向传递有两个阶段：</p><ol><li><strong>「消息传递阶段」</strong>（Message Passing）</li><li><strong>「读出阶段」</strong>（Readout）</li></ol><p>考虑消息传递阶段，消息函数定义为 ，顶点更新函数定义为 ，t 为运行的时间步。在消息传递过程中，隐藏层节点 v 的状态  可以被基于  进行更新：</p><p>其中， 表示图 G 中节点 v 的邻居。</p><p>读出阶段使用一个读出函数 R 来计算整张图的特征向量：</p><p>消息函数 ，向量更新函数  和读出函数  都是可微函数。</p><p> 作用于节点的状态集合，同时对节点的排列不敏感，这样才能保证 MPNN 对图同构保持不变。</p><p>此外，我们也可以通过引入边的隐藏层状态来学习图中的每一条边的特征，并且同样可以用上面的等式进行学习和更新。</p><h3 id="适配"><a href="#适配" class="headerlink" title="适配"></a>适配</h3><p>可以通过定义<strong>「消息函数」</strong>、<strong>「更新函数」</strong>和<strong>「读出函数」</strong>来适配不同种模型。本文举了 8 个例子：</p><p><strong>「Paper 1」</strong> : Convolutional Networks for Learning Molecular Fingerprints, Duvenaud et al. (2015)</p><p>消息函数：，其中  表示拼接（concat）</p><p>更新函数：，其中  为 sigmoid 函数， 表示节点 v 的度， 是一个可学习的矩阵，t 为时间步，N 为节点度；</p><p>读出函数： R 将先前所有隐藏层的状态  进行连接：，其中 f 是一个神经网络， 是一个可学习的读出矩阵。</p><p>这种消息传递阶段可能会存在一些问题，比如说最终的消息向量分别对连通的节点和连通的边求和 。可见，该模型实现的消息传递无法识别节点和边之间的相关性。</p><p><strong>「Paper 2」</strong> : Gated Graph Neural Networks (GG-NN), Li et al. (2016)</p><p>GG-NN 这篇论文比较有名，作者后续也是在这个模型的基础上进行改进的。</p><p>消息函数：，其中  是  的一个可学习矩阵，每条边都会对应一个矩阵；</p><p>更新函数：，其中 GRU 为门控制单元（Gate Recurrent Unit）。该工作使用了权值捆绑，所以在每一个时间步 t 下都会使用相同的更新函数；</p><p>读出函数： R 为 ，其中 i 和 j 为神经网络， 表示元素相乘。</p><p><strong>「Paper 3」</strong> : Interaction Networks, Battaglia et al. (2016)</p><p><strong>「Paper 4」</strong> : Molecular Graph Convolutions, Kearnes et al. (2016)</p><p><strong>「Paper 5」</strong> : Deep Tensor Neural Networks, Schutt et al. (2017)</p><p><strong>「Paper 6」</strong> : Laplacian Based Methods, Bruna et al. (2013); Defferrard et al. (2016); Kipf &amp; Welling (2016)</p><p>基于拉普拉斯矩阵的方法将图像中的卷积运算扩展到网络图 G 的邻接矩阵 A 中。</p><p>在 Bruna et al. (2013); Defferrard et al. (2016); 的工作中，消息函数为：，其中，矩阵  为拉普拉斯矩阵 L 的特征向量组成的矩阵；</p><p>节点的更新函数为：，其中，σ 为非线性的激活函数，比如 ReLU。</p><p>在 Kipf &amp; Welling (2016) 的工作中，消息函数为：，其中，</p><p> ；</p><p>节点的更新函数为：</p><p>可以看到以上模型都是 MPNN 框架的不同实例，所以作者呼吁大家应该致力于将这一框架应用于某个实际应用，并根据不同情况对关键部分进行修改，从而引导模型的改进，这样才能最大限度的发挥模型的能力。</p><h2 id="个人总结"><a href="#个人总结" class="headerlink" title="个人总结"></a>个人总结</h2><ol><li>本文首先从已有的图模型中提取共性，抽象了 MPNN 通用框架。MPNN 的前向传播过程包括 Message Passing 和 Readout 两个阶段。<ul><li>Message Passing阶段中有两个关键函数，分别是消息函数和更新函数。该过程执行多步，Train的过程。</li><li>Readout阶段有一个关键函数，即读出函数。该阶段用来读出整图的 Embedding。</li></ul></li><li>从 MPNN 框架到具体图模型的过程是一个特化的过程，更具体地来说，就是给出 消息函数、更新函数、读出函数的定义。文中给出了8个例子（第6个例子包括了三代GCN），其中包括了之前看过的GGNN和三代GCN。</li><li>本文基于 MPNN 框架应用于化学分子结构预测任务，提出 MPNN 的变体，来探索MPNN框架下的各种模型方案。</li><li>通过把 读出函数 设计为序列无关，从而实现了不受同构图的影响。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> Graph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Graph Embedding </tag>
            
            <tag> Message Passing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图结构学习</title>
      <link href="2020/12/11/%E5%9B%BE%E7%BB%93%E6%9E%84%E5%AD%A6%E4%B9%A0/"/>
      <url>2020/12/11/%E5%9B%BE%E7%BB%93%E6%9E%84%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h1 id="Part-1-时空图中的图结构学习"><a href="#Part-1-时空图中的图结构学习" class="headerlink" title="Part 1: 时空图中的图结构学习"></a>Part 1: 时空图中的图结构学习</h1><h2 id="STGCN"><a href="#STGCN" class="headerlink" title="STGCN"></a>STGCN</h2><blockquote><p>Yu, B., Yin, H., &amp; Zhu, Z. (2017). <strong>Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting</strong>. <em>arXiv preprint arXiv:1709.04875</em>.</p><p><a href="https://github.com/Knowledge-Precipitation-Tribe/STGCN-keras/tree/master/ppt" target="_blank" rel="noopener">https://github.com/Knowledge-Precipitation-Tribe/STGCN-keras/tree/master/ppt</a></p></blockquote><p>IJCAI-18</p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>任务：中长期（超过30分钟）的交通流量预测</p><p>预测根据时间长度划分：</p><ul><li>短期预测：5~30分钟</li><li>中长期预测：超过30分钟</li></ul><p>交通流量预测中的常用指标：speed, volume, density。本文主要是对速度进行预测</p><p>任务难点：高度非线性和复杂性，传统方法无法满足中长期预测的需求</p><p>对传统方法和已有的Deep Learning方法的缺陷进行了分析：</p><p><img src="http://img.cdn.leonwang.top/Previous_work1.jpeg" alt="Previous_work1.jpeg"></p><p><img src="http://img.cdn.leonwang.top/Previous_work2.jpeg" alt="Previous_work2.jpeg"></p><h3 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h3><p>把问题形式化为 Graph 上的问题，构建图卷积模型</p><h4 id="构图"><a href="#构图" class="headerlink" title="构图"></a>构图</h4><p>构图方式：</p><ul><li>带权邻接矩阵：把地点作为节点，地点之间的边用带权重的邻接矩阵表示。</li><li>速度矩阵：因为要预测的是每个地点的速度，所以每个地点节点都有速度这个属性。</li></ul><p><img src="http://img.cdn.leonwang.top/image-20201211125258134.png" alt="image-20201211124557898"></p><h4 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h4><p><img src="http://img.cdn.leonwang.top/image-20201211130606189.png" alt="image-20201211124804212"></p><h5 id="Graph-CNNs-for-Extracting-Spatial-Features"><a href="#Graph-CNNs-for-Extracting-Spatial-Features" class="headerlink" title="Graph CNNs for Extracting Spatial Features"></a>Graph CNNs for Extracting Spatial Features</h5><p>首先使用图卷积来捕获空间相关性，本篇论文采用的是谱域卷积中切比雪夫近似与一阶近似后的图卷积公式（第三代GCN）</p><p><img src="http://img.cdn.leonwang.top/image-20201211124557898.png" alt="image-20201211125258134"></p><h5 id="Gated-CNNs-for-Extracting-Temporal-Features"><a href="#Gated-CNNs-for-Extracting-Temporal-Features" class="headerlink" title="Gated CNNs for Extracting Temporal Features"></a>Gated CNNs for Extracting Temporal Features</h5><p>在时间维度上采用门控卷积来捕获时间依赖性</p><p>因为基于RNN的方法很耗时，而CNN比较快，所以采用了在时间维度上进行卷积的方法。时序卷积层与传统的卷积方法不同，其中包含一个一维因果卷积，从而能够考虑到时间序列问题。</p><p>因为我们使用卷积操作，就不用像以前的采用RNN的方法依赖于之前的输出，所以我们可以对数据进行并行计算，这样使得模型训练速度更快。</p><p>而且采用还采用了GLU操作(<a href="https://arxiv.org/pdf/1612.08083.pdf" target="_blank" rel="noopener">Language Modeling with Gated Convolutional Networks</a>)，缓解梯度消失等现象还可以保留模型的非线性能力。</p><h5 id="Spatio-temporal-Convolutional-Block"><a href="#Spatio-temporal-Convolutional-Block" class="headerlink" title="Spatio-temporal Convolutional Block"></a>Spatio-temporal Convolutional Block</h5><p>将以上的图卷积和门控CNN组合成如图所示的结构</p><p><img src="http://img.cdn.leonwang.top/image-20201211124804212.png" alt="image-20201211130606189"></p><p>其中，是图卷积的卷积核，和是两个时间卷积的卷积核</p><p>最终模型采用堆叠两个ST-Conv Block后接输出层的结构</p><p>损失函数：</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>提出了STGCN模型结构，解决时空图预测问题。可以考虑从交通流量场景扩展到其他场景。</p><p>模型的核心结构是时空卷积块，它结合了图卷积和门控时间卷积，分别进行空间和时间的建模。</p><p>图卷积采用的是谱域上的卷积而不是空间域的卷积，更具体来说是第三代GCN（经过切比雪夫近似和一阶近似）</p><h2 id="DGCNN"><a href="#DGCNN" class="headerlink" title="DGCNN"></a>DGCNN</h2><blockquote><p>Diao, Z., Wang, X., Zhang, D., Liu, Y., Xie, K., &amp; He, S. (2019, July). <strong>Dynamic spatial-temporal graph convolutional neural networks for traffic forecasting</strong>. In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> (Vol. 33, pp. 890-897).</p></blockquote><p><a href="https://gengdd.github.io/2019/04/25/Dynamic-Spatial-Temporal-Graph-Convolutional-Neural-Networks-for-Traffic-Forecasting/" target="_blank" rel="noopener">https://gengdd.github.io/2019/04/25/Dynamic-Spatial-Temporal-Graph-Convolutional-Neural-Networks-for-Traffic-Forecasting/</a></p><h3 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h3><p>时空图中节点之间的空间依赖关系往往是动态的，而固定的拉普拉斯矩阵无法捕捉这种变化</p><h3 id="Methodology-1"><a href="#Methodology-1" class="headerlink" title="Methodology"></a>Methodology</h3><p>提出了一种动态的时空GCN，使用动态拉普拉斯矩阵estimator来发现拉普拉斯矩阵的动态变化。</p><p>引入了张量分解的操作，来降低学习的复杂度</p><p>模型框架大体上和第一篇论文一致，注意多出的Laplacian Matrix Estimator和最后空间卷积中维度的变化：</p><p><img src="http://img.cdn.leonwang.top/image-20201211134514347.png" alt="image-20201211132701458"></p><h4 id="Dynamic-Laplacian-Matrix-Estimator"><a href="#Dynamic-Laplacian-Matrix-Estimator" class="headerlink" title="Dynamic Laplacian Matrix Estimator"></a>Dynamic Laplacian Matrix Estimator</h4><p><img src="http://img.cdn.leonwang.top/image-20201211132701458.png" alt="image-20201211133134409"></p><p><strong>包含5个sub-process</strong>(这部分建议去看论文，讲的比较详细)：</p><ol><li>Tensor Decomposition (TDL).</li><li>Unfolding-Normalization.</li><li>2-D Conv.</li><li>Estimator.</li><li>Normalization.</li></ol><p>下面主要总结第一步TDL</p><h4 id="TDL"><a href="#TDL" class="headerlink" title="TDL"></a>TDL</h4><p><img src="http://img.cdn.leonwang.top/image-20201211133134409.png" alt="image-20201211134514347"></p><p>TDL: Tensor Decomposition Layer</p><p>TDL通过 Contraction 和 Recovery 两个操作来提取交通数据的全局和局部component。可以理解为，把图张量G先contract为X，然后把X分解成了global部分Xs和local部分Xe。</p><p>对于输入的G是一个三阶张量，含义如下，本文把c设置为1从而专注于速度这一属性的预测：</p><p><img src="http://img.cdn.leonwang.top/image-20201211142352601.png" alt="image-20201211133558884"></p><p>下标s：global, long-term</p><p>下标e：local, short-term</p><p>TDL预训练</p><h3 id="Conclusion-1"><a href="#Conclusion-1" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>整体的模型框架和第一篇论文是一样的，但引入了Laplacian Matrix Estimator来空间依赖关系的动态变化进行建模，又引入了张量分解操作来降低复杂度。</p><h2 id="Graph-WaveNet"><a href="#Graph-WaveNet" class="headerlink" title="Graph WaveNet"></a>Graph WaveNet</h2><blockquote><p>Wu, Z., Pan, S., Long, G., Jiang, J., &amp; Zhang, C. (2019). <strong>Graph wavenet for deep spatial-temporal graph modeling</strong>. <em>arXiv preprint arXiv:1906.00121</em>.</p></blockquote><p>IJCAI-19</p><h3 id="Motivation-2"><a href="#Motivation-2" class="headerlink" title="Motivation"></a>Motivation</h3><p>现有的时空图模型存在以下缺陷：</p><ol><li>捕获空间依赖性方面：在固定的图上去捕获依赖关系，这种固定图不一定反映真实的依赖关系，而且真实的依赖关系可能由于邻接关系不完整而缺失</li><li>捕获时间依赖性方面：RNNs/CNNs不善于捕获长期的时序</li></ol><h3 id="Methodology-2"><a href="#Methodology-2" class="headerlink" title="Methodology"></a>Methodology</h3><p>提出了2个component：</p><ol><li>提出自适应邻接矩阵，发现应当存在但缺少的邻接关系（潜在的空间依赖关系），通过node embedding学习它</li><li>利用dilated casual convolution (1D因果卷积) 聚合时间特征，感受野随着堆叠层数而指数级增加，从而可以通过增加层数来提升时间序列的长度</li></ol><p>模型框架：</p><p><img src="http://img.cdn.leonwang.top/image-20201211142144166.png" alt="image-20201211140557826"></p><p>模型首先通过左侧的k个 spatial-temporal layers，然后跳连到右侧相加，在经过 relu, linear, relu, linear，得到输出。</p><p>在每个 spatial-temporal layer 中，先经过 Gated TCN (Gated Temporal Conv Module)，后经过GCN，同时有一个残差连接结构。</p><h4 id="自适应邻接矩阵"><a href="#自适应邻接矩阵" class="headerlink" title="自适应邻接矩阵"></a>自适应邻接矩阵</h4><p>不需要先验知识</p><p>通过随机梯度下降进行E2E的学习</p><p><img src="http://img.cdn.leonwang.top/image-20201211145704086.png" alt="image-20201211142144166"></p><p>结合了自适应邻接矩阵后的图卷积层，其中图卷积是基于空间域的，不是基于频谱域的：</p><p><img src="http://img.cdn.leonwang.top/image-20201211142440698.png" alt="image-20201211142352601"></p><h4 id="堆叠1D因果卷积"><a href="#堆叠1D因果卷积" class="headerlink" title="堆叠1D因果卷积"></a>堆叠1D因果卷积</h4><p><img src="http://img.cdn.leonwang.top/image-20201211151834282.png" alt="image-20201211142440698"></p><p>f 卷积核, d 每次跳跃/间隔的距离</p><p>注意感受野的指数级变化</p><p><a href="https://blog.csdn.net/My123456abc/article/details/100875742" target="_blank" rel="noopener">对因果卷积的解释</a></p><h4 id="Gated-TCN"><a href="#Gated-TCN" class="headerlink" title="Gated TCN"></a>Gated TCN</h4><p>为了处理更复杂的时间序列，采用了门控的方式：</p><p>g 双曲正切激活函数， sigmoid函数控制信息传递到下一层的比例</p><h3 id="Conclusion-2"><a href="#Conclusion-2" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>本文针对时空图建模中空间和时间依赖关系不能得到有效学习的问题，分别从空间和时间的角度各提出了一个 component。</p><p>空间侧通过学习自适应邻接矩阵，体现了对确定性图结构的自动学习。</p><p>时间侧通过卷积结构的堆叠来扩大感受野，从而有效学习长期的时间序列。</p><h2 id="Connecting-the-Dots-MTGNN"><a href="#Connecting-the-Dots-MTGNN" class="headerlink" title="Connecting the Dots (MTGNN)"></a>Connecting the Dots (MTGNN)</h2><blockquote><p>Wu, Z., Pan, S., Long, G., Jiang, J., Chang, X., &amp; Zhang, C. (2020). <strong>Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks</strong>. <em>arXiv preprint arXiv:2005.11650</em>.</p></blockquote><p>KDD 2020</p><h3 id="Motivation-3"><a href="#Motivation-3" class="headerlink" title="Motivation"></a>Motivation</h3><p>任务：基于图神经网络的多元时间序列预测</p><p>多元时间序列预测的基本假设：变量之间相互依赖</p><p>现存问题：</p><ol><li>现有的多元时间序列预测方法不能充分利用变量之间的潜在空间相关性</li><li>GNN能够很好地处理图结构数据，但是多元时间序列预测场景中变量之间的关系是事先不确定的，所以不能直接使用GNNs来解决多元时间序列预测问题</li></ol><h3 id="Methodology-3"><a href="#Methodology-3" class="headerlink" title="Methodology"></a>Methodology</h3><p>提出了一种专门针对多变量时间序列数据设计的<strong>通用图神经网络框架</strong>。该方法通过图学习模块自动提取变量间的单向关系，可以方便地集成变量属性等外部知识。</p><p>在此基础上，提出了一种新的<strong>max-hop传播层</strong>和一个<strong>dilated inception层</strong>来捕捉时间序列中的空间和时间依赖关系。图学习、图卷积和时间卷积模块在端到端框架中联合学习。</p><h4 id="端到端框架"><a href="#端到端框架" class="headerlink" title="端到端框架"></a>端到端框架</h4><p><img src="http://img.cdn.leonwang.top/image-20201211145926648.png" alt="image-20201211145704086"></p><h4 id="图学习"><a href="#图学习" class="headerlink" title="图学习"></a>图学习</h4><p>初步思路：通过距离度量（点积或欧式距离）进行构图</p><p>但存在两个问题：</p><ol><li><p>建图的复杂度是，这会导致计算和存储的开销随着图的大小增加而以平方的速度增长。</p><p> 为了解决这个问题，采用了<strong>采样</strong>的方法，只对节点子集中的pair-wise进行计算，从而消除了mini-batch中的计算和存储瓶颈。</p></li><li><p>距离度量是对称的，只能构造无向图，但多元时序预测场景的节点之间的依赖关系通常是单向的，比如交通流量预测，一个地点的变化对另一个地点的影响程度是单向的。因此需要学习单向关系。</p></li></ol><p>考虑解决上述两个问题，最终图学习的方法如下：</p><p><img src="http://img.cdn.leonwang.top/image-20201211145744755.png" alt="image-20201211151834282"></p><p>E1，E2是两个初始化的node embedding矩阵，两个是要学习的参数，是超参</p><p>公式3中 和 体现了对“单向”关系的学习，两式相减后在进行relu激活的目的是进行正则化，如果是正数，则就会正则化为0。</p><p>得到的矩阵 A 就是要学习的邻接矩阵。</p><p>公式5、6让除 top k 个依赖关系最大的节点之外邻居的邻接关系置为0，目的是让邻接矩阵更加稀疏，从而减少后续计算开销。</p><h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p><img src="http://img.cdn.leonwang.top/image-20201218110033410.png" alt="image-20201211145744755"></p><h4 id="图卷积"><a href="#图卷积" class="headerlink" title="图卷积"></a>图卷积</h4><p>（捕获空间依赖）图卷积和mix-hop propagation层：</p><p>图卷积采用了空间域的图卷积</p><p><img src="http://img.cdn.leonwang.top/image-20201211150000215.png" alt="image-20201211145926648"></p><h4 id="时序卷积"><a href="#时序卷积" class="headerlink" title="时序卷积"></a>时序卷积</h4><p>（捕获时间依赖）时间卷积和Dilated inception层：</p><p><img src="http://img.cdn.leonwang.top/image-20201211140557826.png" alt="image-20201211150000215"></p><h3 id="Conclusion-3"><a href="#Conclusion-3" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>对于图学习部分，和论文三一样都是学习邻接矩阵，但细节更多。</p><p>对于后续的时空建模部分，采用的模型结构和前面几个模型有点区别，但仍然是用GCN捕获空间依赖，用因果卷积捕获时间依赖。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>四篇论文都是和交通流量预测相关的（对于第四篇论文，交通流量预测是多元时间序列预测的一个特例）</li><li>前两篇侧重于把图方法应用于交通流量预测，提供了构图、模型的设计思路，进而提供了针对动态空间依赖关系建模的思路。重点在于用Graph解决具体场景的问题应用，不涉及到图结构学习。</li><li>后两篇论文涉及到图结构学习，这里的图结构学习是指对图中relation的学习，或者说对 edge 的学习。这两篇论文都是通过对邻接矩阵的自动学习实现的，区别在于第四篇论文的方法更加全面，包括正则化处理和针对性能的优化：<ol><li>第三篇论文针对空间依赖关系（确定性图结构的学习），提出了自适应邻接矩阵的方法。</li><li>第四篇论文也是学习自适应邻接矩阵，但考虑到了计算存储开销、不对称的单向关系等细节。</li></ol></li><li>学习单向图的方法是设置两个node embedding矩阵，物理含义可以理解为分别表示source node 和 target node。</li><li>(from 3rd and 4th paper) 图学习和图卷积等下游任务一般会采用端到端的模式进行学习</li><li>时空依赖关系建模的通用方法：用GCN捕获空间依赖（基于谱域和基于空间域的GCN都有，前两篇论文基于谱域，后两篇基于空间域），用因果卷积捕获时间依赖（RNN-based方法计算开销高，可能不适用于large scale的图结构数据）</li></ol><h1 id="Part-2"><a href="#Part-2" class="headerlink" title="Part 2"></a>Part 2</h1><h2 id="Spectral-Temporal-Graph-Neural-Network-for-Multivariate-Time-series-Forecasting"><a href="#Spectral-Temporal-Graph-Neural-Network-for-Multivariate-Time-series-Forecasting" class="headerlink" title="Spectral Temporal Graph Neural Network for Multivariate Time-series Forecasting"></a>Spectral Temporal Graph Neural Network for Multivariate Time-series Forecasting</h2><p><img src="http://img.cdn.leonwang.top/image-20201218115040515.png" alt="image-20201218110033410"></p><p>任务：Multivariate time-series forecasting</p><p>任务难点：捕获时序关系式需要同时考虑intra-series temporal correlations和inter-series correlations</p><blockquote><p>needs to consider <strong>both intra-series temporal correlations</strong> and <strong>inter-series correlations</strong> simultaneously</p></blockquote><p>问题：现有方法只是在考虑了预定义先验作为inter-series relationship的情况下，去捕获temporal correlations</p><blockquote><p><strong>only</strong> capture temporal correlations in the time domain and resort to pre-defined priors as <strong>inter-series</strong> relationships.</p></blockquote><p>本文工作：</p><ol><li>propose <strong>Spectral Temporal Graph Neural Network (StemGNN)</strong> to further improve the accuracy of multivariate time-series forecasting</li><li>captures <strong>inter-series</strong> correlations and <strong>temporal</strong> dependencies <em>jointly</em> in the <em>spectral domain</em>. It combines <strong>Graph Fourier Transform (GFT)</strong> which models <strong>inter-series</strong> correlations and <strong>Discrete Fourier Transform (DFT)</strong> which models <strong>tempora</strong>l dependencies in an <strong>end-to-end</strong> framework. After passing through GFT and DFT, the spectral representations hold clear patterns and can be predicted effectively by <strong>convolution and sequential learning</strong> modules. Moreover, StemGNN learns inter- series correlations automatically from the data <strong>without using pre-defined priors</strong>.</li></ol><p><img src="http://img.cdn.leonwang.top/image-20201218111417140.png" alt="image-20201218111417140"></p><p>上图中，下面是StemGNN的总体结构，上面是StemGNN中StemGNN Block的内部结构。</p><p><strong>Latent Correlation Layer用来进行E2E地对 inter-series correlations 自动学习，因此不需要多变量之间依赖关系的先验。自动学习图的结构，即权重矩阵W。利用了 self-attention 机制</strong></p><p>在StemGNN Block中，先经过一个GFT，把结构多元输入graph 转化成谱域时间序列表示(spectral matrix representation)，不同的trend（单变量）可以被分解为正交的时间序列。DFT再把每个单变量时间序列转换到频域中。经过GFT和DFT后，可以更好地被卷积和序列建模。</p><p>forecast模块们和backcast模块们共享Encoder，以获得更好的多元时序表示能力。</p><p>两个输出：Y用于对future value进行预测，X用于以auto-encoding的方式提高对多元时序的表示能力</p><p>最终的loss是把两个输出的loss结合起来</p><p>应用：交通流量预测、COVIN-19</p><h2 id="Handling-Missing-Data-with-Graph-Representation-Learning"><a href="#Handling-Missing-Data-with-Graph-Representation-Learning" class="headerlink" title="Handling Missing Data with Graph Representation Learning"></a>Handling Missing Data with Graph Representation Learning</h2><p><img src="http://img.cdn.leonwang.top/image-20201218124019408.png" alt="image-20201218115040515"></p><p>任务：用Graph的方法解决机器学习中数据缺失的问题</p><p>传统方法：</p><ol><li>feature imputation：基于观测值进行估算。往往有很强的先验假设，无法从下游任务重学习。</li><li>label prediction：下游标签直接从不完整的数据中学习。设计启发式算法，面临 scalability 问题。</li></ol><p>本文工作：</p><ol><li>提出了GRAPE，是一种基于graph的框架，用于进行缺失值的处理，具体来说是进行上述的 feature imputation 和 label prediction 两类任务。</li><li>构图：二部图，两类节点类型，分别是观测值和特征。观察到的特征值是边。</li><li>任务转化：feature imputation被转化为edge-level prediction任务；label prediction被转化为node-level prediction任务。然后使用GNN的方法来解决这两类任务。</li></ol><p><img src="http://img.cdn.leonwang.top/image-20201218124259945.png" alt="image-20201218115935199"></p><p>左上：原始数据和需要预测的label</p><p>左下：构造二部图</p><p>右上：基于Graph的Feature Imputation任务</p><p>右下：基于Graph的Label Prediction任务</p><p>学习过程：</p><p><img src="http://img.cdn.leonwang.top/image-20201218124441855.png" alt="image-20201218124259945"></p><ol><li><p>GNN结构基于GraphSAGE</p></li><li><p>在节点中引入feature信息：label节点初始化为1，feature节点初始化为one hot 向量。</p><p> <img src="http://img.cdn.leonwang.top/image-20201218115935199.png" alt="image-20201218124019408"></p></li><li><p>通过 edge dropout 提高模型泛化能力</p></li></ol><h2 id="Learning-Node-Representations-from-Noisy-Graph-Structures"><a href="#Learning-Node-Representations-from-Noisy-Graph-Structures" class="headerlink" title="Learning Node Representations from Noisy Graph Structures"></a>Learning Node Representations from Noisy Graph Structures</h2><p><img src="http://img.cdn.leonwang.top/image-20201211133558884.png" alt="image-20201218124441855"></p><p>任务：在Node Representation Learning任务中去噪</p><p>问题：图中的噪声对图表示学习的影响很大，且图中的噪声（分布）通常是未知的</p><p>方法：同时 学习noise-free的节点表示 和 消除噪声</p><p><img src="http://img.cdn.leonwang.top/image-20201218131400343.png" alt="image-20201218131400343"></p><p>有两个generator：</p><ol><li>Graph generator：结合图先验知识（比如上图中的同构性、社区结构等），学习图结构</li><li>Noise generator：结合zero-mean高斯分布和adaptive学习方式，学习图中每条边上的noise</li></ol><p>学习过程：两个generator联合训练，最大似然估计</p><h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><ol><li><p>第一篇论文和上周看的基于时空图的多变量时序预测不同，这篇论文不是针对空间域中的时空图进行构图，而是把多变量时序预测问题transfer到spectral domain，使需要预测的变量之间在频谱域上正交，之后进一步通过频谱域上的图卷积和时序学习等操作完成对时序表示和预测任务的学习。</p><p> 模型中涉及到图结构学习的component是Latent Correlation Layer，通过端到端的嵌入学习过程，可以实现在无先验知识的情况下对图结构(weight matrix)的学习，并结合了self-attention。</p></li><li><p>第二篇论文不是处理构图中的数据缺失，而是用Graph的方法解决结构化数据(matrix)中数据确实的问题，把结构化的feature数据和label构造二部图，从而把对缺失数据的预测transfer成了graph上的link prediction问题。感觉这篇文章对图结构学习帮助不大。</p></li><li><p>第三篇论文解决构图去噪声的问题，通过设计graph generator和noise generator并联合学习，实现对noise真实分布的自动学习，最终实现noise-free的节点表示和去噪</p><p> 这篇文章和图结构学习相关，graph generator学习的是边的概率，但是没有排除噪声，noise generator是对边上噪声的学习，两个任务结合起来，就是对图结构的去噪学习</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> Graph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图结构学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Recommender System 模型梳理</title>
      <link href="2020/12/04/Recommender-System-%E6%A8%A1%E5%9E%8B%E6%A2%B3%E7%90%86/"/>
      <url>2020/12/04/Recommender-System-%E6%A8%A1%E5%9E%8B%E6%A2%B3%E7%90%86/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>对推荐系统模型进行了梳理，分为传统推荐模型和深度推荐模型两大类，主要梳理了以下工作：</p><ul><li>传统推荐模型<ul><li>协同过滤CF</li><li>矩阵分解算法MF</li><li>逻辑回归LR</li><li>因子分解机FM, FFM</li><li>GBDT+LR</li></ul></li><li>深度推荐模型<ul><li>AutoRec</li><li>Deep Crossing</li><li>NeuralCF</li><li>PNN</li><li>Wide&amp;Deep</li><li>深度FM</li><li>Attention机制</li><li>序列模型</li><li>强化学习</li></ul></li></ul><h2 id="传统推荐模型"><a href="#传统推荐模型" class="headerlink" title="传统推荐模型"></a>传统推荐模型</h2><h3 id="协同过滤"><a href="#协同过滤" class="headerlink" title="协同过滤"></a>协同过滤</h3><blockquote><p>[1] Goldberg, D., Nichols, D., Oki, B. M., &amp; Terry, D. (1992). <strong>Using collaborative filtering to weave an information tapestry</strong>. <em>Communications of the ACM</em>, <em>35</em>(12), 61-70.</p><p>[2] Linden, G., Smith, B., &amp; York, J. (2003). <strong>Amazon.com recommendations: Item-to-item collaborative filtering</strong>. <em>IEEE Internet computing</em>, <em>7</em>(1), 76-80.</p></blockquote><p>论文1早在1992提出对协同过滤的研究，开发了一种基于协同过滤的邮件筛选系统，用来过滤用户不感兴趣的无用邮件。</p><p>论文2首次把 CF 模型用于推荐领域。</p><p>CF 模型利用 item 和 user 的共现矩阵计算矩阵中的位置元素，方法是通过计算 user 或 item 之间的相似度，分别称为 UserCF 和 ItemCF，通过对最相似的 top k 个 user/item 的评分进行加权平均，作为未知评分的预估值。</p><p>协同过滤的灵活之处主要在于相似度计算的选择，常用的相似度包括：</p><ol><li>余弦相似度<img src="http://img.cdn.leonwang.top/picgo_img/image-20201204140056408.png" alt="image-20201204140056408" style="zoom:40%;"></li><li>Pearson相关系数<img src="http://img.cdn.leonwang.top/picgo_img/image-20201204140211864.png" alt="image-20201204140211864" style="zoom:40%;"></li></ol><blockquote><p>Pearson相关系数可以理解为中心化的余弦相似度，通过减去 user评分(UserCF) / item被评分(ItemCF) 的均值，消除特定 user/item 的 bias 影响</p></blockquote><p>UserCF与ItemCF的比较：</p><p>业界通常采用ItemCF而不是UserCF，因为UserCF主要有两点缺陷：</p><ol><li>相似度矩阵存储空间开销大。相似度矩阵往往是离线计算好的，UserCF的相似度矩阵大小为$N(user)<em>N(user)$，ItemCF为$N(item)</em>N(item)$。user数量往往远多余item数量。</li><li>用户的历史数据向量更加稀疏。</li></ol><h3 id="矩阵分解算法MF"><a href="#矩阵分解算法MF" class="headerlink" title="矩阵分解算法MF"></a>矩阵分解算法MF</h3><blockquote><p>Koren, Y., Bell, R., &amp; Volinsky, C. (2009). <strong>Matrix factorization techniques for recommender systems</strong>. <em>Computer</em>, <em>42</em>(8), 30-37.</p></blockquote><p><strong>Motivation</strong></p><p>解决协同过滤算法的头部效应明显、<strong>泛化能力</strong>较弱的问题</p><p><strong>Methodology</strong></p><p>在协同过滤算法的共现矩阵的基础上，进行矩阵分解，得到每个 user 和 item 的隐向量，即每个 user 和 item 在隐向量空间中的表达。如下图所示，例如要给 Dave 用户推荐视频，则会选择距离 Dave 用户最近的视频 The Lion King 和 Ocean’s 11。可以按照距离由近到远生成推荐列表。</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204144302463.png" alt="image-20201204142822668" style="zoom:50%;"></p><p>MF算法的关键在于如何进行矩阵分解，常用的矩阵分解方法包括：</p><ol><li>特征值分解 Eigen Decomposition</li><li>奇异值分解 Singular Value Decomposition, SVD</li><li><strong>梯度下降 Gradient Descent</strong></li></ol><p>由于特征值分解只能用于方针，并不适用于 user-item 共现矩阵。</p><p>奇异值分解虽然可用于分解 user-item 共现矩阵，但存在下述两点缺陷：1. 奇异值分解要求矩阵稠密，因此需要对共现矩阵的缺失元素进行填充; 2. 传统奇异值分解的计算复杂度为 $O(mn^2)$，对于互联网大规模数据的场景并不适用。</p><p>所以，梯度下降是MF算法最主要的方法。</p><p>MF中的梯度下降：</p><p>损失函数：</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204142822668.png" alt="image-20201204144302463" style="zoom:50%;"></p><p>优化的目标函数是最小化这个损失函数。</p><p>在考虑了每个 user 和 item 的 bias，以及考虑了正则化项（减少过拟合）之后，实际需要优化的目标函数是这样的：</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204144336519.png" alt="image-20201204144003238" style="zoom:50%;"></p><p>梯度下降的过程就是对目标函数求偏导，然后沿梯度的反方向更新参数：</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204144003238.png" alt="image-20201204144336519" style="zoom:50%;"></p><p><strong>评价</strong></p><p>优点：</p><ul><li>泛化能力强</li><li>空间复杂度低，不需要存储相似度矩阵</li><li>更好的扩展性和灵活性，隐向量已经体现出了Embedding的思想，很方便和其他模型进行组合</li></ul><p>缺点：</p><ul><li>仍然存在CF算法的一些局限，用到的数据只有共现矩阵，没有user侧特征、item侧特征、上下文特征等，仍然只利用了协同的思想来推荐</li></ul><h3 id="逻辑回归LR"><a href="#逻辑回归LR" class="headerlink" title="逻辑回归LR"></a>逻辑回归LR</h3><p>LR模型比较经典，作用在推荐问题上，可以和CF、MF算法进行对比，其<strong>优点</strong>主要在于不只依赖共现矩阵的协同信息，而能够利用到用户特征、物品特征、上下文特征。</p><p>LR模型的另一个<strong>重要意义</strong>在于：通过把推荐问题抽象为分类问题，预估样本为正样本(会发生购买、观看等行为)的概率，从而把推荐问题转化成了一个CTR预估问题。</p><p>LR比较经典，只把关键点总结一下：</p><ol><li>（线性变换）用权重矩阵对特征向量进行加权求和</li><li>（非线性变换）把上一步的结果输入 sigmoid 函数，映射到 0~1 区间，作为 CTR 预估值</li><li>常用训练方法：梯度下降法、牛顿法、拟牛顿法</li></ol><p>LR的梯度下降法：</p><ol><li>定义样本的损失函数为 cross entropy</li><li>定义优化的目标函数是最小化所有样本损失乘积（极大似然估计），进一步转化为对数似然</li><li>对参数求偏导</li><li>对参数沿着梯度(偏导结果)反方向进行更新</li></ol><p><strong>评价</strong></p><p>优点：</p><ol><li>相比 CF，MF 算法：<ol><li>引入 user、item、context 等更加丰富的特征</li></ol></li><li>相比于深度学习方法：<ol><li>数学含义的支撑，因变量 y 服从伯努利分布的假设</li><li>可解释性强</li><li>模型简单、开销小，符合工程化的需求</li></ol></li></ol><p>局限：</p><ol><li>模型简单，表达能力不强，无法特征交叉、特征筛选，因此有信息损失</li></ol><h3 id="因子分解机FM"><a href="#因子分解机FM" class="headerlink" title="因子分解机FM"></a>因子分解机FM</h3><blockquote><p>Rendle, S. (2010, December). <strong>Factorization machines</strong>. In <em>2010 IEEE International Conference on Data Mining</em> (pp. 995-1000). IEEE.</p></blockquote><p>背景：暴力的特征组合方式 POLY2 模型。缺点在于导致特征向量变得更加稀疏，权重参数的数量也会从 n 直接上升到 $n^2$。</p><p>FM思想：为了解决上述问题，去为每个特征学习一个隐权重向量，即每个特征的权重的隐式表达。接下来在计算组合特征的权重的时候，利用两个特征的权重向量计算内积，来作为组合特征的权重值。</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204154107622.png" alt="image-20201204154107622" style="zoom:50%;"></p><h3 id="特征域感知因子分解机FFM"><a href="#特征域感知因子分解机FFM" class="headerlink" title="特征域感知因子分解机FFM"></a>特征域感知因子分解机FFM</h3><blockquote><p>Juan, Y., Zhuang, Y., Chin, W. S., &amp; Lin, C. J. (2016, September). <strong>Field-aware factorization machines for CTR prediction</strong>. In <em>Proceedings of the 10th ACM Conference on Recommender Systems</em> (pp. 43-50).</p></blockquote><p>引入了特征域感知 field-aware 的概念。和 FM 的区别主要在于每个特征不只对应一个唯一的权重隐向量，而是对应一组权重隐向量。当一个特征的特征值属于不同的特征域时，会用不同的权重隐向量去计算组合特征的权重。其背后的思想是，当一个特征取得不同的特征值(属于不同的特征域)时，这个特征对于特征交叉的影响是不同的。</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204154938102.png" alt="image-20201204154938102" style="zoom:50%;"></p><p>论文里提到，相比FM，虽然需要学习的权重隐向量的个数变多了，但是每个向量的维度会大大减少，因为一个向量只需要能够表示这个 field 的特征交叉的影响力。</p><h3 id="GBDT-LR"><a href="#GBDT-LR" class="headerlink" title="GBDT+LR"></a>GBDT+LR</h3><blockquote><p>He, X., Pan, J., Jin, O., Xu, T., Liu, B., Xu, T., … &amp; Candela, J. Q. (2014, August). <strong>Practical lessons from predicting clicks on ads at facebook</strong>. In <em>Proceedings of the Eighth International Workshop on Data Mining for Online Advertising</em> (pp. 1-9).</p></blockquote><p><strong>Motivation</strong></p><p>上面的方法，从 POLY2 到 FM 再到 FFM，都只能进行特征的二阶交叉。如果要进行更高阶的特征交叉，会面临组合爆炸和计算复杂度过高的问题。因此，必须要进行特征组合和筛选。Facebook 这篇论文提供了基于 GBDT+LR 组合模型的解决方案。</p><p><strong>Methodology</strong></p><p>首先，利用 GBDT 自动进行特征筛选和组合，进而生成新的离散特征向量，再把这个特征向量当做 LR 模型的输入，预估 CTR。</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204155841350.png" alt="image-20201204155841350" style="zoom:50%;"></p><p>实际上GBDT部分和LR两部分是分开独立训练的，不存在LR部分的梯度如何回传到GBDT的问题。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol><li>发展过程：<ol><li>基于协同：CF -&gt; MF</li><li>基于LR，特征组合的阶数越来越高：<ol><li>经典 LR</li><li>二阶特征组合：POLY2 -&gt; FM -&gt; FFM</li><li>特征自动筛选和组合：GBDT+LR</li></ol></li></ol></li><li>在传统推荐模型中也到处隐含着 Embedding 的思想。从 CF, MF 算法族到 FM 算法族，体现了 feature 的 Embedding；从 POLY2 到 FM，体现了 weight 的 Embedding。Embedding 的优势，我理解主要就在于得到了一种隐式向量化表示，具备泛化能力，降低存储和计算开销。</li><li>从 POLY2 到 FM 是泛化，但或许是 FM 的表达过于泛化了，从 FM 到 FFM 又是特化。</li></ol><h2 id="深度推荐模型"><a href="#深度推荐模型" class="headerlink" title="深度推荐模型"></a>深度推荐模型</h2><h3 id="AutoRec"><a href="#AutoRec" class="headerlink" title="AutoRec"></a>AutoRec</h3><blockquote><p>Sedhain, S., Menon, A. K., Sanner, S., &amp; Xie, L. (2015, May). <strong>Autorec: Autoencoders meet collaborative filtering</strong>. In <em>Proceedings of the 24th international conference on World Wide Web</em> (pp. 111-112).</p></blockquote><p>这篇paper拉开了深度学习方法解决推荐问题的序幕。</p><p>思想：基于 CF 中的共现矩阵的行向量/列向量作为 user 或 item 的特征向量，引入 AutoEncoder 即单隐层神经网络来对 特征向量进行重建，重建后的特征向量就是“该用户对每个item的评分”或者“每个用户对该item的评分”。</p><p>模型结构：</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204161710996.png" alt="image-20201204161627815" style="zoom:50%;"></p><p>优化目标：</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204161733616.png" alt="image-20201204161710996" style="zoom:50%;"></p><p>引入 L2 正则化项的优化目标：</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204161627815.png" alt="image-20201204161733616" style="zoom:50%;"></p><h3 id="Deep-Crossing"><a href="#Deep-Crossing" class="headerlink" title="Deep Crossing"></a>Deep Crossing</h3><blockquote><p>Shan, Y., Hoens, T. R., Jiao, J., Wang, H., Yu, D., &amp; Mao, J. C. (2016, August). <strong>Deep crossing: Web-scale modeling without manually crafted combinatorial features.</strong> In <em>Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</em> (pp. 255-262).</p></blockquote><p>AutoRec 基于 CF共现矩阵 和 AutoEncoder，结构非常简单。</p><p>Deep Crossing 则是第一次完整地解决了从特征工程、稀疏向量稠密化、多层神经网络进行优化目标拟合等一系列深度学习在推荐系统中的应用问题，定义了规范的 “Embedding+多层神经网络” 结构。</p><p>模型结构：</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204164641405.png" alt="image-20201204164641405" style="zoom:50%;"></p><p>Embedding层：把输入特征处理成embedding，主要任务是完成稀疏向量的稠密化</p><p>Stacking层：把不同的Embedding拼接到一起</p><p>Multiple Residual Units层：多层感知机，采用了多层残差网络的实现，实现特征的交叉组合</p><p>Scoring层：为了拟合优化目标，比如CTR这种二分类问题会采用sigmoid函数，多分类问题会采用softmax函数</p><h3 id="NeuralCF"><a href="#NeuralCF" class="headerlink" title="NeuralCF"></a>NeuralCF</h3><blockquote><p>He, X., Liao, L., Zhang, H., Nie, L., Hu, X., &amp; Chua, T. S. (2017, April). <strong>Neural collaborative filtering</strong>. In <em>Proceedings of the 26th international conference on world wide web</em> (pp. 173-182).</p></blockquote><p>CF 与深度学习的融合</p><p>首先，重新从Deep Crossing框架的角度理解“基于矩阵分解的CF”：</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204165315479.png" alt="image-20201204165315479" style="zoom:50%;"></p><p>矩阵分解可以看做Embedding层，计算Embedding内积可以看做Score层，所以基于矩阵分解的CF算法，本身就已经符合Deep Crossing框架中的某些部分。</p><p>在这套框架下，借助神经网络增强模型的表达能力，用 多层神经网络 代替了 内积 操作，就得到了 NeuralCF 模型。</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204165531625.png" alt="image-20201204165531625" style="zoom:50%;"></p><p>更进一步，还可以把不同的网络得到的特征向量拼接起来，整合多个网络。这篇paper给出了一个Neural CF混合模型的例子：</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204170343396.png" alt="image-20201204165820178" style="zoom:50%;"></p><p>亮点总结：</p><ol><li><p>用多层神经网络代替CF中的内积操作，加强了模型的表达能力。</p></li><li><p>（更重要）模型组合的思想。这种不同模型进行拼接、混合、组合的思想是非常重要的，体现了神经网络的灵活性，可以灵活地组合特征去拟合任意函数的能力。</p></li></ol><h3 id="PNN"><a href="#PNN" class="headerlink" title="PNN"></a>PNN</h3><blockquote><p>Qu, Y., Cai, H., Ren, K., Zhang, W., Yu, Y., Wen, Y., &amp; Wang, J. (2016, December). <strong>Product-based neural networks for user response prediction</strong>. In <em>2016 IEEE 16th International Conference on Data Mining (ICDM)</em> (pp. 1149-1154). IEEE.</p></blockquote><p>和 Deep Crossing 进行比较，PNN 的区别在于：用乘积层 Product Layer 代替了 Stacking 层，用 Product 操作代替了拼接操作。</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204165820178.png" alt="image-20201204170343396" style="zoom:50%;"></p><p>乘积层有两部分组成：</p><ol><li>z部分：线性操作部分</li><li>p部分：乘积操作部分，又分为内积操作和外积操作</li></ol><h3 id="Wide-amp-Deep"><a href="#Wide-amp-Deep" class="headerlink" title="Wide&amp;Deep"></a>Wide&amp;Deep</h3><blockquote><p>Cheng, H. T., Koc, L., Harmsen, J., Shaked, T., Chandra, T., Aradhye, H., … &amp; Anil, R. (2016, September). <strong>Wide &amp; deep learning for recommender systems</strong>. In <em>Proceedings of the 1st workshop on deep learning for recommender systems</em> (pp. 7-10).</p></blockquote><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204172036211.png" alt="image-20201204171118311"></p><p>结合 Wide 模型和 Deep 模型。其中：</p><ul><li>Wide 部分体现泛化能力，本质上是LR模型</li><li>Deep 部分体现记忆能力，是深层神经网络</li></ul><p>模型举例：</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204171459860.png" alt="image-20201204171459860" style="zoom:50%;"></p><h3 id="Deep-amp-Cross"><a href="#Deep-amp-Cross" class="headerlink" title="Deep&amp;Cross"></a>Deep&amp;Cross</h3><blockquote><p>Wang, R., Fu, B., Fu, G., &amp; Wang, M. (2017). Deep &amp; cross network for ad click predictions. In <em>Proceedings of the ADKDD’17</em> (pp. 1-7).</p></blockquote><p>对 Wide&amp;Deep 的改进。主要区别在于把 Wide 部分改进成了 Cross 部分。</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204171118311.png" alt="image-20201204171620942" style="zoom:50%;"></p><p>Cross部分使用 多层交叉层Cross Layer 对输入向量进行特征交叉，第 l+1 层交叉层的输出为：</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204171620942.png" alt="image-20201204171948433" style="zoom:50%;"></p><p>可视化为：</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204171948433.png" alt="image-20201204172036211" style="zoom:50%;"></p><h3 id="FM-深度学习"><a href="#FM-深度学习" class="headerlink" title="FM + 深度学习"></a>FM + 深度学习</h3><h4 id="FNN"><a href="#FNN" class="headerlink" title="FNN"></a>FNN</h4><blockquote><p>Zhang, W., Du, T., &amp; Wang, J. (2016, March). Deep learning over multi-field categorical data. In <em>European conference on information retrieval</em> (pp. 45-57). Springer, Cham.</p></blockquote><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204172456031.png" alt="image-20201204172230140" style="zoom:50%;"></p><p>结构上还是经典的 Deep Crossing 模型，主要是解决模型收敛速度慢的问题。</p><p>模型收敛速度慢主要是Embedding层收敛慢，原因在于 Embedding 向量是随机初始化的。</p><p>FNN 用 FM 模型训练好的个特征隐向量来初始化 Embedding 层参数，相当于提前引入了有价值的先验信息，从而加速模型收敛。</p><h4 id="DeepFm"><a href="#DeepFm" class="headerlink" title="DeepFm"></a>DeepFm</h4><blockquote><p>Guo, H., Tang, R., Ye, Y., Li, Z., &amp; He, X. (2017). DeepFM: a factorization-machine based neural network for CTR prediction. <em>arXiv preprint arXiv:1703.04247</em>.</p></blockquote><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204173028372.png" alt="image-20201204172456031" style="zoom:50%;"></p><p>相比 Wide&amp;Deep，用 FM 模型替换了原来的 Wide 部分，目的是加强 Wide 部分特征组合的能力。</p><p>左侧 FM 部分和右侧 Deep 部分共享 Embedding 层。</p><p>DeepFM 和 Deep&amp;Cross比较：</p><p>二者动机相同，都是为了改进 Wide 部分的特征组合能力。Deep&amp;Cross把Wide部分替换成了多层Cross网络，而DeepFM把Wide部分替换成了FM模型。</p><h4 id="NFM"><a href="#NFM" class="headerlink" title="NFM"></a>NFM</h4><blockquote><p>He, X., &amp; Chua, T. S. (2017, August). Neural factorization machines for sparse predictive analytics. In <em>Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</em> (pp. 355-364).</p></blockquote><p>NFM在Embedding层和多层神经网络之间加入了 特征交叉池化层Bi-interaction Pooling Layer，加强了特征交叉。</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204173323863.png" alt="image-20201204173028372" style="zoom:50%;"></p><p>特征交叉池化层的操作：</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204173559029.png" alt="image-20201204173323863" style="zoom:50%;"></p><blockquote><p>感觉这块还只是特征的二阶交叉</p></blockquote><h3 id="Attention机制"><a href="#Attention机制" class="headerlink" title="Attention机制"></a>Attention机制</h3><h4 id="AFM"><a href="#AFM" class="headerlink" title="AFM"></a>AFM</h4><blockquote><p>Xiao, J., Ye, H., He, X., Zhang, H., Wu, F., &amp; Chua, T. S. (2017). Attentional factorization machines: Learning the weight of feature interactions via attention networks. <em>arXiv preprint arXiv:1708.04617</em>.</p></blockquote><p>浙大</p><p>在Pair-wise特征交叉层和输出层之间加入了一个注意力网络。注意力网络的作用是给每组交叉特征提供权重。</p><p>其中，特征交叉层的交叉方法和 NFM 是一致的。</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204172230140.png" alt="image-20201204173559029" style="zoom:50%;"></p><h4 id="DIN"><a href="#DIN" class="headerlink" title="DIN"></a>DIN</h4><blockquote><p>Zhou, G., Zhu, X., Song, C., Fan, Y., Zhu, H., Ma, X., … &amp; Gai, K. (2018, July). Deep interest network for click-through rate prediction. In <em>Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em> (pp. 1059-1068).</p></blockquote><p>阿里巴巴</p><p>面向电商广告推荐场景</p><p>针对用户侧特征和广告侧特种中的id类特征（例如商品id和商铺id），利用attention机制对这些id类特征进行加权。</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204174301831.png" alt="image-20201204174020885" style="zoom:50%;"></p><h3 id="序列模型"><a href="#序列模型" class="headerlink" title="序列模型"></a>序列模型</h3><h4 id="DIEN"><a href="#DIEN" class="headerlink" title="DIEN"></a>DIEN</h4><blockquote><p>Zhou, G., Mou, N., Fan, Y., Pi, Q., Bian, W., Zhou, C., … &amp; Gai, K. (2019, July). Deep interest evolution network for click-through rate prediction. In <em>Proceedings of the AAAI conference on artificial intelligence</em> (Vol. 33, pp. 5941-5948).</p></blockquote><p>阿里对DIN的改进，仍然是面向电商广告推荐场景，新增了对用户兴趣的时序建模。</p><p>体现在对模型结构的改进，即增加了兴趣进化网络（彩色部分），从下到上：</p><ol><li>行为序列层：把原始的id类行为序列转化成Embedding行为序列</li><li><strong>兴趣抽取层</strong>：通过模拟用户的兴趣迁移过程，抽取用户兴趣</li><li><strong>兴趣进化层</strong>：通过在兴趣抽取层基础上加入注意力机制，模拟与当前目标广告相关的兴趣进化过程</li></ol><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204175019739.png" alt="image-20201204174301831" style="zoom:50%;"></p><p>兴趣抽取层基本结构是GRU</p><p>兴趣进化层引入了attention机制</p><h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><h4 id="DRN"><a href="#DRN" class="headerlink" title="DRN"></a>DRN</h4><blockquote><p>Zheng, G., Zhang, F., Zheng, Z., Xiang, Y., Yuan, N. J., Xie, X., &amp; Li, Z. (2018, April). DRN: A deep reinforcement learning framework for news recommendation. In <em>Proceedings of the 2018 World Wide Web Conference</em> (pp. 167-176).</p></blockquote><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204174731501.png" alt="image-20201204174731501" style="zoom:50%;"></p><p>强化学习框架：</p><ol><li>Agent</li><li>Environment</li><li>Action</li><li>Reward</li><li>State</li></ol><p>Agent是推荐系统本身，也是强化学习推荐框架的核心。在DRN框架中，agent是Deep Q-Network (DQN), Q表示Quality，DQN结构如下：</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204174020885.png" alt="image-20201204175019739" style="zoom:50%;"></p><p>学习过程：</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201204175106937.png" alt="image-20201204175106937" style="zoom:50%;"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 推荐广告搜索 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 推荐 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Graph Embedding 深层模型(GNNs系列)</title>
      <link href="2020/11/27/Graph-Embedding-%E6%B7%B1%E5%B1%82%E6%A8%A1%E5%9E%8B-GNNs%E7%B3%BB%E5%88%97/"/>
      <url>2020/11/27/Graph-Embedding-%E6%B7%B1%E5%B1%82%E6%A8%A1%E5%9E%8B-GNNs%E7%B3%BB%E5%88%97/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2 id="论文列表"><a href="#论文列表" class="headerlink" title="论文列表"></a>论文列表</h2><p>[1] Bruna, J., Zaremba, W., Szlam, A., &amp; LeCun, Y. (2013). <strong>Spectral networks and locally connected networks on graphs</strong>. <em>arXiv preprint arXiv:1312.6203</em>.</p><p>[2] Defferrard, M., Bresson, X., &amp; Vandergheynst, P. (2016). <strong>Convolutional neural networks on graphs with fast localized spectral filtering</strong>. In <em>Advances in neural information processing systems</em> (pp. 3844-3852).</p><p>[3] Kipf, T. N., &amp; Welling, M. (2016). <strong>Semi-supervised classification with graph convolutional networks</strong>. <em>arXiv preprint arXiv:1609.02907</em>.</p><p>[4] Li, Y., Tarlow, D., Brockschmidt, M., &amp; Zemel, R. (2015). <strong>Gated graph sequence neural networks</strong>. <em>arXiv preprint arXiv:1511.05493</em>.</p><p>[5] Hamilton, W., Ying, Z., &amp; Leskovec, J. (2017). <strong>Inductive representation learning on large graphs</strong>. In <em>Advances in neural information processing systems</em> (pp. 1024-1034).</p><p>[6] Veličković, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., &amp; Bengio, Y. (2017). <strong>Graph attention networks</strong>. <em>arXiv preprint arXiv:1710.10903</em>.</p><h2 id="梳理"><a href="#梳理" class="headerlink" title="梳理"></a>梳理</h2><h3 id="GNNs分类"><a href="#GNNs分类" class="headerlink" title="GNNs分类"></a>GNNs分类</h3><p>之前在梳理图表示学习时，把模型分为了浅层和深层两大类。</p><p>浅层包括基于矩阵分解和基于随机游走等方法。</p><p>深层模型GNNs也可以再细分成不同的种类，在清华大学孙茂松组的综述Graph Neural Networks: A Review of Methods and Applications中提供了这样的分类方法：</p><p><img src="http://img.cdn.leonwang.top/image-20201127153021707.png" alt="GNNs"></p><p>从上面的 Fig. 2(c) 中可以看出，图卷积神经网络网络GCNs是GNNs中的一个重要分支。</p><p>需要注意的是，“图卷积”不等于“图卷积神经网络”。前者图卷积的本质目的是用来提取拓扑图的空间结构，它有 spatial domain 和 spectral domain 两种主流形式；而后者图卷积神经网络GCNs是前者在深度学习领域(layer-wise)的发展。</p><h3 id="GNN通用形式"><a href="#GNN通用形式" class="headerlink" title="GNN通用形式"></a>GNN通用形式</h3><p><img src="http://img.cdn.leonwang.top/image-20201127130330059.png" alt="GNN" style="zoom:25%;"></p><h3 id="spatial-domain图卷积-amp-spectral-domain图卷积"><a href="#spatial-domain图卷积-amp-spectral-domain图卷积" class="headerlink" title="spatial domain图卷积 &amp; spectral domain图卷积"></a>spatial domain图卷积 &amp; spectral domain图卷积</h3><p>简单比较一下 spatial domain 方法和 spectral domain 方法：</p><ul><li><p>spatial domain：比较直观。找到每个顶点相邻的 neighbors 作为 receptive field，然后进行聚合。</p><p>  <img src="http://img.cdn.leonwang.top/v2-5f9cf5fdeed19b63e1079ed2b87617b4_r.png" alt="spatial domain"></p></li><li><p>spectral domain：借助图谱的理论来实现拓扑图上的卷积操作。研究的时间过程是：首先研究 GSP(graph signal processing) 的学者定义了 graph 上的 Fourier Transformation，进而定义了 graph 上的 convolution，最后与深度学习结合提出了 Graph Convolutional Network。</p></li></ul><h3 id="spectral-domain图卷积"><a href="#spectral-domain图卷积" class="headerlink" title="spectral domain图卷积"></a>spectral domain图卷积</h3><p>由于 spectral domain 方法是 GCN 的理论基础，为了后面更好地解释 GCN 论文，这里对 spectral domain 方法进行稍微具体的介绍：</p><blockquote><p>逻辑思路：介绍图的Laplacian矩阵 -&gt; 介绍图的Laplacian矩阵的谱分解 -&gt; 基于图的Laplacian矩阵，把传统的傅里叶变换到Graph上来，得到Graph的傅里叶变换 -&gt; 基于上一步中对图的傅里叶变换的定义，利用卷积定理，把传统的卷积推广到Graph上来，得到Graph的卷积</p></blockquote><h4 id="图谱理论"><a href="#图谱理论" class="headerlink" title="图谱理论"></a>图谱理论</h4><p>图谱理论是借助图的拉普拉斯矩阵的特征值和特征向量来研究图的性质。</p><h4 id="图的拉普拉斯矩阵"><a href="#图的拉普拉斯矩阵" class="headerlink" title="图的拉普拉斯矩阵"></a>图的拉普拉斯矩阵</h4><p>在图的傅里叶变换和图的卷积的定义中都用到了图的拉普拉斯矩阵。</p><p>图的Laplacian矩阵定义为 $L=D-A$，其中$D$是度矩阵，$A$是邻接矩阵，示例如下：</p><p><img src="http://img.cdn.leonwang.top/image-20201127120909962.png" alt="Laplacial矩阵计算"></p><p>需要说明的是，常用的拉普拉斯矩阵实际上有三种：</p><ol><li>$L=D-A$ 定义的 Laplacian 矩阵更专业的名称叫做 Combinatorial Laplacian</li><li>$L^{s y s}=D^{-1 / 2} L D^{-1 / 2}$ 定义的叫做 Symmetric normalized Laplacian，跟多 GCN 的论文中应用的是这种拉普拉斯矩阵</li><li>$L^{r w}=D^{-1} L$ 定义的叫 Random walk normalized Laplacian，它与 Diffusion 有相似之处，二者只差一个相似矩阵的变换</li></ol><blockquote><p>后面的理论只给结论，推导过程比价复杂，具体参考：<a href="https://www.zhihu.com/question/54504471/answer/332657604" target="_blank" rel="noopener">https://www.zhihu.com/question/54504471/answer/332657604</a></p></blockquote><h4 id="图的拉普拉斯矩阵的谱分解"><a href="#图的拉普拉斯矩阵的谱分解" class="headerlink" title="图的拉普拉斯矩阵的谱分解"></a>图的拉普拉斯矩阵的谱分解</h4><p>$$<br>L=U\left(\begin{array}{lll}<br>\lambda_{1} \\<br>&amp; \ddots \\<br>&amp; &amp; \lambda_{n}<br>\end{array}\right) U^{-1}<br>$$</p><p>由于 U 是正交矩阵，即 $U U^{T}=E$，所以拉普拉斯矩阵的谱分解最终被推导出这样的形式：<br>$$<br>L=U\left(\begin{array}{lll}<br>\lambda_{1} &amp; &amp; \\<br>&amp; \ddots &amp; \\<br>&amp; &amp; \lambda_{n}<br>\end{array}\right) U^{T}<br>$$</p><blockquote><p>矩阵的谱分解、特征分解、对角化都是同一个概念</p></blockquote><h4 id="图的傅里叶变换和逆变换"><a href="#图的傅里叶变换和逆变换" class="headerlink" title="图的傅里叶变换和逆变换"></a>图的傅里叶变换和逆变换</h4><p>定义Graph上的傅里叶变换：<br>$$<br>F\left(\lambda_{l}\right)=\hat{f}\left(\lambda_{l}\right)=\sum_{i=1}^{N} f(i) u_{l}^{*}(i)<br>$$<br>利用矩阵乘法将Graph上的傅里叶变换推广到矩阵形式：<br>$$<br>\left(\begin{array}{c}<br>\hat{f}\left(\lambda_{1}\right) \\<br>\hat{f}\left(\lambda_{2}\right) \\<br>\vdots \\<br>\hat{f}\left(\lambda_{N}\right)<br>\end{array}\right)=\left(\begin{array}{cccc}<br>u_{1}(1) &amp; u_{1}(2) &amp; \ldots &amp; u_{1}(N) \\<br>u_{2}(1) &amp; u_{2}(2) &amp; \ldots &amp; u_{2}(N) \\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>u_{N}(1) &amp; u_{N}(2) &amp; \ldots &amp; u_{N}(N)<br>\end{array}\right)\left(\begin{array}{c}<br>f(1) \\<br>f(2) \\<br>\vdots \\<br>f(N)<br>\end{array}\right)<br>$$<br>即 $f$ 在Graph上傅里叶变换的矩阵形式为： $\hat{f}=U^{T} f$。（这里的U就是拉普拉斯矩阵谱分解中的U）</p><p>定义Graph上的傅里叶逆变换：<br>$$<br>f(i)=\sum_{l=1}^{N} \hat{f}\left(\lambda_{l}\right) u_{l}(i)<br>$$<br>利用矩阵乘法将Graph上的傅里叶逆变换推广到矩阵形式：<br>$$<br>\left(\begin{array}{c}<br>f(1) \\<br>f(2) \\<br>\vdots \\<br>f(N)<br>\end{array}\right)=\left(\begin{array}{cccc}<br>u_{1}(1) &amp; u_{2}(1) &amp; \ldots &amp; u_{N}(1) \\<br>u_{1}(2) &amp; u_{2}(2) &amp; \ldots &amp; u_{N}(2) \\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>u_{1}(N) &amp; u_{2}(N) &amp; \ldots &amp; u_{N}(N)<br>\end{array}\right)\left(\begin{array}{c}<br>\hat{f}\left(\lambda_{1}\right) \\<br>\hat{f}\left(\lambda_{2}\right) \\<br>\vdots \\<br>\hat{f}\left(\lambda_{N}\right)<br>\end{array}\right)<br>$$<br>即 $f$ 在Graph上傅里叶逆变换的矩阵形式为： $f=U \hat{f}$。（这里的U也是拉普拉斯矩阵谱分解中的U）</p><h4 id="图的卷积"><a href="#图的卷积" class="headerlink" title="图的卷积"></a>图的卷积</h4><p>卷积定理：函数卷积的傅里叶变换是函数傅里叶变换的乘积，即对于函数$f(t)$与$h(t)$两者的卷积是其函数傅里叶变换乘积的逆变换：<br>$$<br>f * h=\mathcal{F}^{-1}[\hat{f}(\omega) \hat{h}(\omega)]=\frac{1}{2 \Pi} \int \hat{f}(\omega) \hat{h}(\omega) e^{i \omega t} d \omega<br>$$<br>卷积定理的意义：利用卷积定理可以简化卷积的运算量。对于一个长度为 $n$ 的序列，按照卷积的定义来计算则需要做 2n-1 组对位乘法，即时间复杂度为$O(n^2)$ ；而利用傅立叶变换后，只需要计算一组对位乘法，而且离散傅立叶变换有快速的算法（快速傅立叶变换），所以总的计算复杂度为$O(nlog(n))$  。</p><p>把卷积定理类比到Graph上，并把上面得到的Graph的傅里叶变换定义带入，最终推导出Graph的卷积：<br>$$<br>(f * h)_{G}=U\left(\begin{array}{lll}<br>\hat{h}\left(\lambda_{1}\right) &amp; &amp; \\<br>&amp; \ddots &amp; \\<br>&amp; &amp; \hat{h}\left(\lambda_{n}\right)<br>\end{array}\right) U^{T} f<br>$$<br>（这里的U也是拉普拉斯矩阵谱分解中的U）</p><p>上式的另一种等价形式更常出现在论文中：<br>$$<br>(f * h)_{G}=U\left(\left(U^{T} h\right) \odot\left(U^{T} f\right)\right)<br>$$<br>其中，$\odot$ 表示 Hadamard product（哈达马积），对于两个维度相同的张量进行对应位置的逐元素乘积运算。</p><h3 id="GCNs-Deep-Learning中的图卷积"><a href="#GCNs-Deep-Learning中的图卷积" class="headerlink" title="GCNs (Deep Learning中的图卷积)"></a>GCNs (Deep Learning中的图卷积)</h3><p>Deep Learning 中的 Convolution 是要设计含有trainable共享参数的kernel，从图卷积公式中可以看出，图卷积的参数就是$\operatorname{diag}\left(\hat{h}\left(\lambda_{l}\right)\right)$，所以可以把这个理解为卷积核。</p><ul><li>第一代GCN</li><li>第二代GCN</li><li>第三代GCN</li></ul><p>第一代GCN直接使用上面推导出的频谱图卷积作为卷积核；第二代GCN通过引入K阶多项式和切比雪夫展开近似，避免了特征分解，降低了复杂度；在第二代GCN的基础上进一步优化，通过多层卷积层的叠加，直接取K=2，使得可以构建更深的模型，从而有了第三代GCN。</p><p>现在普遍意义上的GCN，一般是指第三代GCN。</p><h2 id="GCN-1-谱图卷积"><a href="#GCN-1-谱图卷积" class="headerlink" title="GCN-1 (谱图卷积)"></a>GCN-1 (谱图卷积)</h2><blockquote><p>Bruna, J., Zaremba, W., Szlam, A., &amp; LeCun, Y. (2013). <strong>Spectral networks and locally connected networks on graphs</strong>. <em>arXiv preprint arXiv:1312.6203</em>.</p></blockquote><p>第一代GCN就是我们上面推导出的图卷积直接作为了权重，按照之前对卷积核的理解，可以理解为直接把 $\operatorname{diag}\left(\hat{h}\left(\lambda_{l}\right)\right)$ 作为了卷积核。</p><p><img src="http://img.cdn.leonwang.top/image-20201127142230726.png" alt="GCN-1" style="zoom:50%;"></p><p>改写成<img src="https://www.zhihu.com/equation?tex=y+%3D%5Csigma%28%5Cmathbf%7BU%7Dg_%7B%5Ctheta%7D%5Cmathbf%7BU%7D%5ETx+%29+%3D%5Csigma%28%5Cmathbf%7BU%7D%7B+diag%5B%5Ctheta_1%2C...%2C%5Ctheta_n+%5D%7D%5Cmathbf%7BU%7D%5ET%7Bx%7D%29+%5C%5C+%5C%5C" alt="[公式]">的形式就跟前面推导的图卷积保持一致了。</p><h2 id="GCN-2-ChbeyNet"><a href="#GCN-2-ChbeyNet" class="headerlink" title="GCN-2 (ChbeyNet)"></a>GCN-2 (ChbeyNet)</h2><blockquote><p>Defferrard, M., Bresson, X., &amp; Vandergheynst, P. (2016). <strong>Convolutional neural networks on graphs with fast localized spectral filtering</strong>. In <em>Advances in neural information processing systems</em> (pp. 3844-3852).</p></blockquote><p>第一代的图卷积神经网络很巧妙的利用图谱理论来实现拓扑图的卷积操作，但其有很多缺点，比如说：计算复杂度太高，我们需要对拉普拉斯矩阵进行谱分解得到特征向量矩阵 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf+U" alt="[公式]">，时间复杂度为 <img src="https://www.zhihu.com/equation?tex=O%28n%5E3%29" alt="[公式]"> ；就算实现完成谱分解，在做矩阵运算时时间复杂度也高达 <img src="https://www.zhihu.com/equation?tex=O%28n%5E2%29" alt="[公式]"> ；此外，GCN-1 的卷积操作是全局的操作，并不是仅来源于节点的邻域。</p><p>针对这些问题，学者提出了第二代 GCN：ChbeyNet。</p><p>首先，考虑到傅里叶计算函数与特征值密切相关，所以用拉普拉斯矩阵的特征值函数代替傅里叶计算。对于不具备局部连通性和时间复杂度高的缺陷，引入 K 阶多项式。代入卷积运算后，就不再需要乘特征向量矩阵U，而是直接使用拉普拉斯矩阵的k次方。</p><p>此外，还引入了切比雪夫展开式来近似拉普拉斯矩阵的k次方。</p><p>得到卷积变换：</p><p><img src="http://img.cdn.leonwang.top/image-20201127145755967.png" alt="GCN-2卷积运算" style="zoom:50%;"></p><h2 id="GCN-3-semi-GCN"><a href="#GCN-3-semi-GCN" class="headerlink" title="GCN-3 (semi-GCN)"></a>GCN-3 (semi-GCN)</h2><blockquote><p>Kipf, T. N., &amp; Welling, M. (2016). <strong>Semi-supervised classification with graph convolutional networks</strong>. <em>arXiv preprint arXiv:1609.02907</em>.</p></blockquote><p><strong>基本问题</strong></p><p>本文解决的是<strong>半监督</strong>的<strong>节点分类</strong>问题。半监督是指只有一部分节点的label已知。</p><p><strong>传统方法</strong></p><p>在这种情况下，我们可以通过正则化的方式对label信息做平滑，比如在<strong>损失函数中加入Laplacian正则项</strong>：<br>$$<br>\mathcal{L}=\mathcal{L}_{0}+\lambda \mathcal{L}_{\mathrm{reg}}, \quad \text { with } \quad \mathcal{L}_{\mathrm{reg}}=\sum_{i, j} A_{i j}\left|f\left(X_{i}\right)-f\left(X_{j}\right)\right|^{2}=f(X)^{\top} \Delta f(X)<br>$$<br>其中，$\mathcal{L}_{0}$ 是图中的有监督部分的损失，$f(\cdot)$ 是类似神经网络的可微函数，$\lambda$ 是权重系数。$X$ 是节点特征矩阵，$\Delta=D-A$ 是无向图 $\mathcal{G}=(\mathcal{V}, \mathcal{E})$ 未归一化的图拉普拉斯矩阵。$A \in \mathbb{R}^{N \times N}$ 是邻接矩阵，$D_{i i}=\sum_{j} A_{i j}$ 是度矩阵。</p><p><strong>传统方法的缺陷</strong></p><p>上面的损失函数依赖一个假设：图中相邻的节点通常更可能有相同的 label。但实际上<strong>这个假设会限制模型空间</strong>，因为 edge 可能不只可以用来编码节点之间的相似度，还可能会包含其他信息。</p><p><strong>本文方法</strong></p><p>为了克服上述传统方法的缺陷，本文提出了新的GCN模型用于解决半监督节点分类任务。GCN是一个采用 layer-wise propagation 的神经网络模型，只用到有监督部分，不用到上面的正则项，并且是谱图卷积的一阶近似。</p><p><strong>Layer-wise Propagation</strong></p><p>GCN 中的层与层之间的信息传播：<br>$$<br>H^{(l+1)}=\sigma\left(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)}\right)<br>$$<br>其中：</p><ul><li>$\tilde{A}=A+I_{N}$，$A$ 是无向图邻接矩阵，$I_N$ 是单位矩阵，所以 $\tilde{A}$ 可以看成是在原图基础上加了 self-connection 的新图的邻接矩阵。</li><li>$\tilde{D}_{i i}=\sum_{j} \tilde{A}_{i j}$，即 $\tilde{D}$ 是 $\tilde{A}$ 的度矩阵。</li><li>$W^{(l)}$ 是该层的可训练的权重矩阵。</li><li>$\sigma(\cdot)$ 是激活函数，比如 ReLU。</li><li>$H^{(l)} \in \mathbb{R}^{N \times D}$ 是第 $l$ 层的节点表示，第 0 层时 $H^{(0)}$ 初始化为特征矩阵 $X$。</li></ul><p>文章了证明这个公式的形式可以由频谱卷积核的一阶局部近似得来。</p><p><img src="http://img.cdn.leonwang.top/image-20201120134613155.png" alt="image-20201127151425861" style="zoom:50%;"></p><p><strong>前向传播</strong></p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201127151452236.png" alt="image-20201127151452236" style="zoom:50%;"></p><p>两层的GCN，激活函数分别采用ReLU和Softmax。</p><p><strong>交叉熵损失函数</strong></p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201127151757991.png" alt="image-20201127151757991" style="zoom:50%;"></p><h2 id="GGNN"><a href="#GGNN" class="headerlink" title="GGNN"></a>GGNN</h2><blockquote><p>Li, Y., Tarlow, D., Brockschmidt, M., &amp; Zemel, R. (2015). <strong>Gated graph sequence neural networks</strong>. <em>arXiv preprint arXiv:1511.05493</em>.</p></blockquote><p>基于GRU的经典的空间域message passing的模型</p><p><strong>主要贡献</strong></p><p>基于GRU提出了GGNN，利用RNN类似原理实现了信息在graph中的传递。</p><p>update方式为：</p><p><img src="http://img.cdn.leonwang.top/image-20201127151425861.png" alt="GGNN update" style="zoom: 23%;"></p><p>矩阵A的构建：</p><p><img src="http://img.cdn.leonwang.top/image-20201127155640128.png" alt="矩阵A的构建" style="zoom: 33%;"></p><p>注意图(c)表示邻接矩阵A的构建。其中$B, C, B^{\prime}, C^{\prime}$表示边的特征。</p><p><strong>传播模型</strong></p><p><img src="http://img.cdn.leonwang.top/image-20201127163037999.png" alt="image-20201127163037999" style="zoom:50%;"></p><p><img src="https://img-blog.csdnimg.cn/20190413181441495.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x0aGlyZG9uZWw=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:70%;"><br><strong>输出模型</strong><br><img src="http://img.cdn.leonwang.top/image-20201127161147599.png" alt="在这里插入图片描述" style="zoom:70%;"></p><h2 id="GraphSAGE"><a href="#GraphSAGE" class="headerlink" title="GraphSAGE"></a>GraphSAGE</h2><blockquote><p>Hamilton, W., Ying, Z., &amp; Leskovec, J. (2017). Inductive representation learning on large graphs. In <em>Advances in neural information processing systems</em> (pp. 1024-1034).</p></blockquote><p><strong>motivation</strong></p><p>GCN的缺点： Transductive learning的方式，需要把所有节点都参与训练才能得到node embedding，无法快速得到新node的embedding。</p><blockquote><p><strong>直推式(transductive)学习</strong>：从特殊到特殊，仅考虑当前数据。在图中学习目标是学习目标是直接生成当前节点的embedding，例如DeepWalk、LINE，把每个节点embedding作为参数，并通过SGD优化，又如GCN，在训练过程中使用图的拉普拉斯矩阵进行计算，<br><strong>归纳(inductive)学习</strong>：平时所说的一般的机器学习任务，从特殊到一般：目标是在未知数据上也有区分性。</p></blockquote><p>为了解决transductive问题，GraphSAGE提出一种新的算法框架，可以很方便地得到新node的表示。</p><p><strong>基本思想</strong></p><p>去学习一个节点的信息是怎么通过其邻居节点的特征聚合而来的。 学习到了这样的“<strong>聚合函数</strong>”，而我们本身就已知各个节点的特征和邻居关系，我们就可以很方便地得到一个新节点的表示了。</p><p>GCN等transductive的方法，学到的是每个节点的一个唯一确定的embedding； 而GraphSAGE方法学到的node embedding，是根据node的邻居关系的变化而变化的，也就是说，即使是旧的node，如果建立了一些新的link，那么其对应的embedding也会变化，而且也很方便地学到。</p><p><strong>前向传播算法</strong></p><p><img src="http://img.cdn.leonwang.top/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x0aGlyZG9uZWw=,size_16,color_FFFFFF,t_70.png" alt="image-20201127161147599" style="zoom:50%;"></p><p>循环K次（模型有K层），在每一层，对节点v所有邻居节点的Embedding进行aggregate，把聚合出的邻居信息和节点v上一层的embedding进行拼接，乘上该层的trainable权重矩阵进行线性变换，在过一个非线性激活函数得到新一层的节点v的embedding。</p><p>每完成一轮迭代，对所有节点进行一次归一化。</p><p><strong>aggregate函数</strong></p><ol><li>Mean aggregator</li><li>LSTM aggregator</li><li>Pooling aggregator</li></ol><p><strong>模型配置</strong></p><ul><li>不会对所有邻居做聚合，会采样固定个数的邻居，uniform采样</li><li>当 K=2, S1*S2&lt;500 是模型表现最好</li></ul><h2 id="GAT"><a href="#GAT" class="headerlink" title="GAT"></a>GAT</h2><blockquote><p>Veličković, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., &amp; Bengio, Y. (2017). Graph attention networks. <em>arXiv preprint arXiv:1710.10903</em>.</p></blockquote><p>利用注意力机制给不同的邻居分配不同的权重。</p><p><img src="http://img.cdn.leonwang.top/image-20201127154101734.png" alt="GAT"></p><p>其中，h是每个节点的特征。W是权重矩阵，目标节点和它的邻居是共享这个矩阵的。把目标节点和邻居节点线性变换后的特征拼接起来后，乘上a，a是一个可学习的参数。这样就得到了从邻居节点到目标节点的权重。</p><p>对所有邻居都计算这样的权重，然后softmax归一化到[0, 1]，就得到了最终的注意力权重。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://www.zhihu.com/question/54504471/answer/332657604" target="_blank" rel="noopener">https://www.zhihu.com/question/54504471/answer/332657604</a></p><p><a href="https://zhuanlan.zhihu.com/p/120311352" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/120311352</a></p><p><a href="https://dreamhomes.top/545.html" target="_blank" rel="noopener">https://dreamhomes.top/545.html</a></p><p><a href="https://zhuanlan.zhihu.com/p/54505069" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/54505069</a></p><p><a href="https://zhuanlan.zhihu.com/p/62750137" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/62750137</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> Graph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Graph Embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neural Graph Collaborative Filtering</title>
      <link href="2020/11/23/Neural-Graph-Collaborative-Filtering/"/>
      <url>2020/11/23/Neural-Graph-Collaborative-Filtering/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua (2019). <strong>Neural Graph Collaborative Filtering</strong>, <a href="https://dl.acm.org/citation.cfm?doid=3331184.3331267" target="_blank" rel="noopener">Paper in ACM DL</a> or <a href="https://arxiv.org/abs/1905.08108" target="_blank" rel="noopener">Paper in arXiv</a>. In <strong>SIGIR’19</strong>, Paris, France, July 21-25, 2019.</p></blockquote><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>任务：Vector Representation (aka. embeddings)</p><p><strong>Collaborative Signal</strong>：潜藏在 user-item 的交互中的隐式的协同信号</p><p>问题：已有方法（包括矩阵分解、深度学习方法等）在 embedding 过程中没有对关键的 Collaborative Signal 进行明显的利用和编码</p><p>本文提出的 NGCF 框架：</p><ol><li>构图方式：user 和 item 的二部图，edge 表示 user 和 item 之间的交互历史。</li><li>基于上述构图方式，在图上 propagate embeddings 时就能利用图的结构特点对 user-item 之间的交互进行高效的聚合。</li><li>结果：模型对 user-item 中的 <strong>high-order connectivity</strong> 进行了很好的建模，从而吧 user-item 之间的 Collaborative Signal 编码进了 embedding 中。</li></ol><h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><h3 id="构图"><a href="#构图" class="headerlink" title="构图"></a>构图</h3><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201123210549539.png" alt="user-item交互图与高阶连接" style="zoom:50%;"></p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201123210805320.png" alt="NGCF模型结构" style="zoom: 25%;"></p><p>先用 user/item 的 feature 向量作为初始化的 embedding。</p><p>然后通过多层 Embedding Propagation layers 进行信息传播，训练 embedding 并引入高阶交互信息。</p><p>然后把user/item在每层的embedding进行concatenate，作为user/item最终的embedding。</p><p>最后计算 user embedding 和 item embedding 的内积作为 y。</p><p>Loss 函数是 pairwise BPR loss。</p><h3 id="一阶-Propagation"><a href="#一阶-Propagation" class="headerlink" title="一阶 Propagation"></a>一阶 Propagation</h3><p>Embedding 更新公式：<br>$$<br>\mathbf{e}_{u}^{(1)}=\text { LeakyReLU }\left(\mathbf{m}_{u \leftarrow u}+\sum_{i \in \mathcal{N}_{u}} \mathbf{m}_{u \leftarrow i}\right)<br>$$<br>其中，$m_{u \leftarrow i}$ 表示 “message from i to u”：<br>$$<br>\mathbf{m}_{u \leftarrow i}=\frac{1}{\sqrt{\left|\mathcal{N}_{u}\right|\left|\mathcal{N}_{i}\right|}}\left(\mathbf{W}_{1} \mathbf{e}_{i}+\mathbf{W}_{2}\left(\mathbf{e}_{i} \odot \mathbf{e}_{u}\right)\right)<br>$$</p><h3 id="高阶-Propagation"><a href="#高阶-Propagation" class="headerlink" title="高阶 Propagation"></a>高阶 Propagation</h3><p>即对上述一阶 Propagation 的过程进行叠加。</p><p>Embedding 更新公式：<br>$$<br>\mathbf{e}_{u}^{(l)}=\text { LeakyReLU }\left(\mathbf{m}_{u \leftarrow u}^{(l)}+\sum_{i \in \mathcal{N}_{u}} \mathbf{m}_{u \leftarrow i}^{(l)}\right)<br>$$<br>其中：<br>$$<br>\left\{\begin{array}{l}<br>\mathbf{m}_{u \leftarrow i}^{(l)}=p_{u i}\left(\mathbf{W}_{1}^{(l)} \mathbf{e}_{i}^{(l-1)}+\mathbf{W}_{2}^{(l)}\left(\mathbf{e}_{i}^{(l-1)} \odot \mathbf{e}_{u}^{(l-1)}\right)\right) \\<br>\mathbf{m}_{u \leftarrow u}^{(l)}=\mathbf{W}_{1}^{(l)} \mathbf{e}_{u}^{(l-1)}<br>\end{array}\right.<br>$$<br>矩阵形式表示为：<br>$$<br>\mathbf{E}^{(l)}=\operatorname{LeakyReLU}\left((\mathcal{L}+\mathbf{I}) \mathbf{E}^{(l-1)} \mathbf{W}_{1}^{(l)}+\mathcal{L} \mathbf{E}^{(l-1)} \odot \mathbf{E}^{(l-1)} \mathbf{W}_{2}^{(l)}\right)<br>$$<br>$\mathcal{L}$ 表示 Graph 的 Laplacian matrix：<br>$$<br>\mathcal{L}=\mathbf{D}^{-\frac{1}{2}} \mathrm{AD}^{-\frac{1}{2}} \text { and } \mathbf{A}=\left[\begin{array}{cc}<br>\mathbf{0} &amp; \mathbf{R} \\<br>\mathbf{R}^{\top} &amp; \mathbf{0}<br>\end{array}\right]<br>$$<br><img src="http://img.cdn.leonwang.top/picgo_img/image-20201123212732843.png" alt="符号含义" style="zoom:50%;"></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p><a href="https://github.com/xiangwang1223/neural_graph_collaborative_filtering" target="_blank" rel="noopener">https://github.com/xiangwang1223/neural_graph_collaborative_filtering</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>个人感觉传统的矩阵方法中也隐含了交互信息，但是交互信息，尤其是高阶交互信息的表达是很微弱的。NGCF 用构造二部图的方式，结合 GNN 的训练过程，对高阶交互信息进行了显式的、更有效的表达。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 推荐广告搜索 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> Graph </tag>
            
            <tag> 推荐 </tag>
            
            <tag> CF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ProtoBuf编译生成Go代码</title>
      <link href="2020/11/23/ProtoBuf%E7%BC%96%E8%AF%91%E7%94%9F%E6%88%90Go%E4%BB%A3%E7%A0%81/"/>
      <url>2020/11/23/ProtoBuf%E7%BC%96%E8%AF%91%E7%94%9F%E6%88%90Go%E4%BB%A3%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<p>Protobuf即Protocol Buffers，是Google公司开发的一种跨语言和平台的序列化数据结构的方式，是一个灵活的、高效的用于序列化数据的协议。与XML和JSON格式相比，Protobuf更小、更快、更便捷。</p><p>Protobuf是跨语言的，并且自带编译器(protoc)，只需要用protoc进行编译，就可以编译成Java、Python、C++、C#、Go等多种语言代码，并可以直接使用。</p><h2 id="编译工具"><a href="#编译工具" class="headerlink" title="编译工具"></a>编译工具</h2><ol><li>protoc</li></ol><p>ProtoBuf 本身提供了一个编译器，即 protoc。通常我们说安装 Protocol Buffer 指的就是安装编译器 protoc 和一套其支持的官方库文件。</p><p><a href="https://developers.google.com/protocol-buffers/docs/overview" target="_blank" rel="noopener">ProtoBuf 官方 Guides</a></p><p><a href="https://github.com/protocolbuffers/protobuf" target="_blank" rel="noopener">ProtoBuf GitHub 仓库</a></p><p>protoc的安装有以下几种方式：</p><ol><li>源码编译安装</li><li>(推荐) 直接下载 release 版本并 copy 进系统环境</li><li>(Mac optional) 使用 brew 安装</li></ol><p>方法一相对比较费时，需要先安装一些依赖，然后再 build，而且 c++ 的项目 build 起来也比较费时。</p><p>方法二相对更加推荐，直接下载对应操作系统的 release 版本，解压后主要有两部分内容，<code>二进制可执行文件protoc</code>和<code>一套官方标准库</code>，直接 copy 到系统的<code>/bin</code>和<code>/include</code>目录下。</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201123151748180.png" alt="release package" style="zoom: 40%;"></p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201123151904804.png" alt="package structure" style="zoom:50%;"></p><pre><code class="shell">cd ./protoc-3.x.x-osx-x86_64cp -r ./bin/ /usr/local/bincp -r ./include/ /usr/local/include</code></pre><p>方法三是 mac 下另一种比较方便的安装方式，优点是便捷，直接一句 <code>brew install protobuf</code> 搞定，缺点是 brew 默认安装某个版本，如果要安装其他版本，需要克隆 Homebrew-core 仓库查看 commit hash。具体方法参考<a href="https://www.vitah.net/posts/2020/05/use-homebrew-install-elder-version/" target="_blank" rel="noopener">这里</a>。相比方法二要麻烦一些。</p><h2 id="编译Go代码"><a href="#编译Go代码" class="headerlink" title="编译Go代码"></a>编译Go代码</h2><p>因为官方编译工具 protoc 不支持生成 go 代码，所以需要安装额外插件。这里有几个选择：</p><ol><li><p>goprotobuf</p><p>golang官方提供的插件 protoc-gen-go</p><p>GitHub 仓库: <a href="https://github.com/golang/protobuf" target="_blank" rel="noopener">https://github.com/golang/protobuf</a></p><p>安装：</p><pre><code class="shell">go get github.com/golang/protobuf/protoc-gen-go</code></pre><p>编译命令：</p><pre><code class="shell">protoc --go_out=/path/to/output_dir test.proto</code></pre><p>插件版本问题：因为protoc编译出的go代码会因为插件版本而有所不同，具体兼容性未知，理论上来说是向前兼容的。之前实践使用的时候为了保险起见，还是选择了生成和之前相同格式的代码，所以需要对插件版本做降级。降级方法也记录一下：clone 插件代码仓库到 <code>$GOPATH/src</code> ，用git tag查看历史版本，git checkout到对应tag，执行<code>go install ./</code>。</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201123154129849.png" alt="git tag查看历史版本" style="zoom:50%;"></p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201123154211904.png" alt="切换版本并编译"></p></li><li><p>gogoprotobuf</p><p>这里实际上有两个插件可以用，protoc-gen-gogo 和 protoc-gen-gofast。相比比上面的官方插件在速度和编译出的文件大小等方面有一些优势。</p><ul><li>protoc-gen-gogo：和protoc-gen-go生成的文件差不多，性能也几乎一样(稍微快一点点)</li><li>protoc-gen-gofast：生成的文件更复杂，性能也更高(快5-7倍)</li></ul><p>GitHub 仓库地址: <a href="https://github.com/gogo/protobuf" target="_blank" rel="noopener">https://github.com/gogo/protobuf</a></p><p>安装插件：</p><pre><code class="shell">//gogogo get github.com/gogo/protobuf/protoc-gen-gogo//gofastgo get github.com/gogo/protobuf/protoc-gen-gofast</code></pre><p>安装库文件：</p><pre><code class="shell">go get github.com/gogo/protobuf/protogo get github.com/gogo/protobuf/gogoproto  //这个不装也没关系</code></pre><p>编译命令：</p><pre><code>// gogoprotoc --gogo_out=/path/to/output_dir test.proto// gofastprotoc --gofast_out=/path/to/output_dir test.proto</code></pre></li></ol><p>性能测试：</p><p>参考<a href="https://segmentfault.com/a/1190000009277748" target="_blank" rel="noopener">这里</a></p><pre><code>//goprotobuf&quot;编码&quot;：447ns/op&quot;解码&quot;：422ns/op//gogoprotobuf-go&quot;编码&quot;：433ns/op&quot;解码&quot;：427ns/op//gogoprotobuf-fast&quot;编码&quot;：112ns/op&quot;解码&quot;：112ns/op</code></pre>]]></content>
      
      
      <categories>
          
          <category> Infra </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Protocol Buffer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop</title>
      <link href="2020/11/21/Hadoop/"/>
      <url>2020/11/21/Hadoop/</url>
      
        <content type="html"><![CDATA[<h1 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h1>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Relation Inference 论文梳理</title>
      <link href="2020/11/18/Relation-Inference-%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/"/>
      <url>2020/11/18/Relation-Inference-%E8%AE%BA%E6%96%87%E6%A2%B3%E7%90%86/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2 id="论文列表"><a href="#论文列表" class="headerlink" title="论文列表"></a>论文列表</h2><p>[1] T. Chen and R. C.-W. Wong, <strong>Handling Information Loss of Graph Neural Networks for Session-based Recommendation</strong>, KDD, 2020.</p><p>[2] T. N. Kipf, E. Fetaya, K.-C. Wang, M. Welling, and R. Zemel, <strong>Neural Relational Inference for Interacting Systems</strong>, ICML, 2018.</p><p>[3] C. Graber and A. Schwing, <strong>Dynamic Neural Relational Inference for Forecasting Trajectories</strong>, CVPR Workshops, 2020.</p><p>[4] M. Zhang, P. Li, Y. Xia, K. Wang, and L. Jin, <strong>Revisiting Graph Neural Networks for Link Prediction</strong>, arXiv.org, vol. cs.LG. 30-Oct-2020.</p><p>[5] W. Hu, Y. Yang, Z. Cheng, C. Yang, and X. Ren, <strong>Time-Series Event Prediction with Evolutionary State Graph</strong>, WSDM, 2021.</p><p>[6] A. Kazi, L. Cosmo, N. Navab, and M. Bronstein, <strong>Differentiable Graph Module (DGM) for Graph Convolutional Networks</strong>, arXiv.org, vol. cs.LG. 11-Feb-2020.</p><h2 id="1-Handling-Information-Loss-of-Graph-Neural-Networks-for-Session-based-Recommendation"><a href="#1-Handling-Information-Loss-of-Graph-Neural-Networks-for-Session-based-Recommendation" class="headerlink" title="1. Handling Information Loss of Graph Neural Networks for Session-based Recommendation"></a>1. Handling Information Loss of Graph Neural Networks for Session-based Recommendation</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>基于GNN的 session-based 推荐中存在两类信息损失问题：</p><ol><li><strong>lossy session encoding</strong><ul><li>本质：传统构图方式的缺陷 + GNN中aggregate的缺陷</li><li>概念：把 session 编码成 Graph 的过程是有信息损失的，比如节点之间跳转的顺序。</li><li>原因：1. 把 session 编码成 Graph 的方式会损失信息；2. 消息传递的过程中采用了排列无关的聚合方式</li><li>传统构图方式：每个item作为一个节点，item之间的跳转关系作为有向边，边可以是有权的，也可以是无权的</li><li>构图时信息有损的原因：session序列和graph之间不是一对一的，可能会有多个session序列生成相同的图的情况。如果具有相同graph两个session对应不同的next-item，那么GNN是不可能为两个session都学到正确的推荐item的，因为这个信息在构图时就损失了。</li></ul></li><li><strong>ineffective long-range dependency capturing</strong><ul><li>本质：GNN结构中层数的缺陷</li><li>概念：无法有效地捕获到远程的依赖信息</li><li>原因：模型的层数有限</li><li>GNN聚合方式：GNN中的每一层通常只能捕捉到一跳关系。如果模型叠加 n 层，那么也最多能够捕捉到 n 跳的关系。然而模型层数不能任意增加，否则会导致过拟合和过平滑问题。通常来说，GNN的层数不会超过三层。然而，现实世界中很多session的长度都是超过3的，所以GNN的结构对于超过3跳的远距离关系是很难有效捕捉到的。</li></ul></li></ol><h3 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h3><p>针对问题一：</p><ol><li>提出了一个无损的编码方案（构图方案）</li><li>提出了一个基于GRU的能保留边的顺序的 aggregation layer</li></ol><p>针对问题二：</p><ol><li>提出了一个 shortcut graph attention layer，通过沿着 shortcut 连接传播信息，可以有效地捕获远程依赖</li></ol><p>本文通过结合了上述两种 layers 来构造模型，在三个公开数据集上都取得了 SOTA 的表现。</p><p>模型名称：LESSR (Lossless Edge-order preserving aggregation and Shortcut graph attention for Session-based Recommendation)</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201117125556133.png" alt="Workflow of LESSR"></p><p>Workflow of LESSR：给定一个session，首先分别生成对应的 EOP Multigraph 和 ShortcutGraph，分别送到对应的layer作为输入。同时session本身通过embedding layer生成初始的node embedding，通过后续各层，并和EOP Multigraph、Shortcut Graph融合之后，得到最终的node embedding。再通过Readout层计算出graph的表示，然后和recent interests结合得到session embedding。最后送到prediction layer完成下游任务。</p><p>构图：</p><p>S2MG：对于每个节点，对它的入边按照传入的顺序进行编号；最后一个节点会专门标记出来，图上用点线边表示</p><blockquote><p>问题：这样的编码方式和GNN的聚合方式有关，是否有更好的编码方式？</p></blockquote><p>S2SG：序列中每一对有前后关系的pair（不一定相邻），直接通过shortcut连接；每个节点有一个自循环，作用是当在SGAT中传递信息时，可以把update function和aggregate function结合起来（GAT中的常规做法）</p><p>模型结构：</p><p>EOPA：虽然构图保留了边的顺序，但是还需要对GNN中的聚合函数进行改造。因为传统GNN中的聚合函数是permutation-invariant aggregation functions，还是会忽视掉边的相对顺序。方法是使用了 GRU 聚合。</p><p>SGAT：结合S2SG构图方式，通过attention机制来实现信息传播</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>通过构图编码和模型结构两个方面来解决信息损失的问题。</p><p>构图编码：</p><ul><li>S2MG 解决问题一</li><li>S2SG 解决问题二</li></ul><p>模型结构：</p><ul><li>EOPA 解决问题一</li><li>SGAT 解决问题二</li><li>Stacking Multiple Layers</li><li>Generating Session Embedding</li><li>Prediction and Training</li></ul><h2 id="2-Neural-Relational-Inference-for-Interacting-Systems"><a href="#2-Neural-Relational-Inference-for-Interacting-Systems" class="headerlink" title="2. Neural Relational Inference for Interacting Systems"></a>2. Neural Relational Inference for Interacting Systems</h2><p><a href="https://blog.csdn.net/b224618/article/details/81736984" target="_blank" rel="noopener">读书笔记13：Neural relational inference for interacting systems（ICML 2018）</a></p><p><a href="https://swarma.org/?p=20520" target="_blank" rel="noopener">网络重构经典论文：NRI神经关系推理模型 | 论文笔记</a></p><p>实体间的相互作用（关系），即预测图中哪些节点之间是有边的，还可以表示关系的强弱。</p><p>轨迹数据：可以看成是只有节点的图，没有边，但节点是有轨迹的，也就是位置。比如人在运动时，每个关节可以看成一个节点，这些节点都有各自的轨迹，我们就可以通过各个节点的轨迹数据去预测哪些关节之间是有关联的。再比如，球场上每个球员看成一个节点，通过分析每个球员的轨迹，就可以去预测哪些球员之间是有关联的。</p><p>通过时间序列数据学习网络结构，并且同时学习网络演化的动力学，即预测网络中节点在未来时刻的状态。</p><ul><li>输入：节点时间序列数据</li><li>输出：网络结构，和未来时刻的节点状态</li></ul><p>输入的是时序的节点轨迹数据，可以理解为图上只有节点和节点在不同时刻的位置，但是不知道节点的边。</p><p>输出包括两部分，所谓数据结构，实际上是只节点之间的relation，即预测出图中节点间的边；所谓的动力学特征，即根据当前时刻节点的轨迹位置和预测出的节点间的关系，去进一步预测下一时刻各个节点的轨迹位置。</p><p>模型结构：</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201120130841588.png" alt="image-20201120130841588"></p><p>模型主要分为两个部分，encoder 和 decoder，分别完成上述两个任务：</p><p>encoder 做的事情：是根据输入的节点序列，生成邻接矩阵，即网络结构；</p><p>decoder 做的事情：是根据某一时刻的节点状态，加上邻接矩阵信息，输出下一时刻的节点状态。</p><p>用自己的话来说，encoder通过轨迹数据（只有节点，没有边）来预测关系的概率分布（图中的边），decoder则根据概率分布，并结合当前时刻的轨迹，来重构新的轨迹，实际上就是预测下个时刻的轨迹。</p><p>可以预测实体间的<strong>静态关系</strong>，并提供可解释性的表示用于轨迹预测</p><h2 id="3-Dynamic-Neural-Relational-Inference-for-Forecasting-Trajectories"><a href="#3-Dynamic-Neural-Relational-Inference-for-Forecasting-Trajectories" class="headerlink" title="3. Dynamic Neural Relational Inference for Forecasting Trajectories"></a>3. Dynamic Neural Relational Inference for Forecasting Trajectories</h2><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>背景：Neural Relational Inference （上面的论文2）</p><p>上面论文2中提出的NRI模型，可以根据给出的时序轨迹数据来预测节点之间的关系和未来的轨迹。但NRI模型存在一点限制约束，它假设了节点之间的关系是静态的，即不论任何时刻，节点之间的关系都是一样的，但实际上在真实场景下，节点之间的关系往往是动态变化的。这篇论文就是为了解决这个问题。</p><p>本文：Dynamic Neural Relational Inference </p><p><a href="https://www.youtube.com/watch?v=LAR7969fZm8" target="_blank" rel="noopener">YouTube讲解</a></p><p>实体间的关系通常是<strong>动态的</strong>，所以本文提出了 Dynamic Neural Relational Inference (dNRI)，结合sequential latent variable models中的见解，来预测每个时间步上的关系图。</p><p>从观察到的轨迹中很难表示和恢复其中的动态时序关系，一部分原因是没有（很难提取）ground truth标签，即很难确定实体间的相互作用。</p><h3 id="Methodology-1"><a href="#Methodology-1" class="headerlink" title="Methodology"></a>Methodology</h3><p>NRI，把系统关系形式化表示为潜在变量模型，每个潜在变量表示了实体间关系的强弱。</p><p>然而本文模型dNRI能够预测每个时间点上实体间关系的强弱。</p><p>同时借鉴了时序潜在变量模型，将其应用于NRI框架，来学习先验序列关系和后验近似关系，同时考虑了历史和未来的变量状态。</p><p>具体方法：相比于NRI，<strong>设计了新的 encoder、decoder和prior</strong>。这是受到了 sequential latent variable modeling 的启发。</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201120131057488.png" alt="image-20201120131057488" style="zoom:50%;"></p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201120131123085.png" alt="image-20201120131123085"></p><p>Input: 轨迹</p><p>Output：动态的节点关系，和对未来轨迹的预测</p><h3 id="Conclusion-1"><a href="#Conclusion-1" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>本文关注实体间的关系，背景是已有Neural Relational Interface方法来预测静态关系，但现实中更多关系是动态变化的，所以本文提出了动态的NRI，并用于改善轨迹预测的问题。</p><p>所谓静态动态，静态是指节点之间的关系是不随着时间变化的，即网络结构是静态的，而不是说节点的状态是静态的。毕竟是时序数据，节点的状态肯定是随时间变化的。而动态是指节点间的关系也是随时间变化的。</p><h2 id="4-Revisiting-Graph-Neural-Networks-for-Link-Prediction"><a href="#4-Revisiting-Graph-Neural-Networks-for-Link-Prediction" class="headerlink" title="4. Revisiting Graph Neural Networks for Link Prediction"></a>4. Revisiting Graph Neural Networks for Link Prediction</h2><h3 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h3><p>Link Prediction的目的：根据图中已有的节点和边，预测潜在的边。</p><p>Link Prediction的两个GNN代表性研究：</p><ul><li><strong>GAE</strong>(Graph Autoencoder): 基于source node和target node的node embedding进行aggregate</li><li><strong>SEAL</strong>: 提取source node和target node周围的一个子图，标记子图中的节点，然后用GNN从有标记的子图中学习link prediction</li></ul><p>本文工作：对比了两种方法之间的区别。得出结论，简单地聚合node embedding（方法一）并不能产生有效的link表示，但是link周围的有标记的子图（方法二）可以提供高可表达的、可泛化的link表示。即SEAL优于GAE。</p><p>GAE方法不能学到structural链接表示</p><h3 id="Conclusion-2"><a href="#Conclusion-2" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>本文是对基于GNN的link prediction的两种主要方法的对比和分析，最终结论是SEAL方法在进行link representation方面要优于GAE方法。</p><h2 id="5-Time-Series-Event-Prediction-with-Evolutionary-State-Graph"><a href="#5-Time-Series-Event-Prediction-with-Evolutionary-State-Graph" class="headerlink" title="5. Time-Series Event Prediction with Evolutionary State Graph"></a>5. Time-Series Event Prediction with Evolutionary State Graph</h2><h3 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h3><p>时序数据下状态之间的转换，看作是状态图上的有向边。</p><p>Node: states</p><p>Edge: evolving relations（状态之间的转换，具体的形式比如是发生了某个事件）</p><p>本文提出 <strong>evolutionary state graph</strong>，是一种动态图结构，目的是系统地表示状态（节点）之间随时间变化的关系</p><p>通过对动态图结构分析，发现图结构的变化可以表示时间的发生。</p><p>受此启发，提出了一种新的GNN模型，即<strong>Evolutionary State Graph Network (EvoNet)</strong>，来对演化状态图进行编码，从而实现对时间序列事件的预测。</p><p>Task: 事件预测</p><p>Segment: 一个时序segment，是一些state（节点）的组合</p><blockquote><p>Segment可以理解为子图吗？</p></blockquote><h3 id="Methodology-2"><a href="#Methodology-2" class="headerlink" title="Methodology"></a>Methodology</h3><p>evolutionary state graph：</p><p>有权有向图的<strong>时间序列</strong></p><blockquote><p>图中权重的理解？</p></blockquote><p><strong>Evolutionary State Graph Network (EvoNet)</strong>：</p><p>利用一下两类信息：</p><ul><li>Local structural influence（快跑-&gt;停止运动-&gt;晕倒；慢跑-&gt;停止运动-&gt;身体监控数据更加正常）</li><li>Temporal influence 先前已经发生的的状态转换（不只是局部）也会有影响</li></ul><p>结构：</p><ul><li>Local information aggregate</li><li>Temporal Graph propagation</li></ul><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201118104509237.png" alt="image-20201118104509237"></p><p>算法过程：</p><ol><li>首先识别每个segment的state，从而构建evolutionary state graph</li><li>然后定义每个node的Representation vector，和graph的Representation vector</li><li>EvoNet通过信息传递的方式去聚合局部信息，并通过循环的EvoBLock来引入时序信息</li><li>最后，EvoNet用学到的表示来完成预测任务</li></ol><p>实例：流量异常检测</p><h3 id="Conclusion-3"><a href="#Conclusion-3" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>为了解决对演进的状态图中link的预测，即对事件的预测（因果关系），本文从 图结构 和 模型 两个方面提出了解决方案。</p><h2 id="6-Differentiable-Graph-Module-DGM-for-Graph-Convolutional-Networks"><a href="#6-Differentiable-Graph-Module-DGM-for-Graph-Convolutional-Networks" class="headerlink" title="6. Differentiable Graph Module (DGM) for Graph Convolutional Networks"></a>6. Differentiable Graph Module (DGM) for Graph Convolutional Networks</h2><h3 id="Motivation-2"><a href="#Motivation-2" class="headerlink" title="Motivation"></a>Motivation</h3><p>目前图神经网络的一个局限性：transductive setting假设</p><p>One of the limitations of the majority of current graph neural network architectures is that they are often restricted to the <strong>transductive</strong> setting and <strong>rely on the assumption that the underlying graph is known and ﬁxed</strong>.</p><h3 id="Methodology-3"><a href="#Methodology-3" class="headerlink" title="Methodology"></a>Methodology</h3><p>Differentiable Graph Module (DGM)：</p><p>input：node feature、the set of edges</p><p>output：new set of edges</p><p>DGM的结构：</p><p><img src="http://img.cdn.leonwang.top/image-20201118004516041.png" alt="image-20201118004516041"></p><p>它可以分为3部分，</p><p>a）图表示特征学习$f_{\theta}$</p><p>b）概率图生成器：先生成完全连接的图并计算概率</p><p>c）图采样：k-NN规则的随机松弛</p><p>DGM和GCN的结合：</p><p><img src="http://img.cdn.leonwang.top/image-20201118004432382.png" alt="image-20201118004432382"></p><h3 id="Conclusion-4"><a href="#Conclusion-4" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>提出了一种 DGM 的结构来进行 link prediction，主要包括“特征学习 -&gt; 概率计算 -&gt; 采样”这三个步骤。并且，本文把 DGM 结构和 GCN 结合，来解决 GCN 只能用于 tansductive 而不能用于inductive 的缺陷。</p><p>总而言之，DGM 解决了图卷积网络中图结构学习的挑战，允许对离散图进行采样，从而在任何图卷积操作中使用。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="论文总结"><a href="#论文总结" class="headerlink" title="论文总结"></a>论文总结</h3><p>这六篇论文主题都和建图/relation Inference/link prediction相关。</p><ul><li>其中第1篇论文只是针对session-based rec场景下传统建图方式信息有损的问题，提出了一种新的建图方式，并且需要对应地对模型结构进行修改。目的是信息无损。最终的任务是得到图的表示，然后进行图分类。</li></ul><p>但论文1中提到的建图方式其实只是一种手工设计好的，并经过证明是信息无损的建图方式，不是对图结构的自动发现和学习。后面5篇论文从不同角度、不同场景提出了一些自动结构发现的方法。</p><ul><li>第2、3篇论文是连贯的，都是针对轨迹数据进行 relation Inference 和预测未来时刻的状态，区别在于第2篇文章的NRI方法仅用于静态关系，第3篇论文对方法进行了改进，可以学习和预测动态的关系。</li><li>第4篇论文是一篇回顾性的revisiting类型的文章，比较了link prediction问题中两类方法 GAE 和 SEAL，结果表示 SEAL 方法由于考虑了上下文信息，效果要优于 GAE 方法。</li><li>第5篇论文针对状态图进行结构发现，包括对状态节点的学习，以及对节点间转移关系（物理意义是“发生了某个事件”）的预测。这篇论文的结构发现不仅局限于对 edge 的发现，还包括对 node 的发现。这篇文章可以进一步深入研究一下。</li><li>第6篇论文提出了一种 DGM 的结构来进行 link prediction，主要包括“特征学习 -&gt; 概率计算 -&gt; 采样”这三个步骤。并且，本文把 DGM 结构和 GCN 结合，来解决 GCN 只能用于 tansductive 而不能用于inductive 的缺陷。</li></ul><h3 id="一些思考"><a href="#一些思考" class="headerlink" title="一些思考"></a>一些思考</h3><p>图的形式是多样的，在不同的场景下有灵活的构图方式。即使是时序数据也会有两种形式，一种是状态转化，只有一个图（论文5），一种是类似轨迹预测的时空图，每个时间点上都有一个图（论文2、3）。</p><p>图结构的设计是跟其应用场景密切相关的。比如论文2、3的轨迹图用于解决轨迹预测，论文5的状态图用于进行事件预测，论文1的item序列图用于进行session-based推荐。</p><h3 id="Session-based-Rec"><a href="#Session-based-Rec" class="headerlink" title="Session-based Rec"></a>Session-based Rec</h3><ol><li>Session-based Recommendation 是 next-item recommendation 的一个特例。比较 general 的 next-item recommendation 会根据之前的固定数字 k 个历史行为作为依据，而 session-based 会把历史行为根据时序上的接近程度划分成不同的 session，只有同一个 session 中先前的历史行为才会作为本次推荐的依据。</li><li>Session-based 方式是基于这样的假设：intra-session 的 item/action 之间的依赖性/相关性是比较强的，而 inter-session 的 item/action 的关联性是比较弱的。比如：用户可能在同一个 session 中分别购买了手机的不同配件，而在另一个 session 可能购买的都是衣服。</li><li>基于session的推荐的发展：<ol><li>基于 nearest-neighbors</li><li>基于Markov chain</li><li>基于RNNs</li><li>基于CNNs</li><li>基于GNNs</li></ol></li></ol>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> Graph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Relation Inference </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法与数据结构_todo</title>
      <link href="2020/11/17/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-todo/"/>
      <url>2020/11/17/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-todo/</url>
      
        <content type="html"><![CDATA[<p>施工中🚧</p>]]></content>
      
      
      <categories>
          
          <category> 算法与数据结构 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>工具_todo</title>
      <link href="2020/11/17/%E5%B7%A5%E5%85%B7-todo/"/>
      <url>2020/11/17/%E5%B7%A5%E5%85%B7-todo/</url>
      
        <content type="html"><![CDATA[<p>施工中🚧<br>todo: vim, git</p>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>计算机视觉_todo</title>
      <link href="2020/11/17/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-todo/"/>
      <url>2020/11/17/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-todo/</url>
      
        <content type="html"><![CDATA[<p>施工中🚧</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>自然语言处理_todo</title>
      <link href="2020/11/17/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-todo/"/>
      <url>2020/11/17/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-todo/</url>
      
        <content type="html"><![CDATA[<p>施工中🚧<br>先从Word2Vec开始吧</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>推荐广告搜索_todo</title>
      <link href="2020/11/17/%E6%8E%A8%E8%8D%90%E5%B9%BF%E5%91%8A%E6%90%9C%E7%B4%A2-todo/"/>
      <url>2020/11/17/%E6%8E%A8%E8%8D%90%E5%B9%BF%E5%91%8A%E6%90%9C%E7%B4%A2-todo/</url>
      
        <content type="html"><![CDATA[<p>施工中🚧<br>todo: 经典模型到深度模型</p><p>Deep Retrieval<br>TDM</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 推荐广告搜索 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Go_todo</title>
      <link href="2020/11/17/Go-todo/"/>
      <url>2020/11/17/Go-todo/</url>
      
        <content type="html"><![CDATA[<p>施工中🚧<br>todo:</p><ul><li>goroutine and pipeline</li></ul>]]></content>
      
      
      <categories>
          
          <category> Go </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Python_todo</title>
      <link href="2020/11/17/Python-todo/"/>
      <url>2020/11/17/Python-todo/</url>
      
        <content type="html"><![CDATA[<p>施工中🚧</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>C++_todo</title>
      <link href="2020/11/17/C-todo/"/>
      <url>2020/11/17/C-todo/</url>
      
        <content type="html"><![CDATA[<p>施工中🚧<br>todo: </p><ul><li>default构造函数</li><li>智能指针</li><li>std::bind()</li><li>std::move() 和右值引用</li><li>drpc 到 Archon</li><li>Blade编译</li><li>开源库：tbb，folly，rapidjson</li></ul>]]></content>
      
      
      <categories>
          
          <category> Cpp </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>分布式todo</title>
      <link href="2020/11/17/%E5%88%86%E5%B8%83%E5%BC%8Ftodo/"/>
      <url>2020/11/17/%E5%88%86%E5%B8%83%E5%BC%8Ftodo/</url>
      
        <content type="html"><![CDATA[<p>施工中🚧<br>raft, rpc, thrift<br>微服务</p>]]></content>
      
      
      <categories>
          
          <category> 分布式 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Infra_todo</title>
      <link href="2020/11/17/Infra-todo/"/>
      <url>2020/11/17/Infra-todo/</url>
      
        <content type="html"><![CDATA[<p>施工中🚧<br>架构向 工程向<br>todo:</p><ul><li>redis</li><li>k8s &amp; docker</li><li>nsq</li></ul>]]></content>
      
      
      <categories>
          
          <category> Infra </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>大数据todo</title>
      <link href="2020/11/17/%E5%A4%A7%E6%95%B0%E6%8D%AEtodo/"/>
      <url>2020/11/17/%E5%A4%A7%E6%95%B0%E6%8D%AEtodo/</url>
      
        <content type="html"><![CDATA[<p>施工中🚧<br>todo: flink, kafka, hadoop, spark, hdfs, mapreduce, hive</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Graph Embedding 浅层模型</title>
      <link href="2020/11/13/Graph-Embedding-%E6%B5%85%E5%B1%82%E6%A8%A1%E5%9E%8B/"/>
      <url>2020/11/13/Graph-Embedding-%E6%B5%85%E5%B1%82%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2 id="论文列表"><a href="#论文列表" class="headerlink" title="论文列表"></a>论文列表</h2><ol start="0"><li>Word2Vec</li><li>Deep Walk</li><li>Node2Vec</li></ol><h2 id="Word2Vec-Embedding技术的奠基工作"><a href="#Word2Vec-Embedding技术的奠基工作" class="headerlink" title="Word2Vec(Embedding技术的奠基工作)"></a>Word2Vec(Embedding技术的奠基工作)</h2><h4 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h4><blockquote><p>[1] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. <em>ICLR Workshop</em>, 2013</p><p>[2] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed Representations of Words and Phrases and their Compositionality. NIPS 2013</p><p>[3] Le Q, Mikolov T. Distributed representations of sentences and documents[C]//International Conference on Machine Learning. 2014: 1188-1196.</p><p>[4] Goldberg Y, Levy O. word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method[J]. arXiv preprint arXiv:1402.3722, 2014.</p><p>[5] Rong X. word2vec parameter learning explained[J]. arXiv preprint arXiv:1411.2738, 2014.</p></blockquote><p>Mikolov等人在2013年的文献[1]中，同时提出了<strong>CBOW(Continuous Bagof-Words)</strong>和<strong>Skip-gram</strong>模型，希望能够用更高效的方法获取词向量。因此，他们根据前人在 NNLM、 RNNLM 和 C&amp;W 模型上的经验，简化了已有模型，保留核心部分，得到了这两个模型。</p><p>Mikolov在2013年又在[2]中提出了训练Skip-gram模型的两个策略：<strong>Hierarchical Softmax</strong>和<strong>Negative Sampling</strong>，用来提高 Word2Vec 模型的训练效率。</p><p>Mikolov在2014年在论文[3]中提出了doc2vec。</p><p>[4], [5] 中对原论文进行了更详细的推导。</p><h4 id="Word2Vec介绍"><a href="#Word2Vec介绍" class="headerlink" title="Word2Vec介绍"></a>Word2Vec介绍</h4><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201113125604428.png" alt="image-20201113125604428"></p><p>Word2vec能训练超过亿万级的文本，将其转化为50-100维度的词向量（word vectors）。在此之前没有模型具备这种从<strong>海量文本</strong>中学习到<strong>高质量词向量</strong>的能力。</p><p>该模型的词向量的<strong>相似度</strong>并不仅仅是简单的句法规则，在词向量上使用<strong>词抵消手段</strong>（即简单的代数运算），可以得到像下述例子这样的结果，例如：vector（国王）- vector（男）+ vector（女）= vector（皇后）。</p><p>论文提出了两种Word2vec的模型结构，它们可以学习词的分布表示（distributed representations）并且最小化其计算复杂度。这两种Word2vec模型结构分别是：<strong>连续词袋模型（Continuous Bag-of-Words Model）</strong>和<strong>连续Skip-gram模型（Continuous Skip-gram Model）</strong>。</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201113125634930.png" alt="image-20201113125634930"></p><ul><li>CBOW：基于上下文语境预测当前词</li><li>Skip-gram：给定当前词预测周围的词</li></ul><p>两种结构遵循同样的假设：每个词都跟它相邻的词的关系最密切。具体到每个模型结构来说，CBOW假设每个词都是由相邻的词决定的，skip-gram假设每个词都决定了相邻的词。</p><p>经验上来说，skip-gram的效果更好。下面以Skip-gram为框架介绍Word2Vec的模型细节。</p><h4 id="Skip-gram模型细节"><a href="#Skip-gram模型细节" class="headerlink" title="Skip-gram模型细节"></a>Skip-gram模型细节</h4><p>算法过程：</p><ol><li><p>用滑窗法在句子上从左向右滑动，每移动一次，生成一个训练样本。这里滑窗大小为2c+1，包含了目标词前后各c个词。</p></li><li><p>定义优化目标：基于极大似然估计，希望所有样本的条件概率之积最大，转化为对数概率的和最大<br> $$<br> \frac{1}{T} \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log p\left(w_{t+j} \mid w_{t}\right)<br> $$</p></li><li><p>定义上述目标中的条件概率$p\left(w_{t+j} \mid w_{t}\right)$：</p><p> 主要有这两点考虑：a. 作为一个多分类问题，最直接的方法就是用softmax函数；b. word2vec的愿景是希望让词向量的内积能够表示语义的接近程度。</p><p> 基于上述两点，可以给出条件概率的定义如下（softmax的形式&amp;用内积表示距离）：</p></li></ol><p>$$<br>p\left(w_{O} \mid w_{I}\right)=\frac{\exp \left(v_{w_{O}}^{\top} v_{w_{I}}\right)}{\sum_{w=1}^{W} \exp \left(v_{w}^{\prime \top} v_{w_{I}}\right)}<br>$$</p><ol start="4"><li><p>需要注意的是，上面条件概率公式中的输入向量和输出向量并不在同一个向量空间内。具体到神经网络的结构上，输入向量是输入层到隐层的权重矩阵，输出向量是隐层到输出层的权重矩阵。</p><p> <img src="http://img.cdn.leonwang.top/picgo_img/1042406-20170727105326843-18935623.png" alt="img"></p><p> 因此，通过梯度下降训练完成之后，得到的输入层到隐层的权重矩阵就是一个词向量查找表。</p></li></ol><h4 id="加快Word2Vec训练速度的方法"><a href="#加快Word2Vec训练速度的方法" class="headerlink" title="加快Word2Vec训练速度的方法"></a>加快Word2Vec训练速度的方法</h4><p>实际上完全遵循上面原始的 Word2Vec 多分类结构的训练方法是不可行的，因为隐层神经元的个数等于语料库中词的数量，计算量太大。因此实际上会通过下面两种方法来提高训练效率</p><ol><li><p>Hierarchical Softmax</p><p> 改进：</p><ul><li>a. 对于从输入层到隐藏层的映射，没有采取神经网络的线性变换加激活函数的方法，而是采用简单的对所有输入词向量求和并取平均的方法。比如输入的是三个4维词向量：(1,2,3,4),(9,6,11,8),(5,10,7,12),那么我们word2vec映射后的词向量就是(5,6,7,8)。由于这里是从多个词向量变成了一个词向量。</li><li>b. 从隐藏层到输出的softmax层这里的计算量个改进。为了避免要计算所有词的softmax概率，word2vec采样了霍夫曼树来代替从隐藏层到输出softmax层的映射。（详细过程暂不讨论）</li></ul></li><li><p>Negative Sampling</p><p> 相比于计算字典中所有词的预测误差，负采样方法值对采样出的几个负样本预测误差，是多分类问题退化成一个近似二分类问题。用下式替换 $p\left(w_{t+j} \mid w_{t}\right)$。<br> $$<br> \log \sigma\left(v_{w_{0}}^{\prime \top} v_{w_{i}}\right)+\sum_{i=1}^{k} \mathbb{E}_{w_{i} \sim P_{n}(w)}\left[\log \sigma\left(-v_{w_{i}}^{\prime \top} v_{w_{t}}\right)\right]<br> $$</p></li></ol><h2 id="DeepWalk"><a href="#DeepWalk" class="headerlink" title="DeepWalk"></a>DeepWalk</h2><h4 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h4><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201113143229719.png" alt="image-20201113143229719"></p><blockquote><p>Perozzi B, Al-Rfou R, Skiena S. Deepwalk: Online learning of social representations[C]//Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. 2014: 701-710.</p></blockquote><h4 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h4><p>DeepWalk的思想类似word2vec，使用<strong>图中节点与节点的共现关系</strong>来学习节点的向量表示。那么关键的问题就是如何来描述节点与节点的共现关系，DeepWalk给出的方法是使用随机游走(RandomWalk)的方式在图中进行节点采样。</p><p>RandomWalk是一种<strong>可重复访问已访问节点的深度优先遍历</strong>算法。给定当前访问起始节点，从其邻居中随机采样节点作为下一个访问节点，重复此过程，直到访问序列长度满足预设条件。</p><p>获取足够数量的节点访问序列后，使用skip-gram model 进行向量学习。</p><p>总结算法步骤如下：</p><ol><li><p>随机游走采样节点序列</p><p> 构建同构网络，从网络中的每个节点开始分别进行Random Walk 采样，得到局部相关联的训练数据； </p></li><li><p>skip-gram学习表达向量</p><p> 对采样数据进行SkipGram训练，将离散的网络节点表示成向量化，最大化节点共现，使用Hierarchical Softmax来做超大规模分类的分类器</p></li></ol><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201113143307835.png" alt="image-20201113143307835"></p><h2 id="Node2Vec"><a href="#Node2Vec" class="headerlink" title="Node2Vec"></a>Node2Vec</h2><h4 id="论文-1"><a href="#论文-1" class="headerlink" title="论文"></a>论文</h4><blockquote><p>Grover A, Leskovec J. node2vec: Scalable feature learning for networks[C]//Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. 2016: 855-864.</p></blockquote><p>node2vec是一种综合考虑DFS邻域和BFS邻域的graph embedding方法。简单来说，可以看作是deepwalk的一种扩展，是结合了DFS和BFS随机游走的deepwalk。</p><p>这篇文章的<strong>主要贡献是在于如何获取一个更有意义的“sentence”，这个sentence能够更好的包含节点相似性和结构相似性问题</strong>（randomwalk采用uniformly一条路走到黑）。等同于说这个<strong>random walk怎么走的问题</strong>（sampling neighbourhoods）。</p><h4 id="算法原理-1"><a href="#算法原理-1" class="headerlink" title="算法原理"></a>算法原理</h4><p>主要分析这篇文章用的随机游走采样策略。</p><p>node2vec依然采用随机游走的方式获取节点的近邻序列，不同的是node2vec采用的是一种有偏的随机游走。</p><p>给定当前节点 $v$ ，访问下一个节点 $x$ 的概率为：<br>$$<br>P\left(c_{i}=x \mid c_{i-1}=v\right)=\left\{\begin{array}{cc}<br>\frac{\pi_{v x}}{Z} &amp; \text { if }(v, x) \in E \\<br>0 &amp; \text { otherwise }<br>\end{array}\right.<br>$$<br>其中，$\pi_{v x}$ 是节点 $v$ 和节点 $x$ 之间的未归一化转移概率， $Z$ 是归一化常数。</p><p>node2vec 引入两个超参数 $p$ 和 $q$ 来控制随机游走的策略，假设当前随机游走经过边 $(t,v)$ 到达节点 $v$ 设 $\pi_{v x}=\alpha_{p q}(t, x) \cdot w_{v x}$ ， $w_{vx}$ 是节点 $v$ 和 $x$ 之间的边的权重，而$\alpha_{p q}(t, x)$ 定义为<br>$$<br>\alpha_{p q}(t, x)=\left\{\begin{array}{ll}<br>\frac{1}{p}= &amp; \text { if } d_{t x}=0 \\<br>1= &amp; \text { if } d_{t x}=1 \\<br>\frac{1}{q}= &amp; \text { if } d_{t x}=2<br>\end{array}\right.<br>$$<br>其中，$d_{tx}$ 为节点 $t$ 和节点  $x$ 之间的最短路径距离。</p><p>下面讨论超参数 $p$ 和 $q$ 对游走策略的影响</p><ul><li>Return parameter, $p$</li></ul><p>参数 $p$ 控制重复访问刚刚访问过的节点的概率。 注意到 $p$ 仅作用于 $d_{tx}=0$ 的情况，而 $d_{tx}=0$ 表示节点 $x$ 就是访问当前节点 $v$ 之前刚刚访问过的节点。 那么若 $p$ 较高，则访问刚刚访问过的节点的概率会变低，反之变高。</p><ul><li>In-out papameter, $q$</li></ul><p>$q$ 控制着游走是向外还是向内，若 $q&gt;1$ ，随机游走倾向于访问和 $t$ 接近的节点(偏向BFS)。若 $q&lt;1$ ，倾向于访问远离  $t$ 的节点(偏向DFS)。</p><p>下面的图描述的是当从 $t$ 访问到 $v$ 时，决定下一个访问节点时每个节点对应的 $\alpha$ 。</p><p><img src="http://img.cdn.leonwang.top/picgo_img/image-20201113145112864.png" alt="image-20201113145112864"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> Graph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Graph Embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode11 vs LeetCode84 柱状图中的矩形</title>
      <link href="2020/07/11/LeetCode11-vs-LeetCode84/"/>
      <url>2020/07/11/LeetCode11-vs-LeetCode84/</url>
      
        <content type="html"><![CDATA[<p>第11题：盛最多水的容器</p><ul><li>解法：双指针</li></ul><p>第84题：柱状图中的最大矩形</p><ul><li>解法：单调栈</li></ul><p>两道题目有些类似，都是计算柱状图某种条件下的最大矩形区域的面积。但由于具体的限制条件不同，所以用不同的方法来解决。</p><p>两道题目的区别主要在于，<strong>是否需要考虑被夹在两端元素之间的若干元素</strong>。</p><p>在11题中，不需要考虑中间的元素是否比两端元素大。所以可以用双指针的方法，从两边向中间靠近，每次短板向中间走一步，可以证明这样做是正确的（因为如果长板向中间走一步，总面积是一定不会变大的，所以更大的面积只有在移动短板时才会出现）。</p><p>在第84题中，中间的元素要都大于两端元素，所以必须考虑内部元素的大小。思路是找以每个元素为高的最大矩形，转化为找左边沿和右边沿的问题，左边沿是向左第一个小于该元素的元素，右边沿是向右第一个小于该元素的元素。一个比较巧妙的实现方法是用一个单调栈来实现，向右扫描的过程可以找到右边沿，通过出栈可以找到左边沿。</p><p>第11题直接从两个边界的角度来考虑，第84题从某个元素的角度来考虑，去找这个元素对应的两个边界。</p><p>题目及题解：</p><p><img src="http://img.cdn.leonwang.top/image-20200711180745489.png" alt="image-20200711180745489"></p><pre><code class="c++">class Solution {public:    int maxArea(vector&lt;int&gt;&amp; height) {        // 双指针        int i = 0;        int j = height.size() - 1;        int res = 0;        while(i &lt; j) {            int min_height = min(height[i], height[j]);            int cur_res = min_height * (j - i);            res = max(res, cur_res);            if (min_height == height[i])                ++i;            else                --j;        }        return res;    }};</code></pre><p><img src="http://img.cdn.leonwang.top/image-20200711180833079.png" alt="image-20200711180833079"></p><pre><code class="c++">class Solution {public:    int largestRectangleArea(vector&lt;int&gt;&amp; heights) {        if (heights.size()==0)            return 0;        if (heights.size()==1)            return heights[0];        int res = 0;        heights.push_back(0);    // trick: 原数组最后补一个0，使得原来的最后一个元素之后也是递减的        int num = heights.size();        stack&lt;int&gt; st;        for (int i = 0; i &lt; num; ++i) {            if (st.empty() || heights[st.top()]&lt;=heights[i])  // 如果是空栈，或者仍然是在单调递增的，则直接压栈                st.push(i);        // push到栈中的是下标i            else {                int temp_res = 0;                while (!st.empty() &amp;&amp; heights[st.top()]&gt;heights[i]) {                    int temp_index = st.top();                    st.pop();                    if (st.empty())                    // 如果是空的，说明heights[i]左边所有元素都比它小                        temp_res = heights[temp_index] * (i);                    else                        temp_res = heights[temp_index] * (i - st.top() - 1);                    res = max(res, temp_res);                }                st.push(i);            }        }        return res;    }};</code></pre>]]></content>
      
      
      <categories>
          
          <category> 算法与数据结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
            <tag> 双指针 </tag>
            
            <tag> 单调栈 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Loss Function for Medical and Non-medical Image Segmentation</title>
      <link href="2020/04/07/Loss-Function-for-Medical-and-Non-medical-Image-Segmentation/"/>
      <url>2020/04/07/Loss-Function-for-Medical-and-Non-medical-Image-Segmentation/</url>
      
        <content type="html"><![CDATA[<p>论文：</p><p>Deep Semantic Segmentation of Natural and Medical Images: A Review</p><p><img src="http://img.cdn.leonwang.top/image-20200325083606109.png" alt="image-20200325182618956" style="zoom:50%;"></p><p>这篇论文将基于深度学习的医学和非医学图像分割解决方案主要分为六个方面：</p><ul><li>模型架构改进</li><li>基于数据合成的改进</li><li>基于损失函数的改进</li><li>序列模型</li><li>弱监督</li><li>多任务方法</li></ul><p>除了通过修改模型架构来提高分割速度/准确性外，设计新的损失函数也可以提高分割精度。</p><p>本篇学习报告主要关注第三部分，即医学、非医学图像深度模型中的<strong>损失函数</strong>。</p><h2 id="非医学图像分割中的损失函数"><a href="#非医学图像分割中的损失函数" class="headerlink" title="非医学图像分割中的损失函数"></a>非医学图像分割中的损失函数</h2><p>可以分为两大类：CE-based 和 Dice Loss-based。</p><p>CE - base 方法主要包括传统CE、加权CE以及Focal Loss。</p><p>Dice Loss - base 方法主要包括 Dice Loss 及其各种变体。</p><h3 id="CE-based（基于分布相似度）"><a href="#CE-based（基于分布相似度）" class="headerlink" title="CE - based（基于分布相似度）"></a>CE - based（基于分布相似度）</h3><ul><li><p>交叉熵</p><ul><li><p>逐像素交叉熵损失</p></li><li><p>二类像素的分割：</p><p>  p表示ground truth</p><p>  p^ 表示prediction</p><p>  $\operatorname{CE}(p, \hat{p})=-(p \log (\hat{p})+(1-p) \log (1-\hat{p}))$</p><p>  <img src="http://img.cdn.leonwang.top/image-20200325182618956.png" alt="image-20200325083606109" style="zoom: 33%;"></p></li><li><p>多类像素的分割：$\mathrm{CE}=-\sum_{\text {classes}} p \log \hat{p}$</p></li></ul></li><li><p>加权交叉熵</p><p>  交叉熵损失会分别评估每个像素的类别预测，然后对所有像素取平均，这意味着对图像中每个像素的学习是平等的。如果各个类别在图像中的表示不平衡，则可能会出现问题，因为训练过程可能会由最普遍的类别主导。</p><ul><li><p><strong>WCE</strong> 只对正像素进行加权 </p><p>  weighting the cross entropy loss (WCE): $\operatorname{WCE}(p, \hat{p})=-(\beta p \log (\hat{p})+(1-p) \log (1-\hat{p}))$</p><p>  为了减少假阴性的数目，β被设置为大于1的值；为了减少假阳性的数目，β被设置为小于1的值。</p></li><li><p><strong>BCE</strong> 对正负像素都加权</p><p>  balanced cross entropy (BCE)</p><p>  $\operatorname{BCE}(p, \hat{p})=-(\beta p \log (\hat{p})+(1-\beta)(1-p) \log (1-\hat{p}))$</p></li><li><p><strong>引入与最近的cell的距离</strong></p><p>  在交叉熵函数中添加了距离函数，加强在对象彼此非常接近的情况下的学习效果（可以理解为加强对边界处像素的学习），实现更好的分割，如下所示：</p><p>  $\operatorname{BCE}(p, \hat{p})+w_{0} \cdot \exp \left(-\frac{\left(d_{1}(x)+d_{2}(x)\right)^{2}}{2 \sigma^{2}}\right)$</p><p>  其中$d_1(x)$和$d_2(x)$为两个距离函数，表示点x与最近和第二近的cell的距离。</p><p>  后面的距离函数在U-Net损失函数中也用到了，不过U-Net把这一项加到了每个像素的CE的权重当中，而这里是在BCE的基础上在加上这一项。二者都把“边界”这一因素加入到了CE当中，只是方式不一样。</p></li></ul></li><li><p>Focal Loss</p><ul><li><p>RetinaNet 目标检测 样本不平衡问题 降低简单（容易分类）样本的权重 增加困难（难分类）样本的权重，基于交叉熵损失，添加了一项：</p><p>  $\begin{aligned} \operatorname{FL}(p, \hat{p}) &amp;=-\left(\alpha(1-\hat{p})^{\gamma} p \log (\hat{p})\right. \left.+(1-\alpha) \hat{p}^{\gamma}(1-p) \log (1-\hat{p})\right) \end{aligned}$</p><p>  当$\gamma=0$时，FL 变成 BCE</p><p>  $\gamma$称作focusing parameter，$\left(1-p_{t}\right)^{\gamma}$称为调制系数（modulating factor）。加上这个调制系数<strong>目的是通过减少易分类样本的权重，从而使得模型在训练时更专注于难分类的样本。</strong></p><p>  <a href="https://zhuanlan.zhihu.com/p/32423092" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32423092</a></p><p>  BCE和focal loss都是对不同像素进行加权，只是加权的方式不同。BCE对不同的类别进行加权。focal loss是在BCE的基础之上，再次根据样本分类的难度进行加权，比如用softmax进行分类，预测为真实类别的概率值和1之间的差距越大，则认为是样本分类的难度越大；或者在sigmoid中，0.6和0.9都被分类为positive，但是0.9的样本就是更容易分类的样本。</p></li></ul></li></ul><h3 id="Dice-Loss-based（基于重叠）"><a href="#Dice-Loss-based（基于重叠）" class="headerlink" title="Dice Loss - based（基于重叠）"></a>Dice Loss - based（基于重叠）</h3><ul><li><p>Overlap Measure based Loss Function</p><ul><li><p>Dice Loss / F1 Score</p><ul><li><p>Dice系数（==F1 Score）</p><p>  $\mathrm{DC}=\frac{2 T P}{2 T P+F P+F N}=\frac{2|X \cap Y|}{|X|+|Y|}$</p><p>  另外，Jaccard Metric(交并比): $\mathrm{IoU}=\frac{T P}{T P+F P+F N}=\frac{|X \cap Y|}{|X|+|Y|-|X \cap Y|}$，可知$\mathrm{DC} \geq \mathrm{IoU}$</p><p>  Dice Loss: $\operatorname{DL}(p, \hat{p})=\frac{2\langle p, \hat{p}\rangle}{|p|_{1}+|\hat{p}|_{1}}$</p><p>  分子的计算：</p><p>  <img src="http://img.cdn.leonwang.top/image-20200407212218750.png" alt="image-20200407212218750"></p><p>  分母的计算：</p><p>  <img src="http://img.cdn.leonwang.top/image-20200407212239314.png" alt="image-20200407212239314"></p><blockquote><p>Dice损失比较适合样本极度不平衡的情况，但会对反向传播造成影响，导致反向传播不稳定。</p></blockquote></li></ul></li><li><p>Tversky Loss</p><p>  Dice Loss 的推广，给FP和FN加上权重。</p><p>  $\operatorname{TL}(p, \hat{p})=\frac{\langle p, \hat{p}\rangle}{\langle p, \hat{p}\rangle+\beta(1-p, \hat{p}\rangle+(1-\beta)(p, 1-\hat{p})}$</p><p>  $\beta=0.5$时退化成Dice Loss</p></li><li><p>Exponential Logarithmic Loss</p><p>  指数对数Dice Loss：$\mathcal{L}_{\mathrm{ELD}}=\mathbf{E}\left[\left(-\ln \left(D_{i}\right)\right)^{\gamma_{D}}\right]$</p><p>  加权指数交叉熵Loss：$\mathcal{L}_{\mathrm{wece}}=\mathbf{E}\left[\left(-\ln \left(p_{l}(\mathbf{x})\right)\right)^{\gamma_{C E}}\right]$</p><p>  上述二者加权求和：$\mathcal{L}=w_{\mathrm{eld}} \mathcal{L}_{\mathrm{eld}}+w_{\mathrm{wece}} \mathcal{L}_{\mathrm{wece}}$</p><p>  x，i和l表示像素位置，预测标签和ground truth标签。 $D_i$表示平滑的Dice损失</p></li><li><p>Lov´asz-Softmax loss</p><p>  是离散Jaccard Loss的平滑扩展，可用于多类分割，定义为：</p><p>  $\mathcal{L}_{\text {Lovasz Softmax }}=\frac{1}{|\mathcal{C}|} \sum_{c \in \mathcal{C}} \overline{\Delta_{J_{c}}}(\boldsymbol{m}(c))$</p><p>  有点复杂没细看</p></li><li><p>Boundary Loss</p><p>  $\alpha \mathcal{L}_{G D}(\theta)+(1-\alpha) \mathcal{L}_{B}(\theta)$</p><p>  $\mathcal{L}_{B}$: Boundary Loss</p><p>  $\mathcal{L}_{G D}$: Generalized Dice Loss</p><p>  综述里对数学符号的解释不太完整</p></li><li><p>Conservative Loss</p><p>  保守损失，以通过惩罚极端情况和鼓励中等情况来在域适应任务中获得良好的泛化能力。保守损失定义为</p><p>  $C L\left(p_{t}\right)=\lambda\left(1+\log _{a}\left(p_{t}\right)\right)^{2} * \log _{a}\left(-\log _{a}\left(p_{t}\right)\right)$</p></li></ul></li></ul><p>其他工作还包括优化分割指标的方法，加权损失函数以及向损失函数添加正则化器以编码几何和拓扑形状先验。</p><h2 id="医学图像分割中的损失函数"><a href="#医学图像分割中的损失函数" class="headerlink" title="医学图像分割中的损失函数"></a>医学图像分割中的损失函数</h2><p>图像分割（尤其是医学图像）中的<strong>主要问题是克服类别不平衡</strong>，对于此类不平衡，基于重叠度量的方法在克服不平衡方面显示出相当好的性能。在下一节中,我们总结了使用新损失函数（特别是用于医学图像分割）或使用上述（修正）损失函数的方法。</p><p>如上一节所述，<strong>标准的CE损失函数及其加权版本</strong>已应用于众多医学图像分割问题。但是，Miletari等人发现在某些情况下Dice Loss其效果要优于原始的交叉熵，例如，在大背景中具有非常小的前景对象的情况下。</p><p>本节主要介绍了以下方法：</p><ul><li><p>向交叉熵损失函数中<strong>加入正则化项</strong>，以实现更平稳的分段输出。正则项为：</p><p>  $R=\sum_{i=1}^{N} \mathbb{E}_{\xi^{\prime}, \xi}\left|f\left(x_{i} ; \theta, \xi^{\prime}\right)-f\left(x_{i} ; \theta, \xi\right)\right|^{2}$</p></li><li><p>tradition active contour energy minimization <strong>通过定义如下损失函数的方式，把主动轮廓模型（一种传统的图像分割方法）的思想引入CNN</strong>模型.</p><p>  $\text { Loss }=\text { Length }+\lambda \cdot \text { Region }$</p><p>  Length和Region具体的formulation定义可以看综述论文或者原文.</p><blockquote><p>X. Chen, B. Williams, S. Vallabhaneni, G. Czanner, R. Williams, Y. Zheng. Learning Active Contour Models for Medical Image Segmentation. <em>CVPR 2019</em>.</p></blockquote></li><li><p>optimized Hausdorff distance based function <strong>把Dice与Hausdorff距离（一种可以用在边缘匹配的方法）结合</strong></p><p>  $f_{\mathrm{HD}}(p, q)=\operatorname{Loss}(p, q)+\lambda\left(1-\frac{2 \sum_{\Omega}(p \circ q)}{\sum_{\Omega}\left(p^{2}+q^{2}\right)}\right)$</p><p>  第二项是Dice Loss，第一项可以选择Hausdorff距离的三个版本</p></li></ul><p>本节中讨论的大多数方法都尝试通过在损失函数中<strong>提供权重/惩罚项</strong>来处理输入图像中的<strong>类不平衡问题</strong>，即小前景与大背景。其他方法包括首先确定感兴趣的对象，围绕该对象裁剪，然后以更好的平衡类执行任务。这种类型的级联方法已应用于脊髓多发性硬化病灶的分割</p><h2 id="CE-vs-Dice-Loss"><a href="#CE-vs-Dice-Loss" class="headerlink" title="CE vs Dice Loss"></a>CE vs Dice Loss</h2><p>语义分割中一般用交叉熵来做损失函数，而评价的时候却使用IOU来作为评价指标，（GIOU这篇文章中说道：给定优化指标本身与代理损失函数之间的选择，最优选择就是指标本身。）为什么不直接拿类似IOU的损失函数来进行优化呢？</p><ul><li><p>CE的优势：首先采用<strong>交叉熵损失函数</strong>，而非 dice-coefficient 和类似 IoU 度量的损失函数，一个令人信服的愿意是其<strong>梯度形式更优</strong>：</p><p>  交叉熵损失函数中交叉熵值关于 logits 的梯度计算形式类似于p−t，其中，p是 softmax 输出；t为 target；而关于 dice-coefficient 的可微形式，loss 值为 2pt/(p^2+t^2) 或 2pt/(p+t)，其关于p的梯度形式是比较复杂的：2t^2/(p+t)^2 或 2t*(t^2−p^2)/(p^2+t^2)^2. 极端场景下，当p和t的值都非常小时，计算得到的梯度值可能会非常大. 通常情况下，可能导致训练更加不稳定.</p></li><li><p>Dice Loss的优势：直接采用 <strong>dice-coefficient 或者 IoU</strong> 作为损失函数的原因，是因为<strong>分割的真实目标</strong>就是最大化 dice-coefficient 和 IoU 度量. 而交叉熵仅是一种代理形式，利用其在 BP 中易于最大化优化的特点.</p></li></ul><h2 id="案例：U-Net损失函数"><a href="#案例：U-Net损失函数" class="headerlink" title="案例：U-Net损失函数"></a>案例：U-Net损失函数</h2><p>加权交叉熵</p><p>像素级softmax + cross entropy</p><p>像素类别真值，和1之间的交叉熵</p><p>对边界像素进行加权 The separation border is computed <strong>using morphological operations</strong>. The weight map is then computed as</p><p>$w(\mathbf{x})=w_{c}(\mathbf{x})+w_{0} \cdot \exp \left(-\frac{\left(d_{1}(\mathbf{x})+d_{2}(\mathbf{x})\right)^{2}}{2 \sigma^{2}}\right)$</p><p>其中$w_c$：Ω→R是用来平衡类频率的权重图，d1：Ω→R表示到最近一个单元格的边界的距离，d2：Ω→R是到第二个最近格的边界的距离。在我们的实验中，我们设置w0 = 10和σ≈5像素。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>主要有基于CE和基于Dice Loss两类方法，CE的梯度形式更优，Dice Loss的优化目标与评价指标更吻合。</li><li>可以考虑在CE或Dice方法上进行改进。改进的方式包括增加线性加权、对数指数加权、增加正则项/惩罚项等。改进的目的主要是增加对类别、样本分类难易程度、边缘/轮廓/边界等因素的考虑。</li><li>也有一些损失函数不属于上述两类风格，比如通过损失函数实现Active Contour Model思想的方法。</li></ol><blockquote><p>2019年5篇图像分割算法最佳综述<br><a href="https://cloud.tencent.com/developer/article/1550320" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1550320</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 分割 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 损失函数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>U-Net</title>
      <link href="2020/03/22/U-Net/"/>
      <url>2020/03/22/U-Net/</url>
      
        <content type="html"><![CDATA[<p>论文：U-Net: Convolutional Networks for Biomedical Image Segmentation</p><p><img src="http://img.cdn.leonwang.top/20200316082331.png" alt=""></p><p>要点：</p><ol><li><p><strong>基于全卷积网络(FCN)</strong>。FCN和UNet都采用了encoder-decoder的结构。区别在于：1. UNet采用了对称的结构，而FCN中的decoder比较简单；2. 在skip connection操作（深层信息与浅层信息融合）中，FCN用的是对应像素相加，UNet用的是拼接。</p><blockquote><p>在 ResNet 与 DenseNet 中也有一样的区别，Resnet 使用了对应值相加，DenseNet 使用了拼接。<strong>个人理解在相加的方式下，feature map 的维度没有变化，但每个维度都包含了更多特征，对于普通的分类任务这种不需要从 feature  map 复原到原始分辨率的任务来说，这是一个高效的选择；而拼接则保留了更多的维度/位置 信息，这使得后面的 layer 可以在浅层特征与深层特征自由选择，这对语义分割任务来说更有优势。</strong></p></blockquote></li><li><p><strong>压缩路径</strong>(contracting path)包含4个block，每个block的任务：2次有效卷积和1次max pooling。</p></li><li><p><strong>扩展路径</strong>(expensive path)也包含4个block，每个block的任务：反卷积、与来自contracting path的feature map合并、卷积。</p></li><li><p>所有卷积的填充方式（Padding）都是有效卷积(VALID)，而不是SAME卷积，所以特征图合并时，两个路径的feature map的size是不一样大的，扩展路径中的feature比压缩路径中的feature map小。论文采取的方法是对压缩路径的feature map进行<strong>裁剪(crop)</strong>。</p></li><li><p>医学图像是一般相当大，分割时候不可能直接将原图输入网络，所以必须切成一张一张的小patch，在切成小patch的时候，Unet采用了有<strong>overlap</strong>的切图，对<strong>边缘进行了镜像操作</strong>。overlap的一个重要原因是周围overlap部分可以<strong>为分割区域边缘部分提供文理等信息</strong>。</p></li><li><p>损失函数：<strong>带边界权值的损失函数</strong>。</p></li><li><p>由于数据量小，论文采用了<strong>数据增强</strong>的方法，这也有利于提高模型的鲁棒性。可以看一下数据增强的方式。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 分割 </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> U-Net </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Segmentation Review</title>
      <link href="2020/03/22/Segmentation-Review/"/>
      <url>2020/03/22/Segmentation-Review/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><p>需要进行像素级别的分类</p><h2 id="分割任务分类"><a href="#分割任务分类" class="headerlink" title="分割任务分类"></a>分割任务分类</h2><ul><li>语义分割</li><li>实例分割</li><li>全景分割</li></ul><h2 id="分割方法的分类"><a href="#分割方法的分类" class="headerlink" title="分割方法的分类"></a>分割方法的分类</h2><p><strong>2015-2018</strong></p><ul><li>基于上采样/SPP/ASPP方法<ul><li>Encoder-Decoder：FCN, U-Net, SegNet, RefineNet</li><li>SPP（空间金字塔池化，并行的多尺度融合）: PSPNet</li><li>ASPP（空洞/带孔卷积）: DeepLab</li><li>Encoder-Decoder与ASPP结合: DeepLab v3+</li></ul></li><li>基于区域选择的方法<ul><li>Mask R-CNN</li><li>Mask Scoring R-CNN (2019 CVPR oral)</li></ul></li><li>弱监督的方法</li><li>GAN的方法</li></ul><p><strong>2018-2020</strong></p><p>non-local (Attention)</p><p>DANet, CCNet</p><h2 id="分割方法2015-2018"><a href="#分割方法2015-2018" class="headerlink" title="分割方法2015-2018"></a>分割方法2015-2018</h2><h3 id="传统的基于CNN的分割"><a href="#传统的基于CNN的分割" class="headerlink" title="传统的基于CNN的分割"></a>传统的基于CNN的分割</h3><p>使用像素周围的一个图块作为输入，不断滑动窗口，输出尺寸是固定的</p><h3 id="FCN"><a href="#FCN" class="headerlink" title="FCN"></a>FCN</h3><p><img src="http://img.cdn.leonwang.top/image-20200320173319438.png" alt="image-20200320173319438"></p><p><img src="http://img.cdn.leonwang.top/image-20200320191637474.png" alt="image-20200320191637474"></p><p>“Fully convolutional networks for semantic segmentation.” Long, Jonathan, Evan Shelhamer, and Trevor Darrell. <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2015.</p><p>把后面的几个<strong>全连接层全都换成卷积</strong>，用反卷积层对最后一个卷积层上采样，恢复到输入图像的尺寸。后面接softmax，从而获得每个像素的分类。</p><p>输入可以是任意尺寸</p><p>跳级(skip)结构：实际上是加和</p><p>缺点：</p><ul><li>上采样得到的结果不够精细；</li><li>没考虑到像素之间的关系。通常在像素分类的分割方法中会使用空间规整spatial regularization。</li></ul><h3 id="U-Net"><a href="#U-Net" class="headerlink" title="U-Net"></a>U-Net</h3><p><img src="http://img.cdn.leonwang.top/image-20200320173343473.png" alt="image-20200320173343473"></p><p>“U-net: Convolutional networks for biomedical image segmentation. “<em>International Conference on Medical image computing and computer-assisted intervention</em>. Springer, Cham, 2015. 2015 年 arxiv，Ronneberger, Olaf, Philipp Fischer, and Thomas Brox</p><p>基于FCN</p><p>收缩路径+扩展路径</p><p>连接融合操作</p><p>可学习：1. 如果要切patch，可以对边缘进行镜像操作 2. 带边界权值的损失函数</p><h3 id="SegNet"><a href="#SegNet" class="headerlink" title="SegNet"></a>SegNet</h3><p><img src="http://img.cdn.leonwang.top/image-20200320173018319.png" alt="image-20200320173402352"></p><p>Badrinarayanan, Vijay, Alex Kendall, and Roberto Cipolla. “Segnet: A deep convolutional encoder-decoder architecture for image segmentation.” IEEE transactions on pattern analysis and machine intelligence 39.12 (2017): 2481-2495.</p><p>SegNet: 基于FCN，修改VGG-16网络得到的语义分割网络，属于基础的综合网络之一，很好的入门分割模型。</p><p>encoder+decoder结构：encoder与vgg13层卷积层相同，decoder<strong>使用最大池化层的池化索引进行非线性上采样</strong>，上采样过程不需要学习。</p><h3 id="RefineNet"><a href="#RefineNet" class="headerlink" title="RefineNet"></a>RefineNet</h3><p>2016</p><p>encoder采用了<strong>ResNet</strong>，然后把四个resnet block的输出作为四个path通过refinenet block进行融合refine。</p><p>借鉴了ResNet的残差网络结构</p><p>每个refineNet block包含四部分：残差卷积单元、多分辨率融合单元、链式残差池化、输出卷积单元</p><p><img src="http://img.cdn.leonwang.top/image-20200320120502137.png" alt="image-20200320144821299" style="zoom: 67%;"></p><p><img src="http://img.cdn.leonwang.top/image-20200320201013615.png" alt="image-20200320144840958" style="zoom:25%;"></p><h3 id="PSPNet"><a href="#PSPNet" class="headerlink" title="PSPNet"></a>PSPNet</h3><p>Zhao, Hengshuang, et al. “Pyramid scene parsing network.”Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.</p><p><img src="http://img.cdn.leonwang.top/image-20200320192300069.png" alt="image-20200320173018319"></p><p>金字塔场景稀疏网络语义分割模型（Pyramid Scene Parsing Network，PSP）首先结合预训练网络 ResNet和扩张网络来提取图像的特征，得到原图像 1/8 大小的特征图，然后，采用<strong>金字塔池化模块</strong>将特征图同时通过四个并行的池化层得到四个不同大小的输出，将四个不同大小的输出<strong>分别进行上采样</strong>，还原到原特征图大小，最后与之前的特征图进行<strong>连接</strong>后经过卷积层得到最后的预测分割图像。</p><h3 id="DeepLab系列"><a href="#DeepLab系列" class="headerlink" title="DeepLab系列"></a>DeepLab系列</h3><h4 id="v1"><a href="#v1" class="headerlink" title="v1"></a>v1</h4><p>DeepLab 是结合了深度卷积神经网络（DCNNs）和概率图模型（DenseCRFs）的方法。  </p><h4 id="v2"><a href="#v2" class="headerlink" title="v2"></a>v2</h4><p>Chen,Liang-Chieh, et al. “Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.” IEEE transactions on pattern analysis and machine intelligence 40.4 (2017): 834-848.</p><p>DeepLab v2: <strong>多尺度空洞卷积</strong>(ASPP)。DeepLab v1的带孔卷积是串行的，v2用一个ASPP模块改为并行的，每个分支采用不同的rate（获得不同的感知野），以解决多尺度问题。用ASPP可获得更好的分割效果且基础层由VGG16转为ResNet也对结果的提升有所帮助。</p><h4 id="v3"><a href="#v3" class="headerlink" title="v3"></a>v3</h4><p>比较了多种捕获多尺度信息的方式</p><p><img src="http://img.cdn.leonwang.top/image-20200320173402352.png" alt="image-20200320120502137"></p><h4 id="v3-1"><a href="#v3-1" class="headerlink" title="v3+"></a>v3+</h4><p>v3+主要目的在于解决这个问题：</p><p>因为深度网络存在stride=2的层，会导致feature分辨率下降，从而导致预测精度降低，而造成的边界信息丢失问题。  </p><p>解决方案：</p><ol><li><p>编解码器结构；</p></li><li><p>Modified Aligned Xception</p></li></ol><h3 id="Mask-RCNN"><a href="#Mask-RCNN" class="headerlink" title="Mask-RCNN"></a>Mask-RCNN</h3><p><strong>1.技术要点1 - 强化的基础网络</strong>  </p><p><strong>2.技术要点2 - ROIAlign</strong>  </p><p><strong>3.技术要点3 - Loss Function</strong>  </p><h3 id="LinkNet"><a href="#LinkNet" class="headerlink" title="LinkNet"></a>LinkNet</h3><p>Chaurasia, Abhishek, and Eugenio Culurciello. “Linknet: Exploiting encoder representations for efficient semantic segmentation.” 2017 IEEE Visual Communications and Image Processing (VCIP). IEEE, 2017.</p><p>LinkNet: 该文章的创新点在于，网络结构简单，有效减少训练参数，从而加快网络训练。<strong>轻量级网络</strong>。</p><p>本文主要侧重语义分割的速度问题，算法思路类似 U-Net，引入了 residual blocks</p><h2 id="分割方法2018-2020"><a href="#分割方法2018-2020" class="headerlink" title="分割方法2018-2020"></a>分割方法2018-2020</h2><h3 id="non-local-attention"><a href="#non-local-attention" class="headerlink" title="non-local (attention)"></a>non-local (attention)</h3><p>2018CVPR何凯明non-local networks</p><p>Attention 机制继在 NLP 领域取得主导地位之后，近两年在 CV 领域也开始独领风骚。率先将之引入的是 Kaiming He 组的 Nonlocal。此后层出不穷的文章，引发了一波研究 attention 机制的热潮。</p><p>仅2018年，在语义分割领域就有多篇高影响力文章出炉，如 PSANet，DANet，OCNet，CCNet，以及今年的Local Relation Net。此外，针对 Attention 数学形式的优化，又衍生出A2Net，CGNL。而 A2Net 又开启了本人称之为“低秩”重建的探索，同一时期的SGR，Beyonds Grids，GloRe，LatentGNN 都可以此归类。</p><h3 id="DANet"><a href="#DANet" class="headerlink" title="DANet"></a>DANet</h3><p>Dual Attention Network for Scene Segmentation</p><p>2019 CVPR</p><p>成绩：<br>Cityscape：81.5%</p><p><strong>双注意力机制</strong>：Channel Attention+Spatial Attention，得到 non-local 的信息</p><p>（1） 提出了Position Attention Module，也就是把Non-Local用在H*W轴和C轴（特征通道）上，这样可以增强特征的远距离上下文表示能力。</p><p>（2） 基于Position Attention Module，加上ResNet作为base model，提出了Dual Attention Network。</p><p>（3） 在Cityscapes、PASCAL、COCO数据集上的表现不错。</p><h3 id="CCNet"><a href="#CCNet" class="headerlink" title="CCNet"></a>CCNet</h3><p>CCNet: Criss-Cross Attention for Semantic Segmentation</p><p>Contribution:</p><p>（1） 提出了Criss-Cross Attention Module，解决图像分割中像素点的远距离依赖问题，并且这个module只增加很少的参数量。</p><p>（2） 基于提出的module，提出CCNet用于图像分割。</p><p>Nonlocal 对于每个 $\mathbf{y}_{i}$ 的计算，都要在全图上进行，因此复杂度为 $O\left(N^{2} C\right)$ 。CCNet将<strong>全图计算分解为两步，一步是按行计算，一步是按列计算。</strong></p><h3 id="Asymmetric-Non-local-Neural-Networks-for-Semantic-Segmentation"><a href="#Asymmetric-Non-local-Neural-Networks-for-Semantic-Segmentation" class="headerlink" title="Asymmetric Non-local Neural Networks for Semantic Segmentation"></a>Asymmetric Non-local Neural Networks for Semantic Segmentation</h3><p>ICCV 2019 </p><p>实现高效计算</p><h3 id="EMANet"><a href="#EMANet" class="headerlink" title="EMANet"></a>EMANet</h3><p>Expectation-Maximization Attention Networks for Semantic Segmentation</p><p>ICCV 2019 oral</p><p>EM方法与attention结合，定义了EMA模块</p><p><img src="http://img.cdn.leonwang.top/image-20200320144821299.png" alt="image-20200320201013615"></p><p>此篇论文的主要贡献如下：</p><ul><li>作者将自注意力基础重定义为一个迭代的最大期望方法，能够学习到一个更紧密的数据基础集合，大大地降低了计算复杂度。在作者认知范围内，其论文首次将迭代EM引入注意力机制。</li><li>作者将提出的最大期望注意力构建为一个轻量级的神经网络模块，建立了维护、归一化数据基础的特定方法。</li><li>在三个具有挑战性的数据集上进行了扩展实验，包括PASCAL VOC， PASCAL Context，和 COCO Stuff，证明了其方法可以超出现有的最佳方法。</li></ul><h2 id="CVPR-2020"><a href="#CVPR-2020" class="headerlink" title="CVPR 2020"></a>CVPR 2020</h2><p><strong>Cars Can’t Fly up in the Sky: Improving Urban-Scene Segmentation via Height-driven Attention Networks</strong></p><ul><li>论文：<a href="https://arxiv.org/abs/2003.05128" target="_blank" rel="noopener">https://arxiv.org/abs/2003.05128</a></li><li>代码：<a href="https://github.com/shachoi/HANet" target="_blank" rel="noopener">https://github.com/shachoi/HANet</a></li></ul><ol><li><p><del>Semi-Supervised Semantic Image Segmentation with Self-correcting Networks</del><br> 论文地址：<a href="https://arxiv.org/abs/1811.07073" target="_blank" rel="noopener">https://arxiv.org/abs/1811.07073</a></p><p> <img src="http://img.cdn.leonwang.top/image-20200320144840958.png" alt="image-20200320192300069"></p><p> 半监督</p></li></ol><ol start="2"><li><p>Deep Snake for Real-Time Instance Segmentation<br> 论文地址：<a href="https://arxiv.org/abs/2001.01629" target="_blank" rel="noopener">https://arxiv.org/abs/2001.01629</a></p><p> 提出了圆形卷积，基于轮廓的分割，在迭代过程中轮廓逐渐变形，接近对象的边界。和图卷积进行了对比</p></li></ol><ol start="3"><li>CenterMask : Real-Time Anchor-Free Instance Segmentation<br> 论文地址：<a href="https://arxiv.org/abs/1911.06667" target="_blank" rel="noopener">https://arxiv.org/abs/1911.06667</a><br> 代码：<a href="https://github.com/youngwanLEE/CenterMask" target="_blank" rel="noopener">https://github.com/youngwanLEE/CenterMask</a></li></ol><ol start="4"><li><p>SketchGCN: Semantic Sketch Segmentation with Graph Convolutional Networks<br> 论文地址：<a href="https://arxiv.org/abs/2003.00678" target="_blank" rel="noopener">https://arxiv.org/abs/2003.00678</a></p><p> 图卷积神经网络 手绘草图分割 把图片看成2D点集</p></li></ol><ol start="5"><li><p>PolarMask: Single Shot Instance Segmentation with Polar Representation<br> 论文地址：<a href="https://arxiv.org/abs/1909.13226" target="_blank" rel="noopener">https://arxiv.org/abs/1909.13226</a><br> 代码：<a href="https://github.com/xieenze/PolarMask" target="_blank" rel="noopener">https://github.com/xieenze/PolarMask</a></p><p> 极坐标 基于轮廓的分割</p></li></ol><ol start="6"><li><p>xMUDA: Cross-Modal Unsupervised Domain Adaptation for <strong>3D</strong> Semantic Segmentation<br> 论文地址：<a href="https://arxiv.org/abs/1911.12676" target="_blank" rel="noopener">https://arxiv.org/abs/1911.12676</a></p><p> 点云 无监督</p></li></ol><h2 id="医学图像"><a href="#医学图像" class="headerlink" title="医学图像"></a>医学图像</h2><h3 id="医学图像的特点"><a href="#医学图像的特点" class="headerlink" title="医学图像的特点"></a>医学图像的特点</h3><ol><li><p>图像语义较为简单、结构较为固定。</p></li><li><p>数据量少。医学影像的数据获取相对难一些，很多比赛只提供不到100例数据。所以我们设计的模型不宜多大，参数过多，很容易导致过拟合。</p></li><li><p>多模态。相比自然影像，医疗影像比较有趣和不同的一点是，医疗影像是具有多种模态的。以ISLES脑梗竞赛为例，其官方提供了CBF,MTT,CBV,TMAX,CTP等多种模态的数据。</p><p> 这就需要我们更好的设计网络去提取不同模态的特征feature。这里提供两篇论文供大家参考。 </p><p> Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical Segmentation(CVPR 2017) , </p><p> Dense Multi-path U-Net for Ischemic Stroke Lesion Segmentation in Multiple Image Modalities.</p></li><li><p>可解释性重要。由于医疗影像最终是辅助医生的临床诊断，所以网络告诉医生一个3D的CT有没有病是远远不够的，医生还要进一步的想知道，病灶在哪一层，在哪一层的哪个位置，分割出来了吗，能求体积嘛？同时对于网络给出的分类和分割等结果，医生还想知道为什么，所以一些神经网络可解释性的trick就有用处了，比较常用的就是画activation map。看网络的哪些区域被激活了，如下图。 </p><p> <img src="https://pic1.zhimg.com/v2-88f9571550eb123ce18c43ec99efdb4c_b.jpg" alt="img" style="zoom: 67%;"></p></li></ol><h3 id="AAAI-2020-医学图像分割的Non-local-U-Nets"><a href="#AAAI-2020-医学图像分割的Non-local-U-Nets" class="headerlink" title="[AAAI 2020] 医学图像分割的Non-local U-Nets"></a>[AAAI 2020] 医学图像分割的Non-local U-Nets</h3><p>global aggregation block 获取全图信息</p><p>减少参数量</p><h3 id="Multi-scale-self-guided-attention-for-medical-image-segmentation"><a href="#Multi-scale-self-guided-attention-for-medical-image-segmentation" class="headerlink" title="Multi-scale self-guided attention for medical image segmentation"></a>Multi-scale self-guided attention for medical image segmentation</h3><p><a href="https://arxiv.org/abs/1906.02849" target="_blank" rel="noopener">https://arxiv.org/abs/1906.02849</a></p><p>dual attention方法用于医学图像分割</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="弱监督学习"><a href="#弱监督学习" class="headerlink" title="弱监督学习"></a>弱监督学习</h3><p>图像分割数据标注是比较困难的</p><h4 id="Scribble标记"><a href="#Scribble标记" class="headerlink" title="Scribble标记"></a>Scribble标记</h4><p>论文地址：<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Lin_ScribbleSup_Scribble-Supervised_Convolutional_CVPR_2016_paper.pdf" target="_blank" rel="noopener">ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation (CVPR 2016)</a>  </p><p>只需要<strong>画五条线</strong>就能完成对一副图像的标记工作。</p><h4 id="图像级别标记"><a href="#图像级别标记" class="headerlink" title="图像级别标记"></a>图像级别标记</h4><p>论文地址：<a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Pathak_Constrained_Convolutional_Neural_ICCV_2015_paper.pdf" target="_blank" rel="noopener">Constrained Convolutional Neural Networks for Weakly Supervised Segmentation （ICCV 2015）</a>  </p><p><strong>只给出图像中包含某种物体</strong>，但是没有其位置信息和所包含的像素信息。该文章的方法将image tags转化为对CNN输出的label分布的限制条件，因此称为 Constrained convolutional neural network (CCNN).</p><h4 id="DeepLab-bounding-box-image-level-labels"><a href="#DeepLab-bounding-box-image-level-labels" class="headerlink" title="DeepLab+bounding box+image-level labels"></a>DeepLab+bounding box+image-level labels</h4><p>论文地址：<a href="https://arxiv.org/pdf/1502.02734.pdf" target="_blank" rel="noopener">Weakly-and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation</a>  </p><p>使用<strong>bounding box</strong>和<strong>image-level labels</strong>作为标记的训练数据。使用了期望值最大化算法（EM）来估计未标记的像素的类别和CNN的参数。</p><h4 id="统一的框架"><a href="#统一的框架" class="headerlink" title="统一的框架"></a>统一的框架</h4><p>提出了一个统一的框架来<strong>处理各种不同类型的弱标记</strong>：图像级别的标记、bounding box和部分像素标记如scribbles。</p><h4 id="弱监督学习的最新进展"><a href="#弱监督学习的最新进展" class="headerlink" title="弱监督学习的最新进展"></a>弱监督学习的最新进展</h4><ul><li><p>bbox监督</p><ul><li><p>Learning to Segment via Cut-and-Paste（ECCV 2018）</p><p>  利用GAN对抗学习的思想，在cut-paste思想指导下利用bbox弱监督进行实例分割。</p></li><li><p>Simple Does It: Weakly Supervised Instance and Semantic Segmentation（CVPR2017）</p><p>  讨论了使用弱监督语义标签进行迭代训练的方法，以及其限制和不足之处；证明了通过类似GrabCut的算法能通过bbox生成分割训练标签方法的可行性，可以避免像上面的迭代方法重新调整网络训练策略；在VOC数据集上逼近监督学习的分割任务效果。</p></li></ul></li><li><p>分类监督</p><ul><li>Weakly Supervised Learning of Instance Segmentation with Inter-pixel Relations(CVPR2019)</li><li>Weakly Supervised Instance Segmentation using Class Peak Response（CVPR2018）</li><li>Weakly Supervised Semantic Segmentation Using Superpixel Pooling Network（AAAI 2017）</li></ul></li></ul><h3 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h3><p>CVPR 2017 最佳论文</p><p>key：identify mapping</p><p><img src="http://img.cdn.leonwang.top/image-20200320143632042.png" alt="image-20200320143632042" style="zoom:25%;"></p><p>更密集的resnet，实现特征重用</p><p>和resnet的区别：1. resnet是值相加、densenet是通道合并 2. densenet的输出是后面所有层的输入</p><h3 id="空洞卷积-dilated-convolution"><a href="#空洞卷积-dilated-convolution" class="headerlink" title="空洞卷积 dilated convolution"></a>空洞卷积 dilated convolution</h3><p>传统的增大感受野的方式：先pooling 再deconv，会损失信息</p><p>空洞卷积：不做pooling损失信息的情况下也能有较大感受野</p><p><img src="http://img.cdn.leonwang.top/image-20200320145044074.png" alt="image-20200320145044074" style="zoom:25%;"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>特征提取网络的backbone</li><li>多尺度融合、高层特征与底层特征融合：编解码器、金字塔池化(spp)、多尺度空洞卷积(aspp)</li><li>Attention</li><li>loss function</li></ol>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 分割 </tag>
            
            <tag> 计算机视觉 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bias, Variance, N-fold Cross Validation</title>
      <link href="2020/03/15/Bias-Variance-N-fold-Cross-Validation/"/>
      <url>2020/03/15/Bias-Variance-N-fold-Cross-Validation/</url>
      
        <content type="html"><![CDATA[<h2 id="Bias-and-Variance"><a href="#Bias-and-Variance" class="headerlink" title="Bias and Variance"></a>Bias and Variance</h2><p>在模型选择时往往面临多种选择，例如进行多项式回归可以用一次多项式、二次多项式、…、五次多项式等。人们都知道模型太简单容易欠拟合，太复杂会导致过拟合，这个问题可以从模型的误差（error）角度给出解释。</p><p>模型的误差来自两个方面：Bias和Variance。</p><p>如何理解Bias和Variance呢？先举两个概率与统计中的例子（无偏估计和有偏估计）：</p><ol><li><p>用样本的均值$m$来估测随机变量的均值$μ$，分析m的bias和variance。</p><p> 通过计算m的期望和方差，可以知道m这个estimator是unbiased的，即bias是0，variance是$\sigma^2/N$。</p><p> 其中，n是每次取样中样本点的个数，不是取样的次数。可以看出，取样取的样本点越多，m的variance越小。</p></li></ol><p><img src="http://img.cdn.leonwang.top/20200315100536.png" style="zoom:33%;"></p><p><img src="http://img.cdn.leonwang.top/20200315102217.png" style="zoom:33%;"></p><ol start="2"><li><p>用样本的方差$s$来估测随机变量的方差$\sigma^2$，分析s的bias。</p><p> 通过计算s的期望，可以知道s这个estimator是biased的。</p><p> 其中，n是每次取样中样本点的个数，不是取样的次数。可以看出，取样取的样本点越多，E[s]越接近$\sigma^2$，也就是estimator的bias越小。</p></li></ol><p><img src="http://img.cdn.leonwang.top/20200315100702.png" style="zoom:33%;"></p><p><img src="http://img.cdn.leonwang.top/20200315165438.png" style="zoom:30%;"></p><font color="red">（其实上面用到的样本方差的概念是李宏毅博士在机器学习课程中用到的概念，是样本二阶中心矩，用样本二阶中心矩来估计总体方差（即总体二阶中心矩）是有偏估计。但是实际上，不论是在同济概统课本还是在浙大概统课本里，所谓的“样本方差”的概念都不是用的样本二阶中心矩，“样本方差”的系数的分母是n-1而不是n，这样用“样本方差”去估计“总体方差”就是无偏的。具体可以去看一下概率论与数理统计那篇blog post。）</font><p>总而言之，当用一个模型来估测一个数据时，bias表示这个模型的期望与ground truth之间的差距，variance表示这个模型的离散程度。</p><p>回到回归这个问题，把寻找合适的模型看作打靶，靶心是模型的ground truth，但实际这个靶心的具体formulation我们是不知道的。</p><p><img src="http://img.cdn.leonwang.top/20200315085814.png" width="500"></p><p>选择不同的模型进行回归，因为不同模型有不同的 bias 和 variance，所以最终也会有不同的error。</p><p>选定一类模型（假设是3次多项式），进行多次回归，能够得到多个不同的模型（例如每次回归设定的学习率不同，就将得到不同的训练效果），每一个模型在靶上都是一个点，但它们都属于同一类模型。<strong>把这些模型取平均，与靶心之间的差距，就是这类模型的bias。这些模型的分散程度，就是这个模型的variance。</strong></p><p>比较不同的模型，其中的黑线表示ground truth，蓝线表示对应模型的平均：</p><p><img src="http://img.cdn.leonwang.top/20200315091920.png" width="500"></p><p><strong>简单的模型（如线性回归）有大的 bias 和小的 variance</strong></p><p><strong>复杂的模型（如五次多项式回归）有小的 bias 和大的 variance</strong></p><h2 id="欠拟合与过拟合"><a href="#欠拟合与过拟合" class="headerlink" title="欠拟合与过拟合"></a>欠拟合与过拟合</h2><p>据此来解释<strong>过拟合</strong>与<strong>欠拟合</strong>的问题：</p><p><img src="http://img.cdn.leonwang.top/20200315092159.png" width="500"></p><p>简单的模型的error主要来自bias，属于欠拟合</p><p>复杂的模型的error主要来自variance，属于过拟合</p><p>如何处理太大的bias（欠拟合）：</p><ul><li>增加更多的features</li><li>选择更加复杂的model</li></ul><p>如何处理太大的variance（过拟合）：</p><ul><li>获取更多的数据（很有效，但是往往不现实）</li><li>正则化（曲线会更平滑，但有可能会导致bias增大）</li></ul><h2 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h2><h3 id="Cross-Validation"><a href="#Cross-Validation" class="headerlink" title="Cross Validation"></a>Cross Validation</h3><p>如果是参加比赛，会有public testing set，也会有private testing set。在public上error小，往往不一定在private上的error也小。</p><p>由于这种做法会把public testing set中的某些特点考虑进去，所以直接选择在public testing set上error最小的model往往不是最好的选择，一个比较好的做法是采用交叉验证（Cross Validation）。</p><p><img src="http://img.cdn.leonwang.top/20200315093711.png" style="zoom: 33%;"></p><h3 id="N-fold-Cross-Validation"><a href="#N-fold-Cross-Validation" class="headerlink" title="N-fold Cross Validation"></a>N-fold Cross Validation</h3><p>如果不相信training set和validation set划分的足够好，可以采用N折交叉验证。</p><p><img src="http://img.cdn.leonwang.top/20200315094023.png" style="zoom:33%;"></p><p>比如图中所示，把训练集分成3折，来从三类model中进行选择。对应训练集的三种划分情况，分别训练三类模型，最终得到九个训练好的模型，计算每个模型在各自的validation set上的error。</p><p>计算每一类模型在所有划分情况下的平均error，选择平均error最小的那类Model，这个例子中是Model 1。</p><p>然后用Model 1重新在全体训练集上训练一次，训练出了最终模型。</p><h3 id="N折交叉验证的意义"><a href="#N折交叉验证的意义" class="headerlink" title="N折交叉验证的意义"></a>N折交叉验证的意义</h3><ul><li><strong>防止过拟合</strong>，尤其是在数据集样本量较少的情况下。</li></ul><p>上面讲的比赛中public测试集和private测试集的问题，实际上是希望通过交叉验证的方式，避免在训练时考虑public testing set中一些独有的特点，从而避免模型在public测试集上error小却在private测试集上error大的现象。所以这本质上也是在解决过拟合的问题。</p><p>而在实际项目中，N折交叉验证更常用于解决样本量少、训练集测试集划分地不好而引起的过拟合。</p><blockquote><p>Reference: Dr Hung-yi Lee’s Machine Learning course</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 过拟合 </tag>
            
            <tag> 欠拟合 </tag>
            
            <tag> Cross Validation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>概率论与数理统计</title>
      <link href="2020/03/01/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/"/>
      <url>2020/03/01/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/</url>
      
        <content type="html"><![CDATA[<p>做点概率论的笔记✍️</p><p>TODO: 切比雪夫不等式、辛钦大数定律</p><h2 id="概率论"><a href="#概率论" class="headerlink" title="概率论"></a>概率论</h2><p>TODO</p><h2 id="数理统计"><a href="#数理统计" class="headerlink" title="数理统计"></a>数理统计</h2><h3 id="数理统计的概念"><a href="#数理统计的概念" class="headerlink" title="数理统计的概念"></a>数理统计的概念</h3><p><strong>数理统计学</strong>是一门以数据为基础的学科. </p><p>数理统计学的<strong>任务</strong>就是如何<strong>获得样本</strong>和<strong>利用样本</strong>，从而<strong>对事物的某些未知方面进行分析、推断并作出一定的决策</strong>。</p><p>研究问题的基本方式：以<strong>部分数据</strong>信息来<strong>推断整体</strong>未知参数</p><h3 id="总体与样本"><a href="#总体与样本" class="headerlink" title="总体与样本"></a>总体与样本</h3><p><strong>总体</strong>：研究对象的全体；</p><p><strong>个体</strong>：总体中的成员；</p><p>如何推断总体分布的未知参数（或分布）？</p><p>需要从总体中抽取一部分个体, 根据这部分个体的数据，并利用概率论的知识等作出分析推断. 被抽取的部分个体叫做总体的一个<strong>样本</strong>.</p><blockquote><p>进行一次随机抽样，获得一个样本，一个样本中包含多个样本观测值</p></blockquote><h3 id="样本统计量"><a href="#样本统计量" class="headerlink" title="样本统计量"></a>样本统计量</h3><p>实际上就是一个样本的<strong>函数</strong>，这个函数不包含任何未知参数，只要取得一个样本，就能计算出这个样本的统计量。</p><p>区分<strong>总体数字特征</strong>和<strong>样本统计量</strong>：前者描述总体，后者描述样本。用样本统计量去估计总体的数字特征。</p><p>总体的数字特征如：</p><ul><li>总体均值 $\mu$</li><li>总体方差 $\sigma^2$</li><li>总体矩</li></ul><p>样本的常用统计量有：</p><ul><li><p>样本均值 $\bar{X}=\frac{1}{n} \sum_{i=1}^{n} X_{i}$</p></li><li><p>样本方差 $S^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}$ （注意系数分母是n-1，样本2阶原点矩的系数分母是n）</p><p>  样本标准差 $S=\sqrt{S^{2}}$</p></li><li><p>样本矩</p><ul><li>k阶原点矩 $A_{k}=\frac{1}{n} \sum_{i=1}^{n} X_{i}^{k}$</li><li><p>k阶中心矩 $B_{k}=\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{k}$</p><blockquote><p>$B_{2}=\frac{n-1}{n} S^{2}$</p></blockquote></li></ul></li></ul><p>如何用样本统计量估计总体数字特征：</p><p><img src="http://img.cdn.leonwang.top/20200315133219.png" style="zoom:40%;"></p><p>统计量的分布——<strong>抽样分布</strong>：</p><p><img src="http://img.cdn.leonwang.top/20200315134249.png" style="zoom:40%;"></p><p>而如果总体服从正态分布，则统计量的分布有三类重要的分布：</p><ul><li>$\chi^{2}$分布</li><li>t分布</li><li>F分布</li></ul><h3 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h3><p>区分估计量和估计值：</p><ul><li>估计量</li><li>估计值：把具体数值带入估计量，计算出估计值</li></ul><p>总体分布已知，但分布的具体的参数中可能有未知参数。<strong>参数估计</strong>就是根据样本来估计总体分布中的未知参数。</p><ul><li><p>点估计</p><p>  以样本的某一函数值作为未知参数的估计值。</p><ol><li><p>矩估计</p><p> 基本思想：<strong>用样本矩去估计总体矩，用样本矩的函数估计总体矩的函数</strong>。</p><p> 理论依据：根据<strong>辛钦大数定律</strong>，当总体的k阶矩存在时，样本的k阶矩<strong>依概率收敛</strong>于总体的k阶矩。</p><p> 由于大数定律，当样本容量趋近于无穷时，矩估计有很好的性质。</p><p> 矩估计不涉及总体的分布</p><p> 矩估计的结果不唯一，使用的矩不一样，求出来的结果可能不一样</p><p> 求解步骤：</p><ol><li>求总体矩关于参数的函数</li><li>求参数关于总体矩关于参数的反函数</li><li>以样本矩代替总体矩，求得参数的矩估计量。</li></ol></li><li><p>极大似然估计</p><p> 基本思想：考虑当前的情形，为了估计参数，我们取到了n个样本值。为什么会取到这n个样本值呢？<strong>因为我们认为在当前参数条件下，取到这n个样本值的概率最大</strong>（或者说出现这种抽样结果的可能性最大）。出于这样的考虑，我们去<strong>寻求使这个抽样结果出现的可能性最大的那个值来作为参数估计值</strong>。</p><p> 似然函数：$L(\theta)=\prod_{i=1}^{n} p\left(x_{i} ; \theta\right)$</p><p> 似然函数是一个关于参数的函数，表示在该参数下取到当前样本的概率。</p><p> 极大似然原理：$L\left(\hat{\theta}\left(x_{1}, \ldots, x_{n}\right)\right)=\max _{\theta \in \Theta} L(\theta)$</p><p> 通常把求<strong>似然函数</strong>的最大值问题转化为求<strong>对数似然函数</strong>的最大值问题，目的是为了简化计算</p><p> 对数似然函数：$\ln L(\theta)$</p><p> 求解步骤：</p><ol><li>写出似然函数$L(\theta)$</li><li>取对数，得到对数似然函数$\ln L(\theta)$</li><li>通过求解导数等于零的方程，解决参数的<strong>极大似然估计量</strong>（是一个关于样本值得函数）</li><li><p>把样本值带入上面的极大似然估计量，得到<strong>极大似然估计值</strong></p><blockquote><p>矩估计量是关于样本矩的函数，极大似然估计值是关于样本值的函数</p></blockquote></li></ol></li></ol></li><li><p>区间估计</p><p>  将未知参数确定在一个范围内，并在一定可靠度下使这个范围包含未知参数的真值。</p></li></ul><h3 id="估计量的评选标准"><a href="#估计量的评选标准" class="headerlink" title="估计量的评选标准"></a>估计量的评选标准</h3><p>不同的估计方法可能得到不同的估计值，如何评价不同估计的好坏？</p><ul><li><p>无偏性准则</p><p>  无偏估计量：$E(\hat{\theta})=\theta$，估计量$\hat \theta$的<strong>数学期望</strong>等于$\theta$</p><p>  渐近无偏估计量：$\lim _{n \rightarrow \infty} E(\hat{\theta})=\theta$</p><p>  无偏性的统计意义：在<strong>大量重复试验</strong>下，$\hat \theta$的估计的<strong>平均</strong>是$\theta$。所以无偏性保证了$\hat \theta$<strong>没有系统误差</strong>。</p><ul><li>样本均值是总体均值的无偏估计</li><li>样本方差是总体方差的无偏估计</li><li>样本二阶中心矩是总体方差的有偏估计、渐近无偏估计</li></ul></li><li><p>有效性准则</p><p>  <strong>方差较小</strong>的无偏估计量是一个更有效的估计量。</p></li><li><p>均方误差准则</p></li><li><p>相合性准则</p><p>  区分渐近无偏性和相合性：</p><ul><li>渐近无偏性是指当样本容量趋近无穷时，<strong>参数估计量的均值(期望)</strong>趋近于参数真实值。</li><li>相合性是指当样本容量趋近无穷时，<strong>参数估计值</strong>趋近于参数真实值。</li></ul></li></ul><h3 id="样本均值、样本方差的无偏性和样本二阶中心矩的渐近无偏性的证明"><a href="#样本均值、样本方差的无偏性和样本二阶中心矩的渐近无偏性的证明" class="headerlink" title="样本均值、样本方差的无偏性和样本二阶中心矩的渐近无偏性的证明"></a>样本均值、样本方差的无偏性和样本二阶中心矩的渐近无偏性的证明</h3><h4 id="符号表示"><a href="#符号表示" class="headerlink" title="符号表示"></a>符号表示</h4><table><thead><tr><th>符号</th><th>总体特征/样本统计量</th></tr></thead><tbody><tr><td>$\mu=E(X)$</td><td>总体期望(均值)</td></tr><tr><td>$\sigma^2=D(X)$</td><td>总体方差</td></tr><tr><td>$\bar{X}=\frac{1}{n} \sum_{i=1}^{n} X_{i}$</td><td>样本均值</td></tr><tr><td>$S^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}$</td><td>样本方差</td></tr><tr><td>$B_{2}=\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}$</td><td>样本二阶中心矩</td></tr></tbody></table><h4 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h4><h5 id="1-样本均值-bar-X-是总体均值-mu-的无偏估计"><a href="#1-样本均值-bar-X-是总体均值-mu-的无偏估计" class="headerlink" title="1. 样本均值$\bar{X}$是总体均值$\mu$的无偏估计"></a>1. 样本均值$\bar{X}$是总体均值$\mu$的无偏估计</h5><p>$\begin{aligned} E(\bar{X}) &amp;=E\left(\frac{1}{n} \sum_{i=1}^{n} X_{i}\right)=\frac{1}{n} \sum_{i=1}^{n} E\left(X_{i}\right)=\frac{1}{n} \cdot n \mu=\mu \end{aligned}$</p><h5 id="2-样本方差-S-2-是总体均值-sigma-2-的无偏估计"><a href="#2-样本方差-S-2-是总体均值-sigma-2-的无偏估计" class="headerlink" title="2. 样本方差$S^{2}$是总体均值$\sigma^2$的无偏估计"></a>2. 样本方差$S^{2}$是总体均值$\sigma^2$的无偏估计</h5><p>和下面第三个结论的证明过程是一样的，只是把系数$\frac{1}{n}$换成$\frac{1}{n-1}$，最后期望等于$\sigma ^2$。因为先写了下面的证明，这里就不写了。</p><h5 id="3-样本二阶中心矩-B-2-是总体均值-sigma-2-的有偏估计、渐近无偏估计"><a href="#3-样本二阶中心矩-B-2-是总体均值-sigma-2-的有偏估计、渐近无偏估计" class="headerlink" title="3. 样本二阶中心矩$B_2$是总体均值$\sigma^2$的有偏估计、渐近无偏估计"></a>3. 样本二阶中心矩$B_2$是总体均值$\sigma^2$的有偏估计、渐近无偏估计</h5><p><img src="http://img.cdn.leonwang.top/20200315174822.png" style="zoom:30%;"></p><p>因为$E(B_2)\neq \sigma^2$，所以不是无偏估计。</p><p>又因为$\lim _{n \rightarrow \infty} E\left(B_{2}\right)=\lim _{n \rightarrow \infty} \frac{n-1}{n} \sigma^{2}=\sigma^{2}$，所以是渐近无偏估计。</p><blockquote><p>References: </p><ol><li>同济版《概率论与数理统计》</li><li>浙大版《概率论与数理统计》</li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 概率论与数理统计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>奇异值分解 (SVD, Singular Value Decomposition)</title>
      <link href="2020/02/29/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3-SVD-Singular-Value-Decomposition/"/>
      <url>2020/02/29/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3-SVD-Singular-Value-Decomposition/</url>
      
        <content type="html"><![CDATA[<p>特征值分解的方法只适用于实对称矩阵，对于更一般的矩阵，一般采用奇异值分解。这里简单记录一下有关奇异值和奇异值分解的内容，不详细论述。</p><p>博客插件对LaTex数学公式中换行符”\\”的支持不是很完美，研究了一下，是因为生成静态页面的时候markdown中的”\\”都被认为是”\”的转义字符，公式中的换行符也被转移成了”\”，所以换行符不能被正常编译显示，导致用到换行符的公式都显示有问题甚至直接挂掉了orz…感觉是hexo-math插件背锅，难搞哦🤕</p><p>找到了上述问题的解决方案：<a href="https://www.sail.name/2018/05/31/use-mathjax-in-hexo/" target="_blank" rel="noopener">在hexo使用mathjax</a>。改了源码，原来挂掉的现在不会挂了，但还是无法换行，暂时先把\\手动改成\\\\了，以后有时间再搞搞。</p><blockquote><p>参考教材：《矩阵论(第3版)》程文鹏 著</p></blockquote><h2 id="论述前提"><a href="#论述前提" class="headerlink" title="论述前提"></a>论述前提</h2><p><img src="http://img.cdn.leonwang.top/20200229011023.png" alt=""></p><blockquote><p>$\boldsymbol{A} \in \mathbf{C}_{r}^{m \times n}$表示A为m行n列的矩阵，矩阵的秩为r</p><p>$A^H$在实数范围内等价于$A^T$</p></blockquote><h2 id="奇异值"><a href="#奇异值" class="headerlink" title="奇异值"></a>奇异值</h2><p>定义：</p><p><img src="http://img.cdn.leonwang.top/20200229011058.png" alt=""></p><h2 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p><img src="http://img.cdn.leonwang.top/20200229011310.png" alt=""></p><p>(4.4.4)式通过证明表示成如下形式就是奇异值分解：<br>$$<br>A=U\left[\begin{array}{ll}\Sigma &amp; 0 \\\\<br>0 &amp; 0\end{array}\right] V^{T}<br>$$</p><blockquote><p>酉矩阵在实数范围内等价于正交矩阵，即$A^T=A^{-1}$</p></blockquote><h3 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h3><p>在证明过程中构造了V和U，可以通过学习证明过程学习如何构造V和U，如何进行奇异值分解的计算。</p><p><img src="http://img.cdn.leonwang.top/20200229011839.png" alt=""></p><p><img src="http://img.cdn.leonwang.top/20200229011858.png" alt=""></p><p><img src="http://img.cdn.leonwang.top/20200229011914.png" alt=""></p><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p><img src="http://img.cdn.leonwang.top/20200229011952.png" alt=""></p><p><img src="http://img.cdn.leonwang.top/20200229012014.png" alt=""></p><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p>说点人话，奇异值分解，就是在矩阵A左右两边分别乘以 $U^H$ 和 $V$，得到一个对称矩阵，最后通过证明得到了下面的形式：</p><p>$$<br>A=U\left[\begin{array}{ll}\Sigma &amp; 0 \\\ 0 &amp; 0\end{array}\right] V^{T}<br>$$</p><p>也就是说，<strong>任意一个矩阵A，都可以分解成”正交矩阵1*对角矩阵*正交矩阵2“的形式。</strong></p><p>上式中各自的shape如下：<br>$$<br>\begin{array}{l}\mathrm{A}: \mathrm{m} \times \mathrm{n} \\\\<br>\mathrm{U}: \mathrm{m} \times \mathrm{m} \\\ <br>\left[\begin{array}{cc}\Sigma &amp; 0 \\\ 0 &amp; 0\end{array}\right]: \mathrm{m} \times \mathrm{n} \\\ <br>V^{T}: \mathrm{n} \times \mathrm{n}\end{array}<br>$$</p><p>如果矩阵A的秩$R(A)=r$，则$\Sigma$是$r×r$对角阵，对角线上的元素就是 $A^TA$ 的不为0的特征值的算术平方根（或者说是A的非零奇异值），把U和V写成列向量的形式：</p><p>$$<br>A=\left[u_{1}, u_{2}, \ldots, u_{r}, u_{r+1}, \ldots, u_{m}\right]\left[\begin{array}{ccccccc}\sigma_{1} &amp; 0 &amp; \cdots &amp; &amp; &amp; &amp; \\\ 0 &amp; \sigma_{2} &amp; 0 &amp; \cdots &amp; &amp; &amp; \\\ \vdots &amp; 0 &amp; \ddots &amp; &amp; &amp; &amp; \\\ &amp; \vdots &amp; 0 &amp; \sigma_{r} &amp; 0 &amp; &amp; \\\ &amp; &amp; &amp; &amp; 0 &amp; &amp; \\\ &amp; &amp; &amp; &amp; &amp; \ddots &amp; 0 \\\ &amp; &amp; &amp; &amp; &amp; 0 &amp; 0\end{array}\right]_{m \times n}\left[\begin{array}{c}v_{1}^{T} \\\ \vdots \\\ v_{r}^{T} \\\ \vdots \\\ v_{n}^{T}\end{array}\right]<br>$$</p><p>展开后：</p><p>$$<br>A=\sigma_{1} u_{1} v_{1}^{T}+\sigma_{2} u_{2} v_{2}^{T}+\ldots+\sigma_{r} u_{r} v_{r}^{T}<br>$$</p><p>根据奇异值的定义，有：</p><p>$$<br>\sigma_{1} \geq \sigma_{2} \geq \ldots \geq \sigma_{r}&gt;0<br>$$</p><p>根据上式可以分析SVD的一些应用：</p><h4 id="图像压缩"><a href="#图像压缩" class="headerlink" title="图像压缩"></a>图像压缩</h4><h4 id="矩阵乘法加速"><a href="#矩阵乘法加速" class="headerlink" title="矩阵乘法加速"></a>矩阵乘法加速</h4>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数学 </tag>
            
            <tag> 线性代数 </tag>
            
            <tag> 奇异值 </tag>
            
            <tag> 奇异值分解 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linear Algebra</title>
      <link href="2020/02/29/Linear-Algebra/"/>
      <url>2020/02/29/Linear-Algebra/</url>
      
        <content type="html"><![CDATA[<p>记录一些对线性代数的理解。边学习边补充。😵</p><h2 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h2><p>矩阵描述的是向量空间中的<strong>线性变换（映射）</strong>，其本质是线性变换的数学表示。</p><ul><li>矩阵描述的是向量的变换，而且只局限在线性变换。</li><li>线性变换的两条性质：<ol><li>所有的直线经过变换之后仍是直线</li><li>原点保持固定（如果满足第一条性质，但是原点会移动，则是仿射变换，不是线性变换）</li></ol></li></ul><h2 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h2><p>矩阵乘法为何如此定义？为了实现<strong>复合变换</strong>的定义。两个矩阵相乘，可以看作两个矩阵分别表示的线性变换复合之后的复合变换。</p><h2 id="特征值与特征向量"><a href="#特征值与特征向量" class="headerlink" title="特征值与特征向量"></a>特征值与特征向量</h2><p>一个特定的矩阵表示一种特定的线性变换，而这个矩阵的特征向量是在<strong>这种线性变换下的一类特殊的向量</strong>。这类向量的的特点是，经过线性变换后，它们的<strong>方向不变，只有长度改变，长度变化的倍数就是特征值</strong>。</p><p>一个特征值对应无数个特征向量，通常写范数为1的那个。</p><h2 id="两种分解"><a href="#两种分解" class="headerlink" title="两种分解"></a>两种分解</h2><p>特征值分解：即同济线性代数课本中的”<strong>对称矩阵的对角化</strong>“，课本中的讨论仅<strong>适用于实对称矩阵</strong>。</p><p>奇异值分解：<strong>适用于任意矩阵</strong>。</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数学 </tag>
            
            <tag> 线性代数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>朴素贝叶斯分类算法</title>
      <link href="2019/11/11/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
      <url>2019/11/11/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<ol><li>朴素贝叶斯算法是<strong>分类</strong>算法。</li><li>朴素贝叶斯算法基于<strong>贝叶斯公式</strong>和<strong>特征条件独立假设</strong>。</li><li>所谓”<strong>朴素</strong>“，指的是<strong>特征之间是条件独立的</strong>。</li><li>根据后验概率最大化来确定最终分类。《统计学习方法》对后验概率最大化的解释是，从令损失函数最小化推导转化而来。更为直接得来看，根据已知的数据集，在当前给出的特征下，出现哪种类型的概率最大，则预测是这种类型。</li><li>推导的中间结果，在预测类别时，需要根据不同的类别 $c_k$ 求 $arg max_{c_k}$ ，在利用贝叶斯公式计算时，不同 $c_k$ 对应的分母是相同的，所以只需要求最大化分子即可。</li></ol><p>下面的一篇文章来自知乎：<a href="https://zhuanlan.zhihu.com/p/26262151" target="_blank" rel="noopener">带你理解朴素贝叶斯分类算法</a>。有助于理解朴素贝叶斯算法的基本概念。但文章中有一处错误：</p><p><img src="http://img.cdn.leonwang.top/20191111165011.png" alt=""></p><p>作者这里在分母上的换算是错误的，因为<strong>特征条件独立并不是特征独立，计算分母的联合特征概率时应该用贝叶斯全概率公式</strong>，这样就可转化为特征条件独立进行计算，即<code>p(不帅、性格不好、不上进，矮)=p(嫁)*p(不帅、性格不好、不上进，矮|嫁)+p(不嫁)*p(不帅、性格不好、不上进，矮|不嫁)</code>。</p><hr><p><strong>《带你理解朴素贝叶斯分类算法》</strong>   ——<a href="https://www.zhihu.com/people/qinlibo_nlp/activities" target="_blank" rel="noopener">@忆臻</a></p><p>贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。而<strong>朴素贝叶斯分类是贝叶斯分类中最简单，也是常见的一种分类方法</strong>。这篇文章我尽可能用直白的话语总结一下我们学习会上讲到的朴素贝叶斯分类算法，希望有利于他人理解。</p><h2 id="分类问题综述"><a href="#分类问题综述" class="headerlink" title="分类问题综述"></a><strong>分类问题综述</strong></h2><p><strong>对于分类问题，其实谁都不会陌生，日常生活中我们每天都进行着分类过程。</strong>例如，当你看到一个人，你的脑子下意识判断他是学生还是社会上的人；你可能经常会走在路上对身旁的朋友说“这个人一看就很有钱、”之类的话，其实这就是一种分类操作。</p><p>既然是贝叶斯分类算法，那么<strong>分类的数学描述</strong>又是什么呢？</p><p><strong>从数学角度来说，分类问题可做如下定义：已知集合<img src="https://www.zhihu.com/equation?tex=C%3D%7B%7By_%7B1%7D%2Cy_%7B2%7D%2C....y_%7Bn%7D%7D+%7D" alt="[公式]">和<img src="https://www.zhihu.com/equation?tex=I%3Dx_%7B1%7D%2C+x_%7B2%7D%2C+x_%7B3%7D......x_%7Bn%7D" alt="[公式]">，确定映射规则y = f()，使得任意<img src="https://www.zhihu.com/equation?tex=x_%7Bi%7D%5Cepsilon++I" alt="[公式]">有且仅有一个<img src="https://www.zhihu.com/equation?tex=y_%7Bi%7D%5Cepsilon+C+" alt="[公式]">,使得<img src="https://www.zhihu.com/equation?tex=y_%7Bi%7D%5Cepsilon+f%28x_%7Bi%7D+%29+" alt="[公式]">成立</strong>。</p><p>其中C叫做类别集合，其中每一个元素是一个类别，而I叫做项集合（<strong>特征集合</strong>），其中每一个元素是一个待分类项，f叫做分类器。<strong>分类算法的任务就是构造分类器f。</strong></p><p><strong>分类算法的内容是要求给定特征，让我们得出类别，这也是所有分类问题的关键。那么如何由指定特征，得到我们最终的类别，也是我们下面要讲的，每一个不同的分类算法，对应着不同的核心思想。</strong></p><p><strong>本篇文章，我会用一个具体实例，对朴素贝叶斯算法几乎所有的重要知识点进行讲解。</strong></p><h2 id="朴素贝叶斯分类"><a href="#朴素贝叶斯分类" class="headerlink" title="朴素贝叶斯分类"></a><strong>朴素贝叶斯分类</strong></h2><p>那么既然是朴素贝叶斯<strong>分类算法</strong>，它的核心算法又是什么呢？</p><p><strong>是下面这个贝叶斯公式：</strong></p><p><strong><img src="http://img.cdn.leonwang.top/20191111181026.png" alt=""></strong></p><p>换个表达形式就会明朗很多，如下：</p><p><img src="http://img.cdn.leonwang.top/20191111181045.png" alt=""></p><p>我们最终求的p(类别|特征)即可！就相当于完成了我们的任务。</p><h2 id="例题分析"><a href="#例题分析" class="headerlink" title="例题分析"></a><strong>例题分析</strong></h2><p><strong>下面我先给出例子问题。</strong></p><p><strong>给定数据如下：</strong></p><p><img src="http://img.cdn.leonwang.top/20191111181107.png" alt=""></p><p><strong>现在给我们的问题是，如果一对男女朋友，男生想女生求婚，男生的四个特点分别是不帅，性格不好，身高矮，不上进，请你判断一下女生是嫁还是不嫁？</strong></p><p>这是一个典型的分类问题，<strong>转为数学问题就是比较p(嫁|(不帅、性格不好、身高矮、不上进))与p(不嫁|(不帅、性格不好、身高矮、不上进))的概率</strong>，谁的概率大，我就能给出嫁或者不嫁的答案！</p><p>这里我们联系到朴素贝叶斯公式：</p><p><img src="http://img.cdn.leonwang.top/20191111181129.png" alt=""></p><p>我们需要求p(嫁|(不帅、性格不好、身高矮、不上进),这是我们不知道的，但是通过朴素贝叶斯公式可以转化为好求的三个量，</p><p>p(不帅、性格不好、身高矮、不上进|嫁)、p（不帅、性格不好、身高矮、不上进)、p(嫁)（至于为什么能求，后面会讲，那么就太好了，将待求的量转化为其它可求的值，这就相当于解决了我们的问题！）</p><h2 id="朴素贝叶斯算法的朴素一词解释"><a href="#朴素贝叶斯算法的朴素一词解释" class="headerlink" title="朴素贝叶斯算法的朴素一词解释"></a><strong>朴素贝叶斯算法的朴素一词解释</strong></h2><p><strong>那么这三个量是如何求得？</strong></p><p>是根据已知训练数据统计得来，下面详细给出该例子的求解过程。</p><p>回忆一下我们要求的公式如下：</p><p><img src="http://img.cdn.leonwang.top/20191111181149.png" alt=""></p><p>那么我只要求得p(不帅、性格不好、身高矮、不上进|嫁)、p（不帅、性格不好、身高矮、不上进)、p(嫁)即可，好的，下面我分别求出这几个概率，最后一比，就得到最终结果。</p><p><strong>p(不帅、性格不好、身高矮、不上进|嫁) = p(不帅|嫁)*p(性格不好|嫁)*p(身高矮|嫁)*p(不上进|嫁)，那么我就要分别统计后面几个概率，也就得到了左边的概率！</strong></p><p>等等，为什么这个成立呢？学过概率论的同学可能有感觉了，这个等式成立的条件需要特征之间相互独立吧！</p><p><strong>对的！这也就是为什么朴素贝叶斯分类有朴素一词的来源，朴素贝叶斯算法是假设各个特征之间相互独立，那么这个等式就成立了！</strong></p><p><strong>但是为什么需要假设特征之间相互独立呢？</strong></p><p>1、我们这么想，假如没有这个假设，那么我们对右边这些概率的估计其实是不可做的，这么说，我们这个例子有4个特征，其中帅包括{帅，不帅}，性格包括{不好，好，爆好}，身高包括{高，矮，中}，上进包括{不上进，上进}，<strong>那么四个特征的联合概率分布总共是4维空间，总个数为2*3*3*2=36个。</strong></p><p><strong>24个，计算机扫描统计还可以，但是现实生活中，往往有非常多的特征，每一个特征的取值也是非常之多，那么通过统计来估计后面概率的值，变得几乎不可做，这也是为什么需要假设特征之间独立的原因。</strong></p><p>2、假如我们没有假设特征之间相互独立，那么我们统计的时候，就需要在整个特征空间中去找，比如统计p(不帅、性格不好、身高矮、不上进|嫁),</p><p><strong>我们就需要在嫁的条件下，去找四种特征全满足分别是不帅，性格不好，身高矮，不上进的人的个数，这样的话，由于数据的稀疏性，很容易统计到0的情况。 这样是不合适的。</strong></p><p>根据上面俩个原因，朴素贝叶斯法对条件概率分布做了条件独立性的假设，由于这是一个较强的假设，朴素贝叶斯也由此得名！这一假设使得朴素贝叶斯法变得简单，但有时会牺牲一定的分类准确率。</p><p>好的，上面我解释了为什么可以拆成分开连乘形式。那么下面我们就开始求解！</p><p>我们将上面公式整理一下如下：</p><p><img src="http://img.cdn.leonwang.top/20191111181206.png" alt=""></p><p>下面我将一个一个的进行统计计算（</p><p>在数据量很大的时候，根据中心极限定理，频率是等于概率的，这里只是一个例子，所以我就进行统计即可</p><p>）。</p><p>p(嫁)=？</p><p>首先我们整理训练数据中，嫁的样本数如下：</p><p><img src="http://img.cdn.leonwang.top/20191111181224.png" alt=""></p><p>则 p(嫁) = 6/12（总样本数） = 1/2</p><p>p(不帅|嫁)=？统计满足样本数如下：</p><p><img src="http://img.cdn.leonwang.top/20191111181243.png" alt=""></p><p>则p(不帅|嫁) = 3/6 = 1/2</p><p><strong>p(性格不好|嫁)= ？统计满足样本数如下：</strong></p><p><img src="http://img.cdn.leonwang.top/20191111181300.png" alt=""></p><p><strong>则p(性格不好|嫁)= 1/6</strong></p><p><strong>p（矮|嫁） = ?统计满足样本数如下：</strong></p><p><img src="http://img.cdn.leonwang.top/20191111181315.png" alt=""></p><p><strong>则p(矮|嫁) = 1/6</strong></p><p><strong>p(不上进|嫁) = ?统计满足样本数如下：</strong></p><p><strong><img src="http://img.cdn.leonwang.top/20191111181330.png" alt=""></strong></p><p>则p(不上进|嫁) = 1/6</p><p><strong>下面开始求分母，p(不帅)，p（性格不好），p（矮），p（不上进）</strong></p><p><strong>统计样本如下：</strong></p><p><img src="http://img.cdn.leonwang.top/20191111181354.png" alt=""></p><p><strong>不帅统计如上红色所示，占4个，那么p（不帅） = 4/12 = 1/3</strong></p><p><img src="http://img.cdn.leonwang.top/20191111181413.png" alt=""></p><p>性格不好统计如上红色所示，占4个，那么p（性格不好） = 4/12 = 1/3</p><p><img src="http://img.cdn.leonwang.top/20191111181431.png" alt=""></p><p>身高矮统计如上红色所示，占7个，那么p（身高矮） = 7/12</p><p><img src="http://img.cdn.leonwang.top/20191111181452.png" alt=""></p><p>不上进统计如上红色所示，占4个，那么p（不上进） = 4/12 = 1/3</p><p><strong>到这里，要求p(不帅、性格不好、身高矮、不上进|嫁)的所需项全部求出来了，下面我带入进去即可，</strong></p><p><img src="http://img.cdn.leonwang.top/20191111181506.png" alt=""></p><p>= (1/2<em>1/6</em>1/6<em>1/6</em>1/2)/(1/3<em>1/3</em>7/12*1/3)</p><p><strong>下面我们根据同样的方法来求p(不嫁|不帅，性格不好，身高矮，不上进)，完全一样的做法，为了方便理解，我这里也走一遍帮助理解。首先公式如下：</strong></p><p><img src="http://img.cdn.leonwang.top/20191111181521.png" alt=""></p><p>下面我也一个一个来进行统计计算，这里与上面公式中，分母是一样的，于是我们分母不需要重新统计计算！</p><p>p（不嫁）=？根据统计计算如下（<strong>红色为满足条件</strong>）：</p><p><img src="http://img.cdn.leonwang.top/20191111181535.png" alt=""></p><p>则p(不嫁)=6/12 = 1/2</p><p>p(不帅|不嫁) = ？统计满足条件的样本如下（<strong>红色为满足条件</strong>）：</p><p><img src="http://img.cdn.leonwang.top/20191111181558.png" alt=""></p><p>则p（不帅|不嫁） = 1/6</p><p>p（性格不好|不嫁） = ？据统计计算如下（<strong>红色为满足条件</strong>）：</p><p><img src="http://img.cdn.leonwang.top/20191111181613.png" alt=""></p><p>则p（性格不好|不嫁） =3/6 = 1/2</p><p>p（矮|不嫁） = ？据统计计算如下（红色为满足条件）：</p><p><img src="http://img.cdn.leonwang.top/20191111181628.png" alt=""></p><p>则p（矮|不嫁） = 6/6 = 1</p><p>p（不上进|不嫁） = ？据统计计算如下（红色为满足条件）：</p><p><img src="http://img.cdn.leonwang.top/20191111181648.png" alt=""></p><p>则p（不上进|不嫁） = 3/6 = 1/2</p><p>那么根据公式：</p><p><img src="http://img.cdn.leonwang.top/20191111181702.png" alt=""></p><p>p (不嫁|不帅、性格不好、身高矮、不上进) = ((1/6<em>1/2</em>1<em>1/2)</em>1/2)/(1/3<em>1/3</em>7/12*1/3)</p><p><strong>很显然(1/6*1/2*1*1/2) &gt; (1/2*1/6*1/6*1/6*1/2)</strong></p><p><strong>于是有p (不嫁|不帅、性格不好、身高矮、不上进)&gt;p (嫁|不帅、性格不好、身高矮、不上进)</strong></p><p><strong>所以我们根据朴素贝叶斯算法可以给这个女生答案，是不嫁！！！！</strong></p><h2 id="朴素贝叶斯分类的优缺点"><a href="#朴素贝叶斯分类的优缺点" class="headerlink" title="朴素贝叶斯分类的优缺点"></a><strong>朴素贝叶斯分类的优缺点</strong></h2><p>优点：</p><p>（1） 算法逻辑简单,易于实现</p><p>（2）分类过程中时空开销小</p><p>缺点：</p><p>理论上，<strong>朴素贝叶斯模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为朴素贝叶斯模型假设属性之间相互独立，这个假设在实际应用中往往是不成立的，在属性个数比较多或者属性之间相关性较大时，分类效果不好。</strong></p><p>而在属性相关性较小时，朴素贝叶斯性能最为良好。对于这一点，有半朴素贝叶斯之类的算法通过考虑部分关联性适度改进。</p><p><strong>整个例子详细的讲解了朴素贝叶斯算法的分类过程，希望对大家的理解有帮助~</strong></p><p>参考：李航博士《统计学习方法》</p><p><a href="https://link.zhihu.com/?target=http%3A//www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html" target="_blank" rel="noopener">算法杂货铺–分类算法之朴素贝叶斯分类(Naive Bayesian classification)</a></p><p>封面图来自于：<a href="https://link.zhihu.com/?target=http%3A//www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html" target="_blank" rel="noopener">算法杂货铺–分类算法之朴素贝叶斯分类(Naive Bayesian classification)</a></p><p>致谢：德川，皓宇，继豪，施琦</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 朴素贝叶斯 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GAN异常检测</title>
      <link href="2019/11/07/GAN%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
      <url>2019/11/07/GAN%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>使用GAN做异常检测有两个思路</p><ol><li>【原始GAN】（generator + discriminator):在推断阶段，GAN的输入的是随机的噪声向量z，输出为discriminator的对G(z)的分数。</li><li>【变异GAN】(encoder + generator + discriminator)：在推断阶段，GAN的输入是样本x，输出为discriminator对重构样本x’(x’ = G(E(x)))的分数。</li></ol><p>那么，【变异GAN】相对【原始GAN】有哪些方面的改进？这种改进是为了解决了什么问题？</p><p>【原始GAN】通过训练，得到了一个 generator（使用z生成x） 和1个 discriminator（对x打分）。但是在inference阶段，我们拿到手的样本是想，而这个时候gan的generator用不了这种输入，还要做一层变换–即<strong>“找到x对应的z”</strong>，然后使用generator生成对应的x。但是“找到x对应的z”，就是去解另外一个优化问题了（梯度下降，迭代），计算量很大。所以【变异GAN】就在【原始GAN】的架构最前面加了一个encoder，目的是，在训练的时候把这个encoder一起训练出来了，后面就不用做一次推断再接一个优化问题（“<strong>找到x对应的z</strong>”）了。</p><h2 id="建模"><a href="#建模" class="headerlink" title="建模"></a>建模</h2><p>构建 三个网络结构, E(encoder /G的逆)， G(generator/decoder）， D(discriminator)。</p><p>原始的信号是正常样本， generator生成的信号是异常样本。</p><p>原始特征+encoder之后的隐层特征作为discriminator的输入特征.</p><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>三个网络联合训练，，然后把正常样本和异常样本一起丢进discriminator。</p><p>目标函数定义为：</p><p>$$\min _{G, E} \max _{D} V(D, E, G)$$</p><p>其中</p><p><img src="http://img.cdn.leonwang.top/20191107214752.png" alt=""></p><p>这个目标函数左边是正常样本 ，右边是异常样本。</p><p>D是discriminator预测x是正常的概率，log项表示预测值和真实值的交叉熵的负数。</p><h2 id="推断"><a href="#推断" class="headerlink" title="推断"></a>推断</h2><p>通过训练固定好GAN的参数之后，就可以来检测新来样本是否是异常了。</p><p>定义:计算异常得分的函数可以定义为：<br>$$<br>A(x)=\alpha L_{G}(x)+(1-\alpha) L_{D}(x)<br>$$<br>这里 $L_{G}$ 表示重构误差， $L_D$ 表示判别器误差（discriminator-based）误差。</p><p>背后的intution是：</p><ul><li>重构误差：利用生成模型，来判断x跟真实数据的差异。</li><li>判别器误差: 从判别器的角度来确定x跟真实样本的差异。</li></ul><p>—没想清楚这两种误差有什么联系？</p><p>具体来说</p><p>$$L_{G}(x)=|x-G(E(x))|_{1}$$</p><p>$L_{D}(x)=\sigma(D(x, E(x)), 1)$ , 表示预测概率与真实值的交叉熵，当x越异常时，D就会远离1，交叉熵就越大。</p><p>x是原始信号，G(z)是重构信号，然后分别把这两个信号通过f映射到另外一个空间，在另外一个空间衡量距离，如果x是正常的，应该跟G(z)的距离很小，映射到另外一个空间之后，距离也会保持很小。经过f之后 ，能把 正常样本 跟 异常样本分的更开，</p><p>参考资料</p><p>[1] <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1703.05921.pdf" target="_blank" rel="noopener">方法1</a></p><p>[2] <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1802.06222.pdf" target="_blank" rel="noopener">方法2</a></p><p>[3] <a href="https://link.zhihu.com/?target=https%3A//github.com/houssamzenati/Efficient-GAN-Anomaly-Detection" target="_blank" rel="noopener">houssamzenati/Efficient-GAN-Anomaly-Detection</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> Common </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> GAN </tag>
            
            <tag> 异常检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习分类</title>
      <link href="2019/11/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB/"/>
      <url>2019/11/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB/</url>
      
        <content type="html"><![CDATA[<p>内容：<strong>监督学习、无监督学习、强化学习、弱监督学习、半监督学习、多示例学习</strong></p><p><strong>监督学习</strong>(supervised learning)：已知数据和其一一对应的标签，训练一个智能算法，将输入数据映射到标签的过程。监督学习是最常见的学习问题之一，就是人们口中常说的分类问题。比如已知一些图片是猪，一些图片不是猪，那么训练一个算法，当一个新的图片输入算法的时候算法告诉我们这张图片是不是猪。</p><p><strong>无监督学习</strong>(unsupervised learning)：已知数据不知道任何标签，按照一定的偏好，训练一个智能算法，将所有的数据映射到多个不同标签的过程。相对于有监督学习，无监督学习是一类比较困难的问题，所谓的按照一定的偏好，是比如特征空间距离最近，等人们认为属于一类的事物应具有的一些特点。举个例子，猪和鸵鸟混杂在一起，算法会测量高度，发现动物们主要集中在两个高度，一类动物身高一米左右，另一类动物身高半米左右，那么算法按照就近原则，75厘米以上的就是高的那类也就是鸵鸟，矮的那类是第二类也就是猪，当然这里也会出现身材矮小的鸵鸟和身高爆表的猪会被错误的分类。</p><p><strong>强化学习</strong>(reinforcement learning)：智能算法在没有人为指导的情况下，通过不断的试错来提升任务性能的过程。“试错”的意思是还是有一个衡量标准，用棋类游戏举例，我们并不知道棋手下一步棋是对是错，不知道哪步棋是制胜的关键，但是我们知道结果是输还是赢，如果算法这样走最后的结果是胜利，那么算法就学习记忆，如果按照那样走最后输了，那么算法就学习以后不这样走。</p><p><strong>弱监督学习</strong>(weakly supervised learning)： 已知数据和其一一对应的弱标签，训练一个智能算法，将输入数据映射到一组更强的标签的过程。标签的强弱指的是标签蕴含的信息量的多少，比如相对于分割的标签来说，分类的标签就是弱标签，如果我们知道一幅图，告诉你图上有一只猪，然后需要你把猪在哪里，猪和背景的分界在哪里找出来，那么这就是一个已知若标签，去学习强标签的弱监督学习问题。</p><p><strong>半监督学习</strong>(semi supervised learning) ：已知数据和部分数据一一对应的标签，有一部分数据的标签未知，训练一个智能算法，学习已知标签和未知标签的数据，将输入数据映射到标签的过程。半监督通常是一个数据的标注非常困难，比如说医院的检查结果，医生也需要一段时间来判断健康与否，可能只有几组数据知道是健康还是非健康，其他的只有数据不知道是不是健康。那么通过有监督学习和无监督的结合的半监督学习就在这里发挥作用了。</p><p><strong>多示例学习</strong>(multiple instance learning) ：已知包含多个数据的数据包和数据包的标签，训练智能算法，将数据包映射到标签的过程，在有的问题中也同时给出包内每个数据的标签。多事例学习引入了数据包的概念，比如说一段视频由很多张图组成，假如1000张，那么我们要判断视频里是否有猪出现，一张一张的标注每一帧是否有猪太耗时，所以人们看一遍说这个视频里有猪或者没猪，那么就得到了多示例学习的数据，1000帧的数据不是每一个都有猪出现，只要有一帧有猪，那么我们就认为这个包是有猪的，所有的都没有猪，才是没有猪的，从这里面学习哪一段视频（1000张）有猪哪一段视频没有就是多事例学习的问题。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于感知机学习算法的对偶形式</title>
      <link href="2019/11/03/%E5%85%B3%E4%BA%8E%E6%84%9F%E7%9F%A5%E6%9C%BA%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9A%84%E5%AF%B9%E5%81%B6%E5%BD%A2%E5%BC%8F/"/>
      <url>2019/11/03/%E5%85%B3%E4%BA%8E%E6%84%9F%E7%9F%A5%E6%9C%BA%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9A%84%E5%AF%B9%E5%81%B6%E5%BD%A2%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<p>《统计学习方法》</p><p>感知机学习的学习算法是基于随机梯度下降法的对损失函数的最优化算法，由原始形式和对偶形式。</p><p>原始形式容易理解。通过损失函数对参数 w 和 b 的梯度进行参数更新，最终收敛得到感知机模型。</p><p>对偶形式在书中表述的不是特别简洁。可以借鉴知乎dalao的回答，更有利于理解。书中说需要更新 $\alpha$ 和 b 两个参数，实际上不需要这么复杂，可以进一步转化为只更新一个参数。</p><p>实际上，对偶形式是基于原始形式，进行了更进一步的推导，最终达到减少计算量的目的。可以这么理解，原本原始形式基于梯度的参数更新公式中，对于同一个样本点 $x_i$，每次梯度更新参数的增量是固定不变的：w每次的增量都是 $\eta y_{i} x_{i}$，b每次的增量都是 $\eta y_{i}$。</p><p><img src="http://img.cdn.leonwang.top/20191103170802.png" alt=""></p><p>所以实际上只需要统计对这个样本点一共需要进行多少次参数更新。用 $n_i$ 统计对于第 i 个样本点需要更新参数的次数，每次需要更新时，令更新次数 $n_i$ 加1，只需要在迭代过程中更新 $n_i$ 这一个参数。最后直接计算参数的总的增量=更新次数*每次的增量。</p><p>而对偶形式的最直接的好处体现在计算误分条件（符合这个条件的点就是误分类点）时。原始形式的误分条件是 $y_{i}\left(w \cdot x_{i}+b\right) \leqslant 0$，而对偶形式的误分条件是 $y_{i}\left(\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j} \cdot x_{i}+b\right) \leqslant 0$。这样对偶形式中对向量内积的计算就可以利用预先计算好 Gram 矩阵的方式来减少计算量。</p><p><img src="http://img.cdn.leonwang.top/20191103165437.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 统计学习方法 </tag>
            
            <tag> 感知机 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++ vs Java 之虚函数</title>
      <link href="2019/10/09/C-vs-Java-%E4%B9%8B%E8%99%9A%E5%87%BD%E6%95%B0/"/>
      <url>2019/10/09/C-vs-Java-%E4%B9%8B%E8%99%9A%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<p>C++代码：</p><pre><code class="c++">#include &lt;iostream&gt;class Base{public:    void print1() {printf(&quot;Hello from Base::print1!\n&quot;);}    virtual void print2() {printf(&quot;Hello from Base::print2!\n&quot;);}};class Derived: public Base{public:    void print1() {printf(&quot;Hello from Derived::print1!\n&quot;);}    virtual void print2() {printf(&quot;Hello from Derived::print2!\n&quot;);}};int main(){    Derived *pd = new Derived();    Base *pb = pd;    pd-&gt;print1();    pb-&gt;print1();    pd-&gt;print2();    pb-&gt;print2();    return 0;}</code></pre><pre><code class="c++">Hello from Derived::print1!Hello from Base::print1!Hello from Derived::print2!Hello from Derived::print2!</code></pre><p>virtual函数：动态多态</p><p>C++下，声明为virtual的函数，当父类指针指向子类对象时，调用的是子类实现；非virtual函数不行。</p><p>但在java下，默认的method都是virtual的。</p>]]></content>
      
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> Java </tag>
            
            <tag> 虚函数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>yolov3 Keras实现解读</title>
      <link href="2019/09/09/yolov3-Keras%E5%AE%9E%E7%8E%B0%E8%A7%A3%E8%AF%BB/"/>
      <url>2019/09/09/yolov3-Keras%E5%AE%9E%E7%8E%B0%E8%A7%A3%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<p><a href="https://danielack.github.io/2018/08/25/yolov3Keras实现解读/" target="_blank" rel="noopener">原文链接</a></p><p>原作者：<a href="https://danielack.github.io/" target="_blank" rel="noopener">Daniel_柏桦</a></p><blockquote><p>刚刚接触深度学习，以目标检测为入手，本文主要以yolov3的Keras实现为主线，穿插入yolov3的论文思想，也是记录自己的学习过程。</p></blockquote><h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><ol><li>首先感谢<strong>@qqwweee</strong>以及各位contributors完美的用Keras实现了yolov3，本文也是以此项目进行yolov3的源码解读学习，<strong>repo</strong>：<a href="https://github.com/qqwweee/keras-yolo3" target="_blank" rel="noopener">https://github.com/qqwweee/keras-yolo3</a></li><li>文章内容大部分也是借鉴了<strong>@SpikeKing</strong> 对此源码的解读，讲解的非常详细，本文只是在此之上加入了自己学习过程中的一些想法，大家有兴趣的可以直接阅读，<strong>repo</strong>：<a href="https://github.com/SpikeKing/keras-yolo3-detection" target="_blank" rel="noopener">https://github.com/SpikeKing/keras-yolo3-detection</a></li><li>由于源码比较复杂，作者也只是刚刚接触yolov3，能力有限，所有会有部分内容不能完整的照顾到，如果没有能帮助到您在此表示抱歉。</li></ol><p><strong>阅读建议</strong></p><p>由于整个项目包含了对yolov3的算法的全部实现，主要分为核心算法实现以及使用相关代码实现，所以整体内容较为庞大，建议大家先按照自己的侧重进行阅读。如果侧重 <strong>使用</strong> ：建议从使用章节开始阅读，如果侧重 <strong>算法学习</strong> ：建议从核心算法实现开始阅读。</p><hr><h1 id="项目结构"><a href="#项目结构" class="headerlink" title="项目结构"></a>项目结构</h1><p>我们可以从项目的包结构中对算法实现进行简单的探索：</p><p>├─font<br>├─model_data<br>├─yolo3<br>│ └─<strong>pycache</strong><br>├─yolo.py<br>├─train.py<br>├─yolo_video.py<br>└─<strong>pycache</strong></p><p><strong>font</strong> 目录下包含一些字体，核心实现还是在 <strong>model_data</strong> 和 <strong>yolo3</strong> 这两个文件夹</p><p><strong>model_data</strong> 文件夹中包含了coco数据集和voc数据集的相关说明文件，例如对数据集聚类后生成的anchors文件，数据集的类别说明文件，后期需要使用的 yolo 的权重文件也会放在这</p><p><strong>yolo3</strong> 文件夹中包含了算法实现的核心文件： <strong>model.py</strong> 和 <strong>util.py</strong>， <strong>model.py</strong> 主要实现算法框架，<strong>util.py</strong>主要封装一些实现需要的功能</p><p><strong>train.py</strong>： 使得可以使用自己的数据集进行训练，其中就用到了核心算法</p><p><strong>yolo.py</strong> 和 <strong>yolo_video.py</strong> : <strong>yolo.py</strong> 实现了主要的使用方面的功能，<strong>yolo_video.py</strong> 是整个项目的使用入口文件，调用了 <strong>yolo.py</strong> 中的相关函数</p><h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><h2 id="简单开始"><a href="#简单开始" class="headerlink" title="简单开始"></a>简单开始</h2><h3 id="下载权重文件"><a href="#下载权重文件" class="headerlink" title="下载权重文件"></a>下载权重文件</h3><p>首先模型需要训练好的权重参数才能够进行检测，可以直接下载官方已经训练好的权重参数（貌似是使用coco和voc两个数据集训练出的结果）到项目目录下</p><p>地址：<a href="https://pjreddie.com/media/files/yolov3.weights" target="_blank" rel="noopener">https://pjreddie.com/media/files/yolov3.weights</a></p><p>将 .weights 文件转换为 Keras支持的 h5 权重文件</p><p>在目录下执行：</p><pre><code>python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5</code></pre><h3 id="进行图片检测"><a href="#进行图片检测" class="headerlink" title="进行图片检测"></a>进行图片检测</h3><p>可以在 <a href="https://github.com/AlexeyAB/darknet/tree/master/data" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet/tree/master/data</a> 中下载你喜欢的测试图片到项目目录下，当然也可以使用自己的图片</p><p>执行：</p><pre><code>python yolo_video.py --image</code></pre><p>之后会要求输入需要检测的图片名称，输入图片名称之后就会出现检测结果：</p><p><img src="http://img.cdn.leonwang.top/20190909193831.png" alt=""></p><p><img src="http://img.cdn.leonwang.top/20190909193901.png" alt=""></p><p><img src="http://img.cdn.leonwang.top/20190909193911.png" alt=""></p><h3 id="利用摄像头进行视频检测"><a href="#利用摄像头进行视频检测" class="headerlink" title="利用摄像头进行视频检测"></a>利用摄像头进行视频检测</h3><p>进行视频检测首先需要 CUDA 9.0+, h5py 以及 OpenCV3.x 的支持，使用 Anaconda 就可以轻松安装</p><p>同样在项目目录下执行：</p><pre><code>python yolo_video.py</code></pre><p>就可以利用摄像头进行视频检测，也可以加上 <code>--input</code> 参数变为视频检测，参数内容就是要检测的视频文件目录</p><h3 id="源码解读"><a href="#源码解读" class="headerlink" title="源码解读"></a>源码解读</h3><h4 id="yolo-video-py"><a href="#yolo-video-py" class="headerlink" title="yolo_video.py"></a>yolo_video.py</h4><p>命令行参数：</p><p><code>--image</code> 进入图片检测模式</p><p><code>--model</code> 指定权重文件的位置，默认是 model_data/yolo.h5</p><p><code>--anchors</code> 指定anchors文件位置，默认是 model_data/yolo_anchors.txt</p><p><code>--classes_path</code> 指定类别文件位置, 默认是 model_data/coco_classes.txt</p><h4 id="yolo-py"><a href="#yolo-py" class="headerlink" title="yolo.py"></a>yolo.py</h4><p>主要实现对图片中物体的检测以及得分和框的绘制</p><p><strong>generate(self)</strong></p><ul><li>加载权重参数文件，生成检测框，得分，以及对应类别</li><li>利用 <strong>model.py</strong> 中的 <strong>yolo_eval</strong> 函数生成检测框，得分，所属类别</li><li>初始化时调用generate函数生成图片的检测框，得分，所属类别（self.boxes, self.scores, self.classes）</li></ul><p><strong>detect_image(self, image)</strong></p><ul><li>要求进行检测的图片尺寸大小是</li></ul><pre><code>32的倍数，原因:- 在网络中，执行的是 **5次步长为2** 的卷积操作，即</code></pre><ul><li>图片的默认尺寸是416 x 416,因为在最底层中的特征图（Feature Map）大小是13，所以 13 * 32 = 416</li></ul><p><strong>注意：</strong><br>特征图和卷积核不是一个概念，特征图是指图片在进行卷积等一系列操作之后最终得到的图片，一开始喂给网络的图片也可以称作是特征图，特征图之后也会作为下一层网络的输入<br>，<br>5次步长为2的卷积操作源码：</p><p>位置：keras-yolo3-master\keras-yolo3-master\yolo3\model.py</p><pre><code>x = compose(           DarknetConv2D_BN_Leaky(num_filters, (1,1)),           DarknetConv2D_BN_Leaky(num_filters*2, (3,3)),           DarknetConv2D_BN_Leaky(num_filters, (1,1)),           DarknetConv2D_BN_Leaky(num_filters*2, (3,3)),           DarknetConv2D_BN_Leaky(num_filters, (1,1)))(x)</code></pre><p><strong>detect_video</strong></p><ul><li>实现视频检测和通过摄像头实时检测的功能，利用到了OpenCV</li></ul><h2 id="使用自己的训练集"><a href="#使用自己的训练集" class="headerlink" title="使用自己的训练集"></a>使用自己的训练集</h2><p>通过使用提供的 <strong>train.py</strong> 可以使用自己的数据集进行训练得到权重文件，从而使用自己的数据集进行检测工作</p><p>为了更方便的训练，我已经把相关的训练脚本代码放到了github上面，有兴趣的同学可以用来跑跑自己的训练集试一试，当然也可以使用原作者的 <strong>train.py</strong> 来训练,如果使用 <strong>train.py</strong> 可以参考 <strong>train.py 解读</strong></p><p>go right ahead ~</p><h3 id="train-py-解读"><a href="#train-py-解读" class="headerlink" title="train.py 解读"></a>train.py 解读</h3><p>训练需要指定</p><ul><li>训练数据集的标注文本</li><li>权重文件、训练日志输出路径：log_dir</li><li>类别说明文件</li><li>聚类好的anchors文件</li></ul><p>创建模型的时候，默认会预加载 model_data 文件夹下的权重文件</p><p>默认将输入的训练图片转变为 input_shape(默认：416 x 416) 的格式<br>具体转变的过程我看的也不是很明白，但输出一定是 input_shape 的格式，代码如下：</p><pre><code># 文件路径：./yolo3/utils.py get_random_data函数中# resize image   new_ar = w/h * rand(1-jitter,1+jitter)/rand(1-jitter,1+jitter)   scale = rand(.25, 2)   if new_ar &lt; 1:       nh = int(scale*h)       nw = int(nh*new_ar)   else:       nw = int(scale*w)       nh = int(nw/new_ar)   image = image.resize((nw,nh), Image.BICUBIC)   # place image   dx = int(rand(0, w-nw))   dy = int(rand(0, h-nh))   new_image = Image.new(&#39;RGB&#39;, (w,h), (128,128,128))   new_image.paste(image, (dx, dy))   image = new_image   # flip image or not   flip = rand()&lt;.5   if flip: image = image.transpose(Image.FLIP_LEFT_RIGHT)   # distort image   hue = rand(-hue, hue)   sat = rand(1, sat) if rand()&lt;.5 else 1/rand(1, sat)   val = rand(1, val) if rand()&lt;.5 else 1/rand(1, val)   x = rgb_to_hsv(np.array(image)/255.)   x[..., 0] += hue   x[..., 0][x[..., 0]&gt;1] -= 1   x[..., 0][x[..., 0]&lt;0] += 1   x[..., 1] *= sat   x[..., 2] *= val   x[x&gt;1] = 1   x[x&lt;0] = 0   image_data = hsv_to_rgb(x) # numpy array, 0 to 1</code></pre><p>然后将训练的数据进行打乱(shuffle),交叉验证的默认比例是 10%</p><p>第一个阶段只训练最后的3个输出层</p><p>第二个阶段使用第一阶段的权重参数，继续训练所有的网络，进行权重的微调</p><ul><li>将学习率从 1e-3 减少为 1e-4</li></ul><p>==================================</p><h1 id="核心算法实现"><a href="#核心算法实现" class="headerlink" title="核心算法实现"></a>核心算法实现</h1><h2 id="model-py-解读"><a href="#model-py-解读" class="headerlink" title="model.py 解读"></a>model.py 解读</h2><p>model.py 是整个项目的核心，也是对yolo论文的复现，在 model.py 中定义了底层框架，实现了darknet框架（darknet_body），最后实现最终的yolo框架（yolo_body）</p><h3 id="底层框架"><a href="#底层框架" class="headerlink" title="底层框架"></a>底层框架</h3><p><strong>DarknetConv2D(*args, \</strong>kwargs)**</p><p>yolo是使用卷积神经网络进行训练，DarknetConv2D用来设置Darknet网络的参数，卷积神经网络用的是Keras的Conv2D</p><ul><li>kernel_regularizer：使用l2正则化 = l2(5e-4)</li><li>如果指定步长strides = (2,2) padding使用valid模式，否则使用same模式</li></ul><p><strong>DarknetConv2D_BN_Leaky(*args, \</strong>kwargs)**</p><p>默认不适用bias：’use_bias’: False<br>然后使用 util包中的 <strong>compose函数</strong> 在卷积之后进行BatchNormalization和LeakyReLU，将这些步骤封装成一个函数</p><p><strong>resblock_body(x, num_filters, num_blocks)</strong></p><p>在yolo中还用到了残差网络，加入了residual blocks的思想，在yolo3的darknet网络架构图中，被一个框框起来的两个卷积网络+一个残差训练就是一个<strong>resblock_body</strong>，利用 <strong>resblock_body</strong>，可以搭建出darknet框架</p><p>源码如下：</p><pre><code>&#39;&#39;&#39;A series of resblocks starting with a downsampling Convolution2D&#39;&#39;&#39;    # Darknet uses left and top padding instead of &#39;same&#39; mode    x = ZeroPadding2D(((1,0),(1,0)))(x)    # 进行步长为 2 的卷积操作    x = DarknetConv2D_BN_Leaky(num_filters, (3,3), strides=(2,2))(x)    for i in range(num_blocks):        y = compose(                DarknetConv2D_BN_Leaky(num_filters//2, (1,1)),                DarknetConv2D_BN_Leaky(num_filters, (3,3)))(x)        x = Add()([x,y])    return x</code></pre><h3 id="darknet框架"><a href="#darknet框架" class="headerlink" title="darknet框架"></a>darknet框架</h3><p>使用了<strong>resblock_body</strong>按照如图所示的框架搭建<br><img src="http://img.cdn.leonwang.top/20190909194027.png" alt=""></p><p>源码如下：</p><pre><code>&#39;&#39;&#39;Darknent body having 52 Convolution2D layers&#39;&#39;&#39;    x = DarknetConv2D_BN_Leaky(32, (3,3))(x)    x = resblock_body(x, 64, 1)    x = resblock_body(x, 128, 2)    x = resblock_body(x, 256, 8)    x = resblock_body(x, 512, 8)    x = resblock_body(x, 1024, 4)    return x</code></pre><p>由于每一次 <strong>resblock_body</strong> 中都含有一次步长为2的卷积，一共执行了5次，所以一共降维了 倍，因此最后的特征图大小就是 416 / 32 = 13</p><p>最后输出的网络中的 num_filter = 1024，输出结构是（sample_num, 13, 13, 1024）</p><h3 id="yolo框架"><a href="#yolo框架" class="headerlink" title="yolo框架"></a>yolo框架</h3><p><strong>make_last_layers(x, num_filters, out_filters)</strong></p><p>在 <strong>yolo_body</strong> 中使用到了这个函数，主要用来进行输出的降维操作，共执行2步操作：</p><ul><li>对 x 进行多次卷积操作，先将num_filters扩大一倍，再恢复原来的大小，最终输出的num_filters的到校仍是参数中的num_filters的大小</li><li>对 y 先进行 3 x 3 的卷积，再执行不含BatchNormalization和LeakyReLU的 <strong>1 x 1</strong> 卷积, 将输出的 num_filters 的大小降维out_filters</li></ul><h4 id="补充-1-x-1-卷积"><a href="#补充-1-x-1-卷积" class="headerlink" title="补充 1 x 1 卷积"></a>补充 1 x 1 卷积</h4><ul><li>1x1的卷积层可以灵活控制网络的depth也就是深度或者厚度，是对相同channel上的信息上的线性组合，从而达到降维\升维的目的，同时卷积之后特征图的大小没有变即在保持平面信息不变的情况下调整维度，所以 1 x 1 的卷积主要是针对网络的depth</li><li>1x1的卷积层在inception结构中还有降低计算量的作用<ul><li>先了解多核卷积的实例计算：<a href="https://www.cnblogs.com/ranjiewen/articles/7467600.html（注意看黑体加粗的重点部分），学会如何计算多核卷几下的神经元个数" target="_blank" rel="noopener">https://www.cnblogs.com/ranjiewen/articles/7467600.html（注意看黑体加粗的重点部分），学会如何计算多核卷几下的神经元个数</a></li><li>在此基础之上就可以理解下面这幅图了：</li><li><img src="http://img.cdn.leonwang.top/20190909194046.png" alt=""></li><li>左边是传统的inception模块，右边是加入1x1 卷积的inception。这层的输入的特征维数是（28x28x192）,卷积核大小以及卷积通道数(包括三种卷积核，分别是1x1x64,3x3x128,5x5x32)，右图中在3x3，5x5 convolution前新加入的1x1的卷积核为 96 和 16 通道的，在max pooling后加入的1x1卷积为 32 通道。</li><li>那么图(a)的参数为（1x1x192x64）+(3x3x192x128)+(5x5x192x32)</li><li>图(b)的参数为(1x1x192x64)+(1x1x192x96)+(1x1x192x16)+(3x3x96x128)+(5x5x16x32)+(1x1x192x32).<br>  比较可知，模型参数减少了。</li><li>参考：<a href="https://blog.csdn.net/a1154761720/article/details/53411365" target="_blank" rel="noopener">https://blog.csdn.net/a1154761720/article/details/53411365</a></li></ul></li></ul><p><strong>yolo_body(inputs, num_anchors, num_classes)</strong></p><p>参数：</p><ul><li>inputs： 输入张量</li><li>num_anchors: 锚的数量</li><li>num_classes: 类别的数量</li></ul><p>输出： 一个以输入张量为输入，三个张量组合[y1,y2,y3]为输出的模型</p><h4 id="13-x-13-特征图"><a href="#13-x-13-特征图" class="headerlink" title="13 x 13 特征图"></a>13 x 13 特征图</h4><p>先使用了darknet框架生成基本模型，将输出传给 <strong>make_last_layers</strong> ，将 x 的 num_filters 变为 512</p><p>输出的 x 的大小是 (sample_num, 13, 13, 512)</p><p>输出的 y1 的大小是(sample_num, 13, 13, num_anchors*(5 + num_classes))</p><h4 id="26-x-26-特征图"><a href="#26-x-26-特征图" class="headerlink" title="26 x 26 特征图"></a>26 x 26 特征图</h4><p>接着将输出为(sample_num, 13, 13, 512) 的 x 进行上采样2倍(UpSampling2D(2))，并且num_filters 变为256，x 的大小变为 (sample_num, 26, 26, 256)</p><p>然后和 darknet 的第152层网络(sample_num, 26, 26, 512)进行连接,<br>生成大小为 (sample_num, 13, 13, 768)</p><p>最后使用 <strong>make_last_layers</strong> 将 num_filters 改为256，并输出y2</p><p>输出的 x 的大小是 (sample_num, 26, 26, 256)</p><p>输出的 y2 的大小是(sample_num, 26, 26, num_anchors*(5 + num_classes))</p><h4 id="52-x-52-特征图"><a href="#52-x-52-特征图" class="headerlink" title="52 x 52 特征图"></a>52 x 52 特征图</h4><p>同样将输出为(sample_num, 26, 26, 256) 的 x 进行2倍的上采样，num_filters 变为 128， x 的大小变为(sample_num, 52, 52, 128)</p><p>然后和 darknet 的第92层网络(sample_num, 52, 52, 256)进行连接,<br>生成大小为 (sample_num, 13, 13, 384)</p><p>使用 <strong>make_last_layers</strong> 将 num_filters 改为128，并输出y3</p><p>输出的 x 的大小是 (sample_num, 52, 52, 128)</p><p>输出的 y3 的大小是(sample_num, 52, 52, num_anchors*(5 + num_classes))</p><p><strong>最终，yolo框架输出的是三个大小不同的特征图</strong></p><p>y1: (sample_num, 26, 26, num_anchors<em>(5 + num_classes))y2: (sample_num, 26, 26, num_anchors</em>(5 + num_classes))<br>y3: (sample_num, 52, 52, num_anchors*(5 + num_classes))</p><p>源码：</p><pre><code>&quot;&quot;&quot;Create YOLO_V3 model CNN body in Keras.&quot;&quot;&quot;    darknet = Model(inputs, darknet_body(inputs))# 13 x 13 特征图x, y1 = make_last_layers(darknet.output, 512, num_anchors*(num_classes+5))# 26 x 26 特征图    x = compose(            DarknetConv2D_BN_Leaky(256, (1,1)),            UpSampling2D(2))(x)    x = Concatenate()([x,darknet.layers[152].output])    x, y2 = make_last_layers(x, 256, num_anchors*(num_classes+5))    # 52 x 52 特征图    x = compose(            DarknetConv2D_BN_Leaky(128, (1,1)),            UpSampling2D(2))(x)    x = Concatenate()([x,darknet.layers[92].output])    x, y3 = make_last_layers(x, 128, num_anchors*(num_classes+5))    return Model(inputs, [y1,y2,y3])</code></pre><h3 id="yolo-boxes-and-scores"><a href="#yolo-boxes-and-scores" class="headerlink" title="yolo_boxes_and_scores()"></a>yolo_boxes_and_scores()</h3><p>提取框_boxes和置信度_box_scores</p><h3 id="yolo-eval"><a href="#yolo-eval" class="headerlink" title="yolo_eval()"></a>yolo_eval()</h3><p>anchor_mask : anchor的掩码，由于anchor文件中是按从小到大排列的，而model.output输出的层是 13-&gt;52 ，而越小的特征图检测的是越大的物体，也就需要越大的anchor，所以anchor_mask 是倒叙排列</p><p>使用yolo_boxes_and_scores获得提取框_boxes和置信度_box_scores</p><p>然后使用非极大抑制算法筛选标注框</p><p>再使用 K.gather 通过nms_index筛选合格的框和框对应得分</p><pre><code>boxes_ = K.concatenate(boxes_, axis=0)scores_ = K.concatenate(scores_, axis=0)classes_ = K.concatenate(classes_, axis=0)</code></pre><p>// tf.gather 使用实例： <a href="https://blog.csdn.net/guotong1988/article/details/53172882" target="_blank" rel="noopener">https://blog.csdn.net/guotong1988/article/details/53172882</a></p><p>使用 K.ones_like x c 生成类别信息， 例：[1,1,1] x 4 = [4,4,4] 代表3个类别为4的框</p><p>最后使用 K.concatenate 合并所有的框，合并所有的得分，合并所有的的类别信息</p><h3 id="补充涉及的小细节"><a href="#补充涉及的小细节" class="headerlink" title="补充涉及的小细节"></a>补充涉及的小细节</h3><p>1.<strong>非极大抑制算法</strong></p><p>在 <strong>yolo_eval()</strong> 中用到了 <strong>非极大抑制算法</strong></p><p><strong>非极大抑制算法（NMS）</strong>：用来剔除重合度高于阈值的框，有时候可能会有多个框检测的是同一个物体</p><p>首先从所有的检测框中找到置信度较大的那个框，然后挨个计算其与剩余框的IOU，如果其值大于一定阈值（重合度过高），那么就将该框剔除；然后对剩余的检测框重复上述过程，直到处理完所有的检测框</p><pre><code># 使用方法tf.image.non_max_suppression(    boxes, # A 2-D float Tensor of shape [num_boxes, 4]    scores, # A 1-D float Tensor of shape [num_boxes], 代表每个框的得分    max_output_size, # 最大被选择的框的数目    iou_threshold=0.5, # 检测的IOU阈值    name=None)</code></pre><p>2.<strong>concatenate</strong></p><p>concatenate 的使用：concatenate将相同维度的数据元素连接到一起<br>例：</p><pre><code>from keras import backend as Ksess = K.get_session()a = K.constant([[2, 4], [1, 2]])b = K.constant([[3, 2], [5, 6]])c = [a, b]c = K.concatenate(c, axis=0)print(sess.run(c))&quot;&quot;&quot;[[2. 4.] [1. 2.] [3. 2.] [5. 6.]]&quot;&quot;&quot;</code></pre><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>等待更新….</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Stanford ML Note</title>
      <link href="2019/07/24/Stanford-ML-Note/"/>
      <url>2019/07/24/Stanford-ML-Note/</url>
      
        <content type="html"><![CDATA[<blockquote><p>讲师：Andrew Ng</p></blockquote><table><thead><tr><th>Date</th><th>Content</th></tr></thead><tbody><tr><td>2019-3-16</td><td>[L1 : L4]</td></tr><tr><td>2019-3-20</td><td>[L6 : L12]</td></tr><tr><td>2019-4-7</td><td>[L14 : L27]</td></tr><tr><td>2019-6-6</td><td>[L28 : L35]</td></tr><tr><td>2019-7-24</td><td>[L46 : L51]</td></tr></tbody></table><h1 id="第1章-绪论：初识机器学习"><a href="#第1章-绪论：初识机器学习" class="headerlink" title="第1章 绪论：初识机器学习"></a>第1章 绪论：初识机器学习</h1><h2 id="L2-什么是机器学习"><a href="#L2-什么是机器学习" class="headerlink" title="L2 什么是机器学习"></a>L2 什么是机器学习</h2><p>任务T 经验E 性能度量P</p><h2 id="L3-监督学习"><a href="#L3-监督学习" class="headerlink" title="L3 监督学习"></a>L3 监督学习</h2><p>Supervised Learning: “right answers” given</p><ul><li><p>回归(regression)：预测值是连续的</p></li><li><p>分类(classification)：预测值是离散的</p><ul><li><p>一个特征：</p><p>  可以把样本点投射到特征轴上</p><p>  <img src="http://img.cdn.leonwang.top/006tKfTcgy1g16s7db4zwj31uh0u0dk0.jpg" alt=""></p></li><li><p>两个特征：</p><p>  用线将样本点分割成不同的区域</p><p>  <img src="http://img.cdn.leonwang.top/006tKfTcgy1g16s8xvhhgj314b0u0qcy.jpg" alt=""></p></li><li><p>更多特征：</p><p>  ……    </p></li></ul></li></ul><h2 id="L4-无监督学习"><a href="#L4-无监督学习" class="headerlink" title="L4 无监督学习"></a>L4 无监督学习</h2><p>Unsupervised Learning</p><p>聚类算法(clustering algorithms)</p><p>例子：给新闻自动划分主题、根据基因表达程度把人划分为不同群体、判断哪些计算机协同工作而提高数据中心的效率、社交软件分析自动划分社交圈子、根据客户数据划分客户圈、天文分析星系形成理论、处理分析音频</p><h1 id="第2章-单变量线性回归"><a href="#第2章-单变量线性回归" class="headerlink" title="第2章 单变量线性回归"></a>第2章 单变量线性回归</h1><h2 id="L6-模型描述"><a href="#L6-模型描述" class="headerlink" title="L6 模型描述"></a>L6 模型描述</h2><p>(x^(i), y^(i))表示第i个训练样本</p><p>训练出的模型是一个函数，称为hypothesis函数</p><p>简单的例子：单变量线性回归</p><p><img src="http://img.cdn.leonwang.top/006tKfTcgy1g191alaixrj30rk0cw0t9.jpg" alt=""></p><h2 id="L7-代价函数"><a href="#L7-代价函数" class="headerlink" title="L7 代价函数"></a>L7 代价函数</h2><p>“#”是训练样本个数的缩写</p><p>回归问题常用的代价函数：平方误差代价函数</p><p>分母中的2是为了便于求导</p><p><img src="http://img.cdn.leonwang.top/006tKfTcgy1g191nvyu1xj30mu0fodg7.jpg" alt=""></p><h2 id="L8-代价函数（一）"><a href="#L8-代价函数（一）" class="headerlink" title="L8 代价函数（一）"></a>L8 代价函数（一）</h2><p>简化上节课的hypothesis（使其只有一个一次项系数参数），然后借助图像，理解hypothesis与代价函数的关系：</p><p><img src="http://img.cdn.leonwang.top/006tKfTcgy1g191yug4nuj30tu0jc409.jpg" alt=""></p><h2 id="L9-代价函数（二）"><a href="#L9-代价函数（二）" class="headerlink" title="L9 代价函数（二）"></a>L9 代价函数（二）</h2><p>取消上节课中的简化操作，讨论两个参数的hypothesis。其代价函数图像不再是二维的曲线，而是一个三维的曲面，但可以用二维的等高线图来表示。</p><h2 id="L10-梯度下降"><a href="#L10-梯度下降" class="headerlink" title="L10 梯度下降"></a>L10 梯度下降</h2><p>梯度下降算法：求最小化代价函数，不只可以用在线性回归问题</p><p>得到的是局部最优</p><p>过程：</p><p><img src="http://img.cdn.leonwang.top/006tKfTcgy1g1955i32adj30ry088dg8.jpg" alt=""></p><p>定义：</p><p><img src="http://img.cdn.leonwang.top/006tKfTcgy1g1955zzel9j30n607qt8x.jpg" alt=""></p><p>:= 赋值</p><p>alpha 学习率（梯度下降时迈出多大的步子）</p><p>theta0 和 theta1 是<strong>同时更新</strong>的</p><p><img src="http://img.cdn.leonwang.top/006tKfTcgy1g1952enn4fj30sw082wf4.jpg" alt=""></p><p>右边的方法是错误的，因为在计算 theta1 的过程中使用了新的 theta0</p><h2 id="L11-梯度下降知识点总结"><a href="#L11-梯度下降知识点总结" class="headerlink" title="L11 梯度下降知识点总结"></a>L11 梯度下降知识点总结</h2><p>以单变量函数的梯度下降法为例，理解梯度下降法中导数部分和学习率alpha的意义。</p><h2 id="L12-线性回归的梯度下降"><a href="#L12-线性回归的梯度下降" class="headerlink" title="L12 线性回归的梯度下降"></a>L12 线性回归的梯度下降</h2><p>平方代价函数 + 梯度下降法</p><p>Batch梯度下降算法：每一步梯度下降都遍历了<strong>整个</strong>训练集的样本。</p><h1 id="第3章-线性代数回顾"><a href="#第3章-线性代数回顾" class="headerlink" title="第3章 线性代数回顾"></a>第3章 线性代数回顾</h1><h2 id="L14-矩阵和向量"><a href="#L14-矩阵和向量" class="headerlink" title="L14 矩阵和向量"></a>L14 矩阵和向量</h2><p>维数：</p><p>​    矩阵 m行n列 （二维）</p><p>​    向量 n行1列 （一维）</p><p>没有特殊说明的情况下，默认向量的下标从1开始，而不是从0。</p><h2 id="L15-加法和标量乘法"><a href="#L15-加法和标量乘法" class="headerlink" title="L15 加法和标量乘法"></a>L15 加法和标量乘法</h2><h2 id="L16-矩阵向量乘法"><a href="#L16-矩阵向量乘法" class="headerlink" title="L16 矩阵向量乘法"></a>L16 矩阵向量乘法</h2><p>形式：</p><p><img src="http://img.cdn.leonwang.top/006tNc79gy1g1thtzm8tkj30qc0fcq6c.jpg" alt=""></p><p>实际应用：</p><p><img src="http://img.cdn.leonwang.top/006tNc79gy1g1thtdplhrj30wa0hsn77.jpg" alt=""></p><p>代码用矩阵向量乘法表示而不是用for循环的优点：</p><ol><li>代码简洁</li><li>计算效率更高</li></ol><h2 id="L17-矩阵乘法"><a href="#L17-矩阵乘法" class="headerlink" title="L17 矩阵乘法"></a>L17 矩阵乘法</h2><p>用于多个hypotheses函数的情形：</p><p><img src="http://img.cdn.leonwang.top/006tNc79gy1g1thwi0rmnj30sg0ean19.jpg" alt=""></p><p>应用：高效地进行多个假设的计算</p><h2 id="L18-矩阵乘法特征"><a href="#L18-矩阵乘法特征" class="headerlink" title="L18 矩阵乘法特征"></a>L18 矩阵乘法特征</h2><p>矩阵乘法</p><ul><li>不服从交换律</li><li>服从结合律</li><li>单位矩阵的概念</li></ul><h2 id="L19-逆和转置"><a href="#L19-逆和转置" class="headerlink" title="L19 逆和转置"></a>L19 逆和转置</h2><p><img src="http://img.cdn.leonwang.top/006tNc79gy1g1ti44z0j9j30j803uwgk.jpg" alt=""></p><p>奇异矩阵/退化矩阵：没有逆矩阵的矩阵，比如全零矩阵</p><h1 id="第5章-多变量线性回归"><a href="#第5章-多变量线性回归" class="headerlink" title="第5章 多变量线性回归"></a>第5章 多变量线性回归</h1><h2 id="L28-多功能"><a href="#L28-多功能" class="headerlink" title="L28 多功能"></a>L28 多功能</h2><p>例：多变量预测房屋价格</p><p><img src="http://img.cdn.leonwang.top/006tNc79gy1g3qn70elnqj30p40cq7a0.jpg" alt=""></p><p>假设形式：</p><p><img src="http://img.cdn.leonwang.top/006tNc79gy1g3qn9fvsp2j30nk0bm0z1.jpg" alt=""></p><p>（惯例：x_0 = 1）</p><h2 id="L29-多元梯度下降法"><a href="#L29-多元梯度下降法" class="headerlink" title="L29 多元梯度下降法"></a>L29 多元梯度下降法</h2><p>多元梯度下降法的更新规则：单变量vs多变量</p><p><img src="http://img.cdn.leonwang.top/006tNc79gy1g3qneduvi7j30oy0dmai4.jpg" alt=""></p><h2 id="L30-多元梯度下降法演练1——特征缩放"><a href="#L30-多元梯度下降法演练1——特征缩放" class="headerlink" title="L30 多元梯度下降法演练1——特征缩放"></a>L30 多元梯度下降法演练1——特征缩放</h2><p><strong>特征缩放：</strong></p><p><img src="http://img.cdn.leonwang.top/image-20190606010011784.png" alt=""></p><p>如果不同特征的取值范围相差较大，则代价函数图像可能不理想（例如上图太细长），导致梯度下降时不断振荡才能收敛到全局最优。</p><p>通过特征缩放，可以让梯度下降找到一条更直接的路径。</p><p>不要求特征的范围完全相同，但应该接近。</p><p><strong>均值归一化：</strong></p><p><img src="http://img.cdn.leonwang.top/image-20190606010355412.png" alt=""></p><p>通过减去一个常数，使特征的均值在0附近。</p><p>通常的做法： x减去均值，再除以范围的长度。</p><p>目的：特征缩放不需要太精确，只是为了让梯度下降更快速。</p><h2 id="L31-多元梯度下降法2——学习率"><a href="#L31-多元梯度下降法2——学习率" class="headerlink" title="L31 多元梯度下降法2——学习率"></a>L31 多元梯度下降法2——学习率</h2><p>通过观察”代价函数-迭代步数”图来判断和调整学习率</p><p>理想学习率的情况下：</p><p><img src="http://img.cdn.leonwang.top/006tNc79gy1g3qnrbbzcqj30j20d8adt.jpg" alt=""></p><p>学习率过大的情况：</p><p><img src="http://img.cdn.leonwang.top/006tNc79gy1g3qnrtw4e2j30cq09cdhj.jpg" alt=""></p><p>对于上面的两种情况，通常调小学习率可以解决。</p><p>但是学习率不能太小，否则收敛很慢。</p><p>选择学习率的策略：</p><p><img src="http://img.cdn.leonwang.top/006tNc79gy1g3qnu4qgjpj30m205itag.jpg" alt=""></p><p>每隔3倍取一个，找到过大的和过小的学习率。通常去尝试比 过大的学习率 稍小一点的数作为学习率。</p><h2 id="L32-特征和多项式回归"><a href="#L32-特征和多项式回归" class="headerlink" title="L32 特征和多项式回归"></a>L32 特征和多项式回归</h2><p>不只用直线进行拟合，可以用多项式函数。</p><p>假设函数是多项式时，特征缩放尤其重要。</p><h2 id="L33-正规方程（区别于迭代方法的直接解法）"><a href="#L33-正规方程（区别于迭代方法的直接解法）" class="headerlink" title="L33 正规方程（区别于迭代方法的直接解法）"></a>L33 正规方程（区别于迭代方法的直接解法）</h2><p><img src="http://img.cdn.leonwang.top/image-20190606011858017.png" alt=""></p><p>特征方程法给出了直接求解令代价函数最小化的参数向量的计算方法。</p><p>正规方程法不需要做特征缩放。</p><p><strong>梯度下降法和正规方程法的比较：</strong></p><p><img src="http://img.cdn.leonwang.top/image-20190606012301141.png" alt=""></p><p>当n的规模较大时，梯度下降法的表现比正规方程法好。</p><h2 id="L34-正规方程在矩阵X‘X不可逆情况下的解决方法"><a href="#L34-正规方程在矩阵X‘X不可逆情况下的解决方法" class="headerlink" title="L34 正规方程在矩阵X‘X不可逆情况下的解决方法"></a>L34 正规方程在矩阵X‘X不可逆情况下的解决方法</h2><p>没有逆的矩阵：奇异或退化矩阵</p><p>可以使用编程包中计算 <strong>伪逆</strong> 的方法来计算。</p><p>机器学习中矩阵X’X不可逆的<strong>原因</strong>通常考虑：</p><ul><li>包含了多余的特征（比如米和英尺存在定值转换的关系）</li><li>特征太多（比如样本数小于特征数） -&gt; 尝试 删掉部分特征 或 正则化</li></ul><h1 id="第7章-Logistic回归"><a href="#第7章-Logistic回归" class="headerlink" title="第7章 Logistic回归"></a>第7章 Logistic回归</h1><h2 id="L46-分类"><a href="#L46-分类" class="headerlink" title="L46 分类"></a>L46 分类</h2><p>分类问题：y 是离散值</p><p>把线性回归应用于分类问题通常不是个好主意。</p><p>Logistic函数：值在0-1之间</p><p>Logistic回归被视为一种分类算法（虽然名字里带回归，但实际上是分类算法）</p><h2 id="L47-假设函数"><a href="#L47-假设函数" class="headerlink" title="L47 假设函数"></a>L47 假设函数</h2><p>假设函数：<br>$$<br>\mathbf{h}_{\theta}(x)=g\left(\mathbf{\theta}^{T} \mathbf{x}\right)<br>$$<br>其中，g 是 Sigmoid 函数（或称 Logistic 函数，二者几乎是等价的）：<br>$$<br>g(z)=\frac{1}{1+e^{-z}}<br>$$<br><img src="http://img.cdn.leonwang.top/20190724150741.png" alt=""></p><p>对 $\mathbf{h}_{\theta}(x)$ 的解释：表示y=1的概率，即 $\mathbf{h}_{\theta}(x)=P(y=1 | x ; \theta)=1-P(y=0 | x ; \theta)$   （y的值只能是0或1）</p><h2 id="L48-决策界限"><a href="#L48-决策界限" class="headerlink" title="L48 决策界限"></a>L48 决策界限</h2><p>决策边界（decision boundary）</p><p>可以帮助我们理解上面的假设函数是如何做出预测的</p><p>即指定判定输出y=0还是y=1的阈值（在假设函数中对应的线/面/…）</p><p>决策边界是假设函数的属性，而不是数据集的属性</p><h2 id="L49-代价函数"><a href="#L49-代价函数" class="headerlink" title="L49 代价函数"></a>L49 代价函数</h2><p>如果使用线性回归中的代价函数计算方式，由于$ \mathbf{h}_{\theta}(x)$ 是非线性的，所以最终计算的代价函数 $ J(\theta)$ 是非凸函数，导致无法很好地应用梯度下降法来寻找全局最优解</p><blockquote><p>非凸函数有很多的局部最优值，例如<img src="http://img.cdn.leonwang.top/20190724153422.png" alt=""></p></blockquote><p>因此不采用上述代价函数，而采用：<br>$$<br>\begin{array}{ll}{J(\theta)=\frac{1}{m} \sum_{i=1}^{m} \operatorname{cost}\left(h_{\theta}\left(x^{(i)}\right), y^{(i)}\right)} &amp; {} \ {\operatorname{cost}\left(h_{\theta}(x), y\right)=-\log \left(h_{\theta}(x)\right)} &amp; {\text { if } y=1} \ {\operatorname{cost}\left(h_{\theta}(x), y\right)=-\log \left(1-h_{\theta}(x)\right)} &amp; {\text { if } y=0}\end{array}<br>$$<br>这样的 $J(\theta)$ 是凸函数且没有局部最优</p><p><img src="http://img.cdn.leonwang.top/20190724155356.png" alt=""></p><p><img src="http://img.cdn.leonwang.top/20190724155414.png" alt=""></p><p>性质：<br>$$<br>\begin{array}{l}{\operatorname{cost}\left(h_{\theta}(x), y\right)=0 \text { if } h_{\theta}(x)=y} \ {\operatorname{cost}(h \theta(x), y) \rightarrow \infty \text { if } y=0 \text { and } h_{\theta}(x) \rightarrow 1} \ {\operatorname{cost}\left(h_{\theta}(x), y\right) \rightarrow \infty \text { if } y=1 \text { and } h_{\theta}(x) \rightarrow 0}\end{array}<br>$$</p><h2 id="L50-简化代价函数与梯度下降"><a href="#L50-简化代价函数与梯度下降" class="headerlink" title="L50 简化代价函数与梯度下降"></a>L50 简化代价函数与梯度下降</h2><p>把上面的代价函数改写成如下形式：<br>$$<br>J(\theta)=-\frac{1}{m} \sum_{i=1}^{m}\left[y^{(i)} \log \left(h_{\theta}\left(x^{(i)}\right)\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]<br>$$</p><blockquote><p>与之前等价，但更加紧凑。合并成一个式子便于梯度下降。</p></blockquote><p>然后可以应用梯度下降法最小化代价函数来更新参数</p><p>线性回归中的特征缩放同样适用于逻辑回归</p><h2 id="L51-高级优化"><a href="#L51-高级优化" class="headerlink" title="L51 高级优化"></a>L51 高级优化</h2><p>梯度下降法以外的优化算法：</p><p>共轭梯度，bfgs，l-bfgs：能自动选择学习率，但很复杂，没必要理解和自己实现</p><p>本节主要介绍了以上高级优化算法在Octave函数库中的调用，没仔细看…</p><h2 id="L52-多元分类：一对多"><a href="#L52-多元分类：一对多" class="headerlink" title="L52 多元分类：一对多"></a>L52 多元分类：一对多</h2><p>多元分类（相比于二分类）：分类的结果超过两种</p><p><img src="http://img.cdn.leonwang.top/20190725165626.png" alt=""></p><p>方法：三分类 =&gt; 拟合3个二分类的分类器</p><p><img src="http://img.cdn.leonwang.top/20190725170109.png" alt=""></p><p>预测（以上述三分类为例）：把 x 分别输入3个分类器，以输出 $\mathbf{h}_{\theta}(x)$ 最高的一个分类器对应的类别作为预测结果 y</p><h1 id="第8章-正则化"><a href="#第8章-正则化" class="headerlink" title="第8章 正则化"></a>第8章 正则化</h1><h2 id="L55-过拟合问题"><a href="#L55-过拟合问题" class="headerlink" title="L55 过拟合问题"></a>L55 过拟合问题</h2><blockquote><p>本节为了引出正则化</p></blockquote><p>正则化 可以改善或减少 过拟合 问题</p><p>欠拟合（underfitting）: has high bias（具有高偏差）</p><p>过拟合（overfitting）：has high variance（具有高方差）</p><p>过拟合可能会在特征过多，但样本很少时出现</p><p>避免过拟合的方法：</p><ul><li>减少特征（选取变量）的数量</li><li><strong>正则化</strong></li></ul><h2 id="L56-代价函数"><a href="#L56-代价函数" class="headerlink" title="L56 代价函数"></a>L56 代价函数</h2><blockquote><p>介绍了正则化，同时给出代价函数</p></blockquote><p>正则化：修改代价函数，来缩小各个参数的值</p><ul><li>方法：在代价函数后面加上正则化项，正则化项的系数称为正则化系数</li></ul><p>$$<br>\begin{array}{l}{J(\theta)=\frac{1}{2 m}\left[\sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+\lambda \sum_{j=1}^{n} \theta_{j}^{2}\right]} \ {\min _{\theta} J(\theta)}\end{array}<br>$$</p><h2 id="L57-线性回归的正则化"><a href="#L57-线性回归的正则化" class="headerlink" title="L57 线性回归的正则化"></a>L57 线性回归的正则化</h2><p>前面学过两种方法：</p><ol><li>梯度下降法</li><li>正规方程法</li></ol><p>分别介绍这两种方法在代价函数中加入正则化项之后应该如何应用</p><h2 id="L61-Logistic回归的正则化"><a href="#L61-Logistic回归的正则化" class="headerlink" title="L61 Logistic回归的正则化"></a>L61 Logistic回归的正则化</h2><p>前面学过两种方法：</p><ol><li>梯度下降法</li><li>高级优化</li></ol><p>分别介绍这两种方法在代价函数中加入正则化项之后应该如何应用</p><h1 id="第9章-神经网络学习"><a href="#第9章-神经网络学习" class="headerlink" title="第9章 神经网络学习"></a>第9章 神经网络学习</h1><h2 id="L62-非线性假设"><a href="#L62-非线性假设" class="headerlink" title="L62 非线性假设"></a>L62 非线性假设</h2><p>传统的逻辑回归进行分类，在面对复杂的（特征空间较大）的非线性假设时，计算量太大</p><p>所以使用神经网络</p><h2 id="L63-神经元与大脑"><a href="#L63-神经元与大脑" class="headerlink" title="L63 神经元与大脑"></a>L63 神经元与大脑</h2><p>story</p><h2 id="L64-模型展示1"><a href="#L64-模型展示1" class="headerlink" title="L64 模型展示1"></a>L64 模型展示1</h2><p>神经元的激活函数（通常sigmoid函数）</p><p>weights == parameters</p><p>神经网络： input layer — hidden layer — output layer</p><p>隐藏层可以不止一个</p><p>神经网络的 <strong>假设函数</strong> 的数学定义：</p><p><img src="http://img.cdn.leonwang.top/20190725205823.png" alt=""></p><h2 id="L65-模型展示2"><a href="#L65-模型展示2" class="headerlink" title="L65 模型展示2"></a>L65 模型展示2</h2><p>计算过程：<strong>前向传播</strong> 及其向量化表示</p><h2 id="L68-例子与直觉理解1"><a href="#L68-例子与直觉理解1" class="headerlink" title="L68 例子与直觉理解1"></a>L68 例子与直觉理解1</h2><p>使用单个神经元实现逻辑运算 AND, OR</p><p><img src="http://img.cdn.leonwang.top/20190725214908.png" alt=""></p><h2 id="L70-例子与直觉理解2"><a href="#L70-例子与直觉理解2" class="headerlink" title="L70 例子与直觉理解2"></a>L70 例子与直觉理解2</h2><p>使用单个神经元实现逻辑运算 NOT</p><p><img src="http://img.cdn.leonwang.top/20190725214854.png" alt=""></p><p>使用多层神经元的神经网络实现逻辑运算 XNOR</p><p>手写数字识别的演示视频</p><h2 id="L71-多元分类"><a href="#L71-多元分类" class="headerlink" title="L71 多元分类"></a>L71 多元分类</h2><p>神经网络实现多元分类的原理：</p><p>输出层神经元的个数与类别的个数相同，假设函数的函数值是类别的 one-hot 编码形式</p><p><img src="http://img.cdn.leonwang.top/20190725215818.png" alt=""></p><h1 id="第10章-神经网络参数的反向传播算法"><a href="#第10章-神经网络参数的反向传播算法" class="headerlink" title="第10章 神经网络参数的反向传播算法"></a>第10章 神经网络参数的反向传播算法</h1><h2 id="L72-代价函数"><a href="#L72-代价函数" class="headerlink" title="L72 代价函数"></a>L72 代价函数</h2><p>L：神经网络总层数</p><p>$s_l$ ：第 $l$ 层的神经元个数（不包括偏差单元）</p><p>神经网络中的代价函数 是 Logistic回归的代价函数 的一般形式：</p><p><img src="http://img.cdn.leonwang.top/20190809221747.png" alt=""></p><h2 id="L73-反向传播算法"><a href="#L73-反向传播算法" class="headerlink" title="L73 反向传播算法"></a>L73 反向传播算法</h2><p>神经网络中让代价函数最小的方法</p><p>$\delta_{j}^{(l)}$：第 l 层的第 j 个神经元的激活值的<strong>误差</strong></p><p>计算误差项的过程，是从输出层开始，逐渐向前计算</p><h2 id="L74-理解反向传播"><a href="#L74-理解反向传播" class="headerlink" title="L74 理解反向传播"></a>L74 理解反向传播</h2><h2 id="L75-使用注意：展开参数"><a href="#L75-使用注意：展开参数" class="headerlink" title="L75 使用注意：展开参数"></a>L75 使用注意：展开参数</h2><h2 id="L76-梯度检测"><a href="#L76-梯度检测" class="headerlink" title="L76 梯度检测"></a>L76 梯度检测</h2><h2 id="L77-随机初始化"><a href="#L77-随机初始化" class="headerlink" title="L77 随机初始化"></a>L77 随机初始化</h2><h2 id="L78-组合到一起"><a href="#L78-组合到一起" class="headerlink" title="L78 组合到一起"></a>L78 组合到一起</h2><h2 id="L80-无人驾驶"><a href="#L80-无人驾驶" class="headerlink" title="L80 无人驾驶"></a>L80 无人驾驶</h2>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 吴恩达 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Global Average Pooling</title>
      <link href="2019/07/23/Global-Average-Pooling/"/>
      <url>2019/07/23/Global-Average-Pooling/</url>
      
        <content type="html"><![CDATA[<p><strong>背景</strong></p><pre><code>最近在整理 Grad-CAM的资料，再一次看到了 GAP (Global Average Pooling)这个模块，之前在做图像分类的比赛时，看到有大佬在模型微调 (fine tune) 时使用了这个模块，而且效果显著，因此觉得有必要研究一下到底是怎么回事了。论文为主</code></pre><p>要想理解深度学习的一些知识，最好的方法就是回归原论文。GAP 最早是由 Min Lin 等人在论文<a href="https://arxiv.org/abs/1312.4400" target="_blank" rel="noopener">《Network In Network》</a>中提出的，推荐阅读一下这篇文章，里面的创新点值得一看。</p><p>在讲解 GAP 之前先做些铺垫，以便更好的理解。我们知道，常见 CNN 的网络结构如下图所示：</p><p><img src="http://img.cdn.leonwang.top/20190723152433.png" alt=""></p><p>在全连接层以前的<code>卷积层</code>负责对图像进行特征提取，<code>池化层</code>负责降采样：保留显著的特征、降低特征维度同时增大 kernel 的感受野 1。实际运用中发现网络层数越深越能获得较丰富的<code>空间信息</code>和<code>语义信息</code>，而这些也需要较大的感受野才能满足。在获取特征后，传统的做法是接上<code>全连接层</code>后再进行激活分类。问题也出在这个全连接层，虽然可以做到对 feature map2 进行降维，但是参数实在太大了，很容易造成过拟合, 既然都是为了降维，那么这个工作是不是<code>池化层</code>也可以做呢？</p><p><strong>答案是肯定的。</strong></p><p>论文作者使用 GAP 来替代最后的全连接层, 直接实现了降维，并且也极大地降低了网络参数 (全连接层的参数在整个 CNN 中占有很大的比重)，更重要的一点是保留了由前面各个卷积层和池化层提取到的空间信息，实际应用中效果提升也比较明显。另外 GAP 的另一个重要作用就是去除了对输入大小的限制，这一方面在卷积可视化 Grad-CAM 中比较重要。GAP 的网络结构图如下：</p><p><img src="http://img.cdn.leonwang.top/20190723152449.png" alt=""></p><p>GAP 真正的意义在于它实现了在整个网络结构上的正则化以实现防止过拟合的功能，原理在于传统的全连接网络 (如上图的左图) 对 feature map 进行处理时附带了庞大的参数以达到 “暗箱操作” 获取足够多的非线性特征，然后接上分类器, 由于参数众多，难免存在过拟合的现象。GAP 直接从 feature map 的通道信息下手，比如我们现在的分类有 20 种，那么最后一层的卷积输出的 feature map 就只有 20 个通道，然后对这个 feature map 进行全局池化操作，获得长度为 20 的向量，这就相当于直接赋予了每个通道类别的意义。</p><pre><code>注：1. 增大感受野就好比你站在地球上看不出它是圆的，站在外太空就可以；2. feature map 就是全连接层之前的长x高x深的块，通常称一片长x高的特征为一个feature map，深也就是最初的图像通道数；</code></pre><p>另外在 keras 中已经定义好了，调用方式如下：</p><pre><code class="python">from keras.layers import GlobalAveragePooling2D,Densefrom keras.applications import VGG16from keras.models import Modeldef build_model():    base_model = VGG16(weights=&quot;imagenet&quot;,include_top=False)    #在分类器之前使用    gap = GlobalAveragePooling2D()(base_model)    predictions = Dense(20,activation=&quot;softmax&quot;)(gap)    model = Model(inputs=base_model.input,outputs=predictions)    return model</code></pre><p>参考文献：</p><ul><li><a href="https://arxiv.org/abs/1312.4400" target="_blank" rel="noopener">Network In Network</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>卷积可视化：Grad-CAM</title>
      <link href="2019/07/23/%E5%8D%B7%E7%A7%AF%E5%8F%AF%E8%A7%86%E5%8C%96%EF%BC%9AGrad-CAM/"/>
      <url>2019/07/23/%E5%8D%B7%E7%A7%AF%E5%8F%AF%E8%A7%86%E5%8C%96%EF%BC%9AGrad-CAM/</url>
      
        <content type="html"><![CDATA[<p>论文<a href="http://cn.arxiv.org/pdf/1610.02391v3" target="_blank" rel="noopener">《Grad-CAM:Visual Explanations from Deep Networks via Gradient-based Localization》</a>，文中介绍了一种<code>卷积神经网络的解释方法</code>，通过构建类似热力图 (heatmap) 的形式，直观展示出卷积神经网络学习到的特征，简单说就是：到底我的模型关注点在哪？凭啥认为这张图中有猫？当然，其本质还是从像素角度去解释，并不能像人类那样直观的解释某一个动物为什么是猫。</p><h5 id="1-CAM"><a href="#1-CAM" class="headerlink" title="1. CAM"></a>1. CAM</h5><p>在很长一段时间内，CNN 虽然效果显著但却饱受争议，根源在于其可解释性较差，同时也因此衍生出一个新的领域：深度学习的可解释性研究。比较经典的研究方法是采用反卷积（Deconvolution）和导向反向传播（Guided-backpropagation），相关的论文也比较多就不在一一列举了，这里给出一个相对比较好的：<a href="http://cn.arxiv.org/pdf/1412.6806.pdf" target="_blank" rel="noopener">《Striving for Simplicity: The All Convolutional Net》</a>。</p><p>在介绍 CAM 之前最好对 <a href="http://spytensor.com/index.php/archives/19/" target="_blank" rel="noopener">GAP</a> 有一定的了解，因为 CAM 的作者就是借鉴了这个方法来处理。下图是 CAM 的一些直观描述：</p><p><img src="http://img.cdn.leonwang.top/20190723144640.png" alt=""></p><p>特征图经过 GAP 处理后每一个特征图包含了不同类别的信息，其具体效果如上图的 Class Activation Mapping 中的图片所示（只看图片，忽略公式），其中的权重 w 对应分类时的权重。这样做的缺陷是因为要替换全连接层为 GAP 层，因此模型要重新训练，这样的处理方式对于一些复杂的模型是行不通的，Grad-CAM 很好的解决了这个问题，具体继续往下看。</p><p>现在的问题是，即使模型训练好了，我们怎么绘制出热点图？这个比较简单，我们只需要提取出所有的权重，往回找到对应的特征图，然后进行加权求和即可。另外如果有兴趣进一步了解 CAM 的话，可以参考一下 Jacob Gildenblat 的复现： <a href="https://github.com/jacobgil/keras-cam" target="_blank" rel="noopener">keras-cam</a>, 提醒一下，这个代码本人没有进行测试，所以不能保证顺利运行。</p><p><strong>总结起来，CAM 的意义就是以热力图的形式告诉我们，模型通过哪些像素点得知图片属于某个类别。</strong></p><h5 id="2-Grad-CAM"><a href="#2-Grad-CAM" class="headerlink" title="2. Grad-CAM"></a>2. Grad-CAM</h5><p>其实 CAM 得到的效果已经很不错了，但是由于其需要修改网络结构并对模型进行重新训练，这样就导致其应用起来很不方便。Grad-CAM 和 CAM 基本思路一样，区别就在于如何获取每个特征图的权重，采用了梯度的全局平均来计算权重，论文中也给出了证明两种方式得到的权重是否等价的详细过程，如果有需要可以阅读论文进行推导。这里为了与 CAM 的权重进行区分，定义 Grad-CAM 中第 k 个特征图对应类别 c 的权重为 αkc，可以通过下面的公式计算得到：</p><p>$$<br>\alpha_{k}^{c}=\frac{1}{Z} \sum_{i} \sum_{j} \frac{\partial y^{c}}{\partial A_{i j}^{k}}<br>$$</p><p>参数解析：</p><ul><li>Z: 特征图的像素个数;</li><li>yc: 第 c 类得分的梯度 (the gradient of the score for class c)；</li><li>Aijk: 第 k 个特征图中，(i,j) 位置处的像素值；</li></ul><p>然后再求得所有的特征图对应的类别的权重后进行加权求和，这样便可以得到最后的热力图，求和公式如下：</p><p>$$<br>L_{G r a d-C A M}^{c}=\operatorname{Re} L U\left(\sum_{k} \alpha_{k}^{c} A^{k}\right)<br>$$</p><p>下图是论文中给出的 Grad-CAM 整体结构图：</p><p><img src="http://img.cdn.leonwang.top/20190723144659.png" alt=""></p><p><strong>提醒：</strong><br>论文中对最终的加权结果进行了一次 ReLU 激活处理，目的是只考虑对类别 c 有正影响的像素点。</p><h5 id="3-效果展示"><a href="#3-效果展示" class="headerlink" title="3. 效果展示"></a>3. 效果展示</h5><p><img src="http://img.cdn.leonwang.top/20190723144712.png" alt=""></p><h5 id="4-keras-实现-Grad-CAM"><a href="#4-keras-实现-Grad-CAM" class="headerlink" title="4. keras 实现 Grad-CAM"></a>4. keras 实现 Grad-CAM</h5><p>源码为 <a href="https://github.com/jacobgil/keras-grad-cam" target="_blank" rel="noopener">Github:keras-grad-cam</a>，但是可能因为框架版本的原因，存在较多的 bug 我修改后可以正常运行，贴出我纠正好的代码，另外这里只给出了 VGG16 的实现，其他模型请自行阅读模型复现源码，进行修改即可，比较容易。<br><code>python 3.6</code> <code>python-opencv 3.4.2.17</code> <code>keras 2.2.0</code> <code>tensorflow 1.9.0</code></p><pre><code class="python">from keras.applications.vgg16 import (    VGG16, preprocess_input, decode_predictions)from keras.preprocessing import imagefrom keras.layers.core import Lambdafrom keras.models import Modelfrom tensorflow.python.framework import opsimport matplotlib.pyplot as pltimport keras.backend as Kimport tensorflow as tfimport numpy as npimport kerasimport sysimport cv2def target_category_loss(x, category_index, nb_classes):    return tf.multiply(x, K.one_hot([category_index], nb_classes))def target_category_loss_output_shape(input_shape):    return input_shapedef normalize(x):    # utility function to normalize a tensor by its L2 norm    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)def load_image(path):    img_path = path    img = image.load_img(img_path, target_size=(224, 224))    x = image.img_to_array(img)    x = np.expand_dims(x, axis=0)    x = preprocess_input(x)    return xdef register_gradient():    if &quot;GuidedBackProp&quot; not in ops._gradient_registry._registry:        @ops.RegisterGradient(&quot;GuidedBackProp&quot;)        def _GuidedBackProp(op, grad):            dtype = op.inputs[0].dtype            return grad * tf.cast(grad &gt; 0., dtype) * \                tf.cast(op.inputs[0] &gt; 0., dtype)def compile_saliency_function(model, activation_layer=&#39;block5_conv3&#39;):    input_img = model.input    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])    layer_output = layer_dict[activation_layer].output    max_output = K.max(layer_output, axis=3)    saliency = K.gradients(K.sum(max_output), input_img)[0]    return K.function([input_img, K.learning_phase()], [saliency])def modify_backprop(model, name):    g = tf.get_default_graph()    with g.gradient_override_map({&#39;Relu&#39;: name}):        # get layers that have an activation        layer_dict = [layer for layer in model.layers[1:]                      if hasattr(layer, &#39;activation&#39;)]        # replace relu activation        for layer in layer_dict:            if layer.activation == keras.activations.relu:                layer.activation = tf.nn.relu        # re-instanciate a new model        new_model = VGG16(weights=&#39;imagenet&#39;)    return new_modeldef deprocess_image(x):    &#39;&#39;&#39;    Same normalization as in:    https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py    &#39;&#39;&#39;    if np.ndim(x) &gt; 3:        x = np.squeeze(x)    # normalize tensor: center on 0., ensure std is 0.1    x -= x.mean()    x /= (x.std() + 1e-5)    x *= 0.1    # clip to [0, 1]    x += 0.5    x = np.clip(x, 0, 1)    # convert to RGB array    x *= 255    if K.image_dim_ordering() == &#39;th&#39;:        x = x.transpose((1, 2, 0))    x = np.clip(x, 0, 255).astype(&#39;uint8&#39;)    return xdef _compute_gradients(tensor, var_list):  grads = tf.gradients(tensor, var_list)  return [grad if grad is not None else tf.zeros_like(var)          for var, grad in zip(var_list, grads)]def grad_cam(input_model, image, category_index, layer_name):    nb_classes = 1000    target_layer = lambda x: target_category_loss(x, category_index, nb_classes)    x = Lambda(target_layer, output_shape = target_category_loss_output_shape)(input_model.output)    model = Model(inputs=input_model.input, outputs=x)    model.summary()    loss = K.sum(model.output)    conv_output =  [l for l in model.layers if l.name is layer_name][0].output    grads = normalize(_compute_gradients(loss, [conv_output])[0])    gradient_function = K.function([model.input], [conv_output, grads])    output, grads_val = gradient_function([image])    output, grads_val = output[0, :], grads_val[0, :, :, :]    weights = np.mean(grads_val, axis = (0, 1))    cam = np.ones(output.shape[0 : 2], dtype = np.float32)    for i, w in enumerate(weights):        cam += w * output[:, :, i]    cam = cv2.resize(cam, (224, 224))    cam = np.maximum(cam, 0)    heatmap = cam / np.max(cam)    #Return to BGR [0..255] from the preprocessed image    image = image[0, :]    image -= np.min(image)    image = np.minimum(image, 255)    cam = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)    cam = np.float32(cam) + np.float32(image)    cam = 255 * cam / np.max(cam)    return np.uint8(cam), heatmappreprocessed_input = load_image(&quot;../../images/dog-cat.jpg&quot;)model = VGG16(weights=&#39;imagenet&#39;)predictions = model.predict(preprocessed_input)top_1 = decode_predictions(predictions)[0][0]print(&#39;Predicted class:&#39;)print(&#39;%s (%s) with probability %.2f&#39; % (top_1[1], top_1[0], top_1[2]))predicted_class = np.argmax(predictions)cam, heatmap = grad_cam(model, preprocessed_input, predicted_class, &quot;block5_conv3&quot;)# cv2.imwrite(&quot;../../images/gradcam.jpg&quot;, cam)register_gradient()guided_model = modify_backprop(model, &#39;GuidedBackProp&#39;)saliency_fn = compile_saliency_function(guided_model)saliency = saliency_fn([preprocessed_input, 0])gradcam = saliency[0] * heatmap[..., np.newaxis]# cv2.imwrite(&quot;../../images/guided_gradcam.jpg&quot;, deprocess_image(gradcam))origin_img = cv2.imread(&quot;../../images/dog-cat.jpg&quot;)origin_img = cv2.resize(origin_img,(414,414))cam = cv2.resize(cam,(414,414))guided_gradcam = cv2.resize(deprocess_image(gradcam),(414,414))plt.subplot(2,2,1), plt.imshow(origin_img), plt.title(&#39;origin&#39;), plt.xticks([]), plt.yticks([])plt.subplot(2,2,3), plt.imshow(cam), plt.title(&#39;gradcam&#39;), plt.xticks([]), plt.yticks([])plt.subplot(2,2,4), plt.imshow(guided_gradcam), plt.title(&#39;guided_gradcam&#39;), plt.xticks([]), plt.yticks([])plt.show()   </code></pre><h5 id="5-pytorch-实现-Grad-CAM展开目录"><a href="#5-pytorch-实现-Grad-CAM展开目录" class="headerlink" title="5. pytorch 实现 Grad-CAM展开目录"></a>5. pytorch 实现 Grad-CAM展开目录</h5><p>另外再附上一份 pytorch 版本的实现，原地址为：<a href="https://github.com/jacobgil/pytorch-grad-cam" target="_blank" rel="noopener">pytorch-grad-cam</a>，由于版本的原因，需要做一些调整，我这边使用的是 <code>pytorch 0.4.0</code>，pytorch 不像 keras 那样接口一致，所以不同的网络模型实现方式有所不同，这里只给出了 VGG 的实现方式，若想要进行修改，详细阅读模型复现源码进行修改，或者移步 <a href="https://github.com/utkuozbulak/pytorch-cnn-visualizations" target="_blank" rel="noopener">pytorch-cnn-visualizations</a> ，这里给出了比较多的可视化方法。</p><pre><code class="python">import torchfrom torch.autograd import Variablefrom torch.autograd import Functionfrom torchvision import modelsfrom torchvision import utilsimport cv2import sysimport numpy as npimport argparseclass FeatureExtractor():    &quot;&quot;&quot; Class for extracting activations and     registering gradients from targetted intermediate layers &quot;&quot;&quot;    def __init__(self, model, target_layers):        self.model = model        self.target_layers = target_layers        self.gradients = []    def save_gradient(self, grad):        self.gradients.append(grad)    def __call__(self, x):        outputs = []        self.gradients = []        for name, module in self.model._modules.items():            x = module(x)            if name in self.target_layers:                x.register_hook(self.save_gradient)                outputs += [x]        return outputs, xclass ModelOutputs():    &quot;&quot;&quot; Class for making a forward pass, and getting:    1. The network output.    2. Activations from intermeddiate targetted layers.    3. Gradients from intermeddiate targetted layers. &quot;&quot;&quot;    def __init__(self, model, target_layers):        self.model = model        self.feature_extractor = FeatureExtractor(self.model.features, target_layers)    def get_gradients(self):        return self.feature_extractor.gradients    def __call__(self, x):        target_activations, output  = self.feature_extractor(x)        output = output.view(output.size(0), -1)        output = self.model.classifier(output)        return target_activations, outputdef preprocess_image(img):    means=[0.485, 0.456, 0.406]    stds=[0.229, 0.224, 0.225]    preprocessed_img = img.copy()[: , :, ::-1]    for i in range(3):        preprocessed_img[:, :, i] = preprocessed_img[:, :, i] - means[i]        preprocessed_img[:, :, i] = preprocessed_img[:, :, i] / stds[i]    preprocessed_img = \        np.ascontiguousarray(np.transpose(preprocessed_img, (2, 0, 1)))    preprocessed_img = torch.from_numpy(preprocessed_img)    preprocessed_img.unsqueeze_(0)    input = Variable(preprocessed_img, requires_grad = True)    return inputdef show_cam_on_image(img, mask):    heatmap = cv2.applyColorMap(np.uint8(255*mask), cv2.COLORMAP_JET)    heatmap = np.float32(heatmap) / 255    cam = heatmap + np.float32(img)    cam = cam / np.max(cam)    cv2.imwrite(&quot;../../images/cam01.jpg&quot;, np.uint8(255 * cam))class GradCam:    def __init__(self, model, target_layer_names, use_cuda):        self.model = model        self.model.eval()        self.cuda = use_cuda        if self.cuda:            self.model = model.cuda()        self.extractor = ModelOutputs(self.model, target_layer_names)    def forward(self, input):        return self.model(input)     def __call__(self, input, index = None):        if self.cuda:            features, output = self.extractor(input.cuda())        else:            features, output = self.extractor(input)        if index == None:            index = np.argmax(output.cpu().data.numpy())        one_hot = np.zeros((1, output.size()[-1]), dtype = np.float32)        one_hot[0][index] = 1        one_hot = Variable(torch.from_numpy(one_hot), requires_grad = True)        if self.cuda:            one_hot = torch.sum(one_hot.cuda() * output)        else:            one_hot = torch.sum(one_hot * output)        self.model.features.zero_grad()        self.model.classifier.zero_grad()        #one_hot.backward(retain_variables=True)        one_hot.backward()        grads_val = self.extractor.get_gradients()[-1].cpu().data.numpy()        target = features[-1]        target = target.cpu().data.numpy()[0, :]        weights = np.mean(grads_val, axis = (2, 3))[0, :]        cam = np.zeros(target.shape[1 : ], dtype = np.float32)        for i, w in enumerate(weights):            cam += w * target[i, :, :]        cam = np.maximum(cam, 0)        cam = cv2.resize(cam, (224, 224))        cam = cam - np.min(cam)        cam = cam / np.max(cam)        return camclass GuidedBackpropReLU(Function):    def forward(self, input):        positive_mask = (input &gt; 0).type_as(input)        output = torch.addcmul(torch.zeros(input.size()).type_as(input), input, positive_mask)        self.save_for_backward(input, output)        return output    def backward(self, grad_output):        input, output = self.saved_tensors        grad_input = None        positive_mask_1 = (input &gt; 0).type_as(grad_output)        positive_mask_2 = (grad_output &gt; 0).type_as(grad_output)        grad_input = torch.addcmul(torch.zeros(input.size()).type_as(input), torch.addcmul(torch.zeros(input.size()).type_as(input), grad_output, positive_mask_1), positive_mask_2)        return grad_inputclass GuidedBackpropReLUModel:    def __init__(self, model, use_cuda):        self.model = model        self.model.eval()        self.cuda = use_cuda        if self.cuda:            self.model = model.cuda()        # replace ReLU with GuidedBackpropReLU        for idx, module in self.model.features._modules.items():            if module.__class__.__name__ == &#39;ReLU&#39;:                self.model.features._modules[idx] = GuidedBackpropReLU()    def forward(self, input):        return self.model(input)    def __call__(self, input, index = None):        if self.cuda:            output = self.forward(input.cuda())        else:            output = self.forward(input)        if index == None:            index = np.argmax(output.cpu().data.numpy())        one_hot = np.zeros((1, output.size()[-1]), dtype = np.float32)        one_hot[0][index] = 1        one_hot = Variable(torch.from_numpy(one_hot), requires_grad = True)        if self.cuda:            one_hot = torch.sum(one_hot.cuda() * output)        else:            one_hot = torch.sum(one_hot * output)        # self.model.features.zero_grad()        # self.model.classifier.zero_grad()        one_hot.backward()        output = input.grad.cpu().data.numpy()        output = output[0,:,:,:]        return outputif __name__ == &#39;__main__&#39;:    &quot;&quot;&quot; python grad_cam.py &lt;path_to_image&gt;    1. Loads an image with opencv.    2. Preprocesses it for VGG19 and converts to a pytorch variable.    3. Makes a forward pass to find the category index with the highest score,    and computes intermediate activations.    Makes the visualization. &quot;&quot;&quot;    image_path = &quot;../../images/dog-cat.jpg&quot;    # Can work with any model, but it assumes that the model has a     # feature method, and a classifier method,    # as in the VGG models in torchvision.    grad_cam = GradCam(model = models.vgg19(pretrained=True), \                    target_layer_names = [&quot;35&quot;], use_cuda=True)    img = cv2.imread(image_path, 1)    img = np.float32(cv2.resize(img, (224, 224))) / 255    input = preprocess_image(img)    # If None, returns the map for the highest scoring category.    # Otherwise, targets the requested index.    target_index = None    mask = grad_cam(input, target_index)    show_cam_on_image(img, mask)    gb_model = GuidedBackpropReLUModel(model = models.vgg19(pretrained=True), use_cuda=True)    gb = gb_model(input, index=target_index)    utils.save_image(torch.from_numpy(gb), &#39;../../images/gb.jpg&#39;)    cam_mask = np.zeros(gb.shape)    for i in range(0, gb.shape[0]):        cam_mask[i, :, :] = mask    cam_gb = np.multiply(cam_mask, gb)    utils.save_image(torch.from_numpy(cam_gb), &#39;../../images/cam_gb.jpg&#39;)</code></pre><h5 id="6-参考文献"><a href="#6-参考文献" class="headerlink" title="6. 参考文献"></a>6. 参考文献</h5><ul><li><a href="http://cn.arxiv.org/pdf/1610.02391v3" target="_blank" rel="noopener">《Grad-CAM:Visual Explanations from Deep Networks via Gradient-based Localization》</a></li><li><a href="http://cn.arxiv.org/pdf/1412.6806.pdf" target="_blank" rel="noopener">《Striving for Simplicity: The All Convolutional Net》</a></li><li><a href="https://github.com/jacobgil/keras-cam" target="_blank" rel="noopener">keras-cam</a></li><li><a href="https://github.com/jacobgil/keras-grad-cam" target="_blank" rel="noopener">Github:keras-grad-cam</a></li><li><a href="https://github.com/jacobgil/pytorch-grad-cam" target="_blank" rel="noopener">pytorch-grad-cam</a></li><li><a href="https://github.com/utkuozbulak/pytorch-cnn-visualizations" target="_blank" rel="noopener">pytorch-cnn-visualizations</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ResNet &amp; K-Fold Cross Validation</title>
      <link href="2019/07/21/ResNet-K-Fold-Cross-Validation/"/>
      <url>2019/07/21/ResNet-K-Fold-Cross-Validation/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p>目的：解决深度网络的退化问题</p><p>随着网络的加深，出现了<strong>训练集</strong>准确率下降的现象，我们可以确定<strong>这不是由于Overfit过拟合造成的</strong>(过拟合的情况训练集应该准确率很高)；所以作者针对这个问题提出了一种全新的网络，叫深度残差网络，它允许网络尽可能的加深，其中引入了全新的结构如图（残差学习单元）； </p><p><img src="http://img.cdn.leonwang.top/resnet.png" alt=""></p><p><strong>残差</strong></p><p>残差指的是预测值和观测值之间的差异</p><blockquote><p>不要和误差混淆</p><p>误差：观测值和真实值之间的差异</p></blockquote><p>ResNet提出了两种mapping：一种是identity mapping，指的就是图1中”弯弯的曲线”，另一种residual mapping，指的就是除了”弯弯的曲线“那部分，所以最后的输出是 y=F(x)+x</p><p>identity mapping顾名思义，就是指本身，也就是公式中的x，而residual mapping指的是“差”，也就是y−x，所以残差指的就是F(x)部分。 </p><p>理论上，对于“随着网络加深，准确率下降”的问题，Resnet提供了两种选择方式，也就是identity mapping和residual mapping，如果网络已经到达最优，继续加深网络，residual mapping将被push为0，只剩下identity mapping，这样理论上网络一直处于最优状态了，网络的性能也就不会随着深度增加而降低了。</p><h2 id="K-Fold-Cross-Validation"><a href="#K-Fold-Cross-Validation" class="headerlink" title="K-Fold Cross Validation"></a>K-Fold Cross Validation</h2><blockquote><p> K次交叉验证，将训练集分割成K个子样本，一个单独的子样本被保留作为验证模型的数据，其他K-1个样本用来训练。交叉验证重复K次，每个子样本验证一次，平均K次的结果或者使用其它结合方式，最终得到一个单一估测。这个方法的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10次交叉验证是最常用的。</p></blockquote><p><img src="http://img.cdn.leonwang.top/kfold.png" alt=""></p><h1 id="scikit-Learn和Keras实践"><a href="#scikit-Learn和Keras实践" class="headerlink" title="scikit-Learn和Keras实践"></a>scikit-Learn和Keras实践</h1><p>Keras在深度学习很受欢迎，但是只能做深度学习：Keras是最小化的深度学习库，目标在于快速搭建深度学习模型。</p><p>基于SciPy的scikit-learn，数值运算效率很高，适用于普遍的机器学习任务，提供很多机器学习工具，包括但不限于：</p><ul><li>使用K折验证模型</li><li>快速搜索并测试超参</li></ul><p>Keras为scikit-learn封装了<code>KerasClassifier</code>和<code>KerasRegressor</code>。</p><p><a href="https://keras.io/zh/scikit-learn-api/" target="_blank" rel="noopener">Keras 中的 Scikit-Learn API 的封装器文档</a></p><p><a href="https://scikit-learn.org/stable/modules/cross_validation.html#k-fold" target="_blank" rel="noopener">Scikit-Learn 交叉验证文档</a></p><h2 id="Scikit-Learn-中的交叉验证"><a href="#Scikit-Learn-中的交叉验证" class="headerlink" title="Scikit-Learn 中的交叉验证"></a>Scikit-Learn 中的交叉验证</h2><p>基础验证法：</p><pre><code class="python">from sklearn.datasets import load_iris # iris数据集from sklearn.model_selection import train_test_split # 分割数据模块from sklearn.neighbors import KNeighborsClassifier # K最近邻(kNN，k-NearestNeighbor)分类算法#加载iris数据集iris = load_iris()X = iris.datay = iris.target#分割数据并X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4)#建立模型knn = KNeighborsClassifier()#训练模型knn.fit(X_train, y_train)#将准确率打印出print(knn.score(X_test, y_test))# 0.973684210526</code></pre><p>交叉验证法：</p><pre><code class="python">from sklearn.cross_validation import cross_val_score # K折交叉验证模块#使用K折交叉验证模块scores = cross_val_score(knn, X, y, cv=5, scoring=&#39;accuracy&#39;)#将5次的预测准确率打印出print(scores)# [ 0.96666667  1.          0.93333333  0.96666667  1.        ]#将5次的预测准确平均率打印出print(scores.mean())# 0.973333333333</code></pre><h2 id="Scikit-Learn-对-Keras-模型的交叉验证"><a href="#Scikit-Learn-对-Keras-模型的交叉验证" class="headerlink" title="Scikit-Learn 对 Keras 模型的交叉验证"></a>Scikit-Learn 对 Keras 模型的交叉验证</h2><h3 id="Tutorial-1：使用Scikit-learn调用Keras模型"><a href="#Tutorial-1：使用Scikit-learn调用Keras模型" class="headerlink" title="Tutorial 1：使用Scikit-learn调用Keras模型"></a>Tutorial 1：使用Scikit-learn调用Keras模型</h3><p><a href="https://cnbeining.github.io/deep-learning-with-python-cn/3-multi-layer-perceptrons/ch9-use-keras-models-with-scikit-learn-for-general-machine-learning.html" target="_blank" rel="noopener">使用Scikit-Learn调用Keras的模型</a></p><ul><li>使用scikit-learn封装Keras的模型</li><li>使用scikit-learn对Keras的模型进行交叉验证</li><li>使用scikit-learn，利用网格搜索调整Keras模型的超参</li></ul><p>(关于交叉验证的代码有点过时，cross_validation已经被model_selection取代)</p><p>使用交叉验证检验深度学习模型</p><pre><code class="python"># MLP for Pima Indians Dataset with 10-fold cross validation via sklearnfrom keras.models import Sequentialfrom keras.layers import Densefrom keras.wrappers.scikit_learn import KerasClassifierfrom sklearn.cross_validation import StratifiedKFoldfrom sklearn.cross_validation import cross_val_scoreimport numpyimport pandas# Function to create model, required for KerasClassifierdef create_model():    # create model    model = Sequential()    model.add(Dense(12, input_dim=8, init=&#39;uniform&#39;, activation=&#39;relu&#39;)) model.add(Dense(8, init=&#39;uniform&#39;, activation=&#39;relu&#39;)) model.add(Dense(1, init=&#39;uniform&#39;, activation=&#39;sigmoid&#39;))    # Compile model    model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) return model# fix random seed for reproducibilityseed = 7numpy.random.seed(seed)# load pima indians datasetdataset = numpy.loadtxt(&quot;pima-indians-diabetes.csv&quot;, delimiter=&quot;,&quot;)# split into input (X) and output (Y) variablesX = dataset[:,0:8]Y = dataset[:,8]# create modelmodel = KerasClassifier(build_fn=create_model, nb_epoch=150, batch_size=10)# evaluate using 10-fold cross validationkfold = StratifiedKFold(y=Y, n_folds=10, shuffle=True, random_state=seed)results = cross_val_score(model, X, Y, cv=kfold)print(results.mean())</code></pre><h3 id="Tutorial-2：基于-skl3earn-和-keras-的数据切分与交叉验证"><a href="#Tutorial-2：基于-skl3earn-和-keras-的数据切分与交叉验证" class="headerlink" title="Tutorial 2：基于 skl3earn 和 keras 的数据切分与交叉验证"></a>Tutorial 2：基于 skl3earn 和 keras 的数据切分与交叉验证</h3><p><a href="https://www.cnblogs.com/bymo/p/9026198.html#_label2" target="_blank" rel="noopener"><a href="https://www.cnblogs.com/bymo/p/9026198.html" target="_blank" rel="noopener">基于sklearn和keras的数据切分与交叉验证</a></a></p><h4 id="自动切分"><a href="#自动切分" class="headerlink" title="自动切分"></a>自动切分</h4><p>在Keras中，可以从数据集中切分出一部分作为验证集，并且在每次迭代(epoch)时在验证集中评估模型的性能．</p><p>具体地，调用<strong>model.fit()</strong>训练模型时，可通过<strong>validation_split</strong>参数来指定从数据集中切分出验证集的比例．</p><pre><code class="python"># MLP with automatic validation setfrom keras.models import Sequentialfrom keras.layers import Denseimport numpy# fix random seed for reproducibilitynumpy.random.seed(7)# load pima indians datasetdataset = numpy.loadtxt(&quot;pima-indians-diabetes.csv&quot;, delimiter=&quot;,&quot;)# split into input (X) and output (Y) variablesX = dataset[:,0:8]Y = dataset[:,8]# create modelmodel = Sequential()model.add(Dense(12, input_dim=8, activation=&#39;relu&#39;))model.add(Dense(8, activation=&#39;relu&#39;))model.add(Dense(1, activation=&#39;sigmoid&#39;))# Compile modelmodel.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;])# Fit the modelmodel.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10)</code></pre><p>validation_split：0~1之间的浮点数，用来指定训练集的一定比例数据作为验证集。验证集将不参与训练，并在每个epoch结束后测试的模型的指标，如损失函数、精确度等。</p><p><strong>注意，validation_split的划分在shuffle之前，因此如果你的数据本身是有序的，需要先手工打乱再指定validation_split，否则可能会出现验证集样本不均匀。</strong></p><h4 id="手动切分"><a href="#手动切分" class="headerlink" title="手动切分"></a>手动切分</h4><p>Keras允许在训练模型的时候手动指定验证集．</p><p>例如，用<strong>sklearn</strong>库中的<strong>train_test_split()</strong>函数将数据集进行切分，然后在<strong>keras</strong>的<strong>model.fit()</strong>的时候通过<strong>validation_data</strong>参数指定前面切分出来的验证集．</p><pre><code class="python"># MLP with manual validation setfrom keras.models import Sequentialfrom keras.layers import Densefrom sklearn.model_selection import train_test_splitimport numpy# fix random seed for reproducibilityseed = 7numpy.random.seed(seed)# load pima indians datasetdataset = numpy.loadtxt(&quot;pima-indians-diabetes.csv&quot;, delimiter=&quot;,&quot;)# split into input (X) and output (Y) variablesX = dataset[:,0:8]Y = dataset[:,8]# split into 67% for train and 33% for testX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seed)# create modelmodel = Sequential()model.add(Dense(12, input_dim=8, activation=&#39;relu&#39;))model.add(Dense(8, activation=&#39;relu&#39;))model.add(Dense(1, activation=&#39;sigmoid&#39;))# Compile modelmodel.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;])# Fit the modelmodel.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=150, batch_size=10)</code></pre><h4 id="K折交叉验证"><a href="#K折交叉验证" class="headerlink" title="K折交叉验证"></a>K折交叉验证</h4><p>将数据集分成k份，每一轮用其中(k-1)份做训练而剩余1份做验证，以这种方式执行k轮，得到k个模型．将k次的性能取平均，作为该算法的整体性能．k一般取值为5或者10．</p><ul><li>优点：能比较鲁棒性地评估模型在未知数据上的性能．</li><li>缺点：计算复杂度较大．因此，在数据集较大，模型复杂度较高，或者计算资源不是很充沛的情况下，可能不适用，尤其是在训练深度学习模型的时候．</li></ul><p><strong>sklearn.model_selection</strong>提供了<strong>KFold</strong>以及<strong>RepeatedKFold, LeaveOneOut, LeavePOut, ShuffleSplit, StratifiedKFold, GroupKFold, TimeSeriesSplit</strong>等变体．</p><p>下面的例子中用的StratifiedKFold采用的是分层抽样，它保证各类别的样本在切割后每一份小数据集中的比例都与原数据集中的比例相同．</p><pre><code class="python"># MLP for Pima Indians Dataset with 10-fold cross validationfrom keras.models import Sequentialfrom keras.layers import Densefrom sklearn.model_selection import StratifiedKFoldimport numpy# fix random seed for reproducibilityseed = 7numpy.random.seed(seed)# load pima indians datasetdataset = numpy.loadtxt(&quot;pima-indians-diabetes.csv&quot;, delimiter=&quot;,&quot;)# split into input (X) and output (Y) variablesX = dataset[:,0:8]Y = dataset[:,8]# define 10-fold cross validation test harnesskfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)cvscores = []for train, test in kfold.split(X, Y):      # create model    model = Sequential()    model.add(Dense(12, input_dim=8, activation=&#39;relu&#39;))    model.add(Dense(8, activation=&#39;relu&#39;))    model.add(Dense(1, activation=&#39;sigmoid&#39;))    # Compile model    model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;])    # Fit the model    model.fit(X[train], Y[train], epochs=150, batch_size=10, verbose=0)    # evaluate the model    scores = model.evaluate(X[test], Y[test], verbose=0)    print(&quot;%s: %.2f%%&quot; % (model.metrics_names[1], scores[1]*100))    cvscores.append(scores[1] * 100)print(&quot;%.2f%% (+/- %.2f%%)&quot; % (numpy.mean(cvscores), numpy.std(cvscores)))</code></pre><blockquote><p>这里只用了sklearn做了数据集的切分，没有用sklearn中的cross_val_score()做训练和交叉验证，而是用的keras中的model.fit()进行训练</p></blockquote><h2 id="References"><a href="#References" class="headerlink" title="References"></a><em>References</em></h2><p><a href="https://blog.csdn.net/lanran2/article/details/79057994" target="_blank" rel="noopener">ResNet 解析 ——CSDN</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Siamese Network</title>
      <link href="2019/07/21/Siamese-Network/"/>
      <url>2019/07/21/Siamese-Network/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2 id="Siamese-Network-2-branches-network"><a href="#Siamese-Network-2-branches-network" class="headerlink" title="Siamese Network (2-branches network)"></a>Siamese Network (2-branches network)</h2><p>siamese 网络， 是05年Yann Lecun提出来的。它的特点是它接收两个图片作为输入，而不是一张图片作为输入。</p><p><img src="http://img.cdn.leonwang.top/sianet.png" alt=""></p><hr><p><img src="http://img.cdn.leonwang.top/decisionnet.png" alt=""></p><p>目的：比较两幅图片是否相似（相似度）</p><p>输入：两幅图片</p><p>输出：一个相似度数值</p><p><strong>Siamese &amp; Pseudo-Siamese</strong></p><p>Siamese Network: 两个神经网络共享权值，在代码实现的时候，甚至可以是同一个网络，不用实现另外一个，因为权值都一样。</p><p><img src="http://img.cdn.leonwang.top/Xnip2019-07-21_10-39-10.jpg" alt=""></p><p>Pseudo-Siamese Network（伪孪生神经网络）：两边可以是不同的神经网络（如一个是lstm，一个是cnn），也可以是相同类型的神经网络。</p><p><img src="http://img.cdn.leonwang.top/Xnip2019-07-21_10-42-34.jpg" alt=""></p><p><strong>孪生神经网络和伪孪生神经网络分别适用的场景</strong></p><p>孪生神经网络用于处理两个输入<strong>“比较类似”</strong>的情况。伪孪生神经网络适用于处理两个输入<strong>“有一定差别”</strong>的情况。比如，我们要计算两个句子或者词汇的语义相似度，使用siamese network比较适合；如果验证标题与正文的描述是否一致（标题和正文长度差别很大），或者文字是否描述了一幅图片（一个是图片，一个是文字），就应该使用pseudo-siamese network。也就是说，要根据具体的应用，判断应该使用哪一种结构，哪一种Loss。</p><p><strong>Siamese network的用途</strong></p><p>应用领域很多，nlp&amp;cv领域都有很多应用。</p><ul><li>词汇的语义相似度分析，QA中question和answer的匹配，签名/人脸验证。</li><li>手写体识别（已有github代码）。</li><li>kaggle上Quora的question pair的比赛，即判断两个提问是不是同一问题，冠军队伍用的就是n多特征+Siamese network。</li><li>在图像上，基于Siamese网络的视觉跟踪算法也已经成为热点《<a href="https://link.springer.com/chapter/10.1007/978-3-319-48881-3_56" target="_blank" rel="noopener">Fully-convolutional siamese networks for object tracking</a>》。</li></ul><h2 id="改进的-Siamese-Network-2-channel-network"><a href="#改进的-Siamese-Network-2-channel-network" class="headerlink" title="改进的 Siamese Network (2-channel network)"></a>改进的 Siamese Network (2-channel network)</h2><p>2015年CVPR的一篇关于图像相似度计算的文章：《Learning to Compare Image Patches via Convolutional Neural Networks》，本篇文章对经典的算法Siamese Networks 做了改进。</p><p>Siamese 网络(2-branches networks)的大体思路：</p><ol><li>让patch1、patch2分别经过网络，进行提取特征向量(Siamese 对于两张图片patch1、patch2的特征提取过程是相互独立的)</li><li>然后在最后一层对两个两个特征向量做一个相似度损失函数，进行网络训练。</li></ol><p>paper所提出的算法(2-channel networks) 的大体思路：</p><ol><li>把patch1、patch2合在一起，把这两张图片，看成是一张双通道的图像。也就是把两个(1，64，64)单通道的数据，放在一起，成为了(2，64，64)的双通道矩阵，</li><li>然后把这个矩阵数据作为网络的输入，这就是所谓的：2-channel。</li></ol><p>这样，跳过了分支的显式的特征提取过程，而是直接学习相似度评价函数。最后一层直接是全连接层，输出神经元个数直接为1，直接表示两张图片的相似度。当然CNN，如果输入的是双通道图片，也就是相当于网络的输入的是2个feature map，经过第一层的卷积后网，两张图片的像素就进行了相关的加权组合并映射，这也就是说，用2-channel的方法，经过了第一次的卷积后，两张输入图片就不分你我了。而Siamese网络是到了最后全连接的时候，两张图片的相关神经元才联系在一起。2015年CVPR文章几个重要创新写在下面：</p><h2 id="损失函数：Contrastive-Loss（对比损失）"><a href="#损失函数：Contrastive-Loss（对比损失）" class="headerlink" title="损失函数：Contrastive Loss（对比损失）"></a>损失函数：Contrastive Loss（对比损失）</h2><p>传统的siamese network使用Contrastive Loss。损失函数还有更多的选择，siamese network的初衷是计算两个输入的相似度,。左右两个神经网络分别将输入转换成一个”向量”，在新的空间中，通过判断cosine距离就能得到相似度了。Cosine是一个选择，exp function也是一种选择，欧式距离什么的都可以，训练的目标是让两个相似的输入距离尽可能的小，两个不同类别的输入距离尽可能的大。</p><p>在Keras的孪生神经网络（siamese network）中，其采用的损失函数是contrastive loss，这种损失函数可以有效的处理孪生神经网络中的paired data的关系。contrastive loss的表达式如下：<br>$$<br>L=\frac{1}{2 N} \sum_{n=1}^{N} y d^{2}+(1-y) m a x(\text {margin}-d, 0)^{2}<br>$$<br>其中$d=\left|a_{n}-b_{n}\right|_{2}$，代表两个样本特征的欧氏距离，y为两个样本是否匹配的标签，y=1代表两个样本相似或者匹配，y=0则代表不匹配，margin为设定的阈值。</p><p>这种损失函数最初来源于Yann LeCun的Dimensionality Reduction by Learning an Invariant Mapping，主要是用在降维中，即本来相似的样本，在经过降维（特征提取）后，在特征空间中，两个样本仍旧相似；而原本不相似的样本，在经过降维后，在特征空间中，两个样本仍旧不相似。</p><p>观察上述的contrastive loss的表达式可以发现，这种损失函数可以很好的表达成对样本的匹配程度，也能够很好用于训练提取特征的模型。当y=1（即样本相似）时，损失函数只剩下$\sum y d^{2}$，即原本相似的样本，如果在特征空间的欧式距离较大，则说明当前的模型不好，因此加大损失。而当y=0时（即样本不相似）时，损失函数为$\sum(1-y) \max (\operatorname{margin}-d, 0)^{2}$，即当样本不相似时，其特征空间的欧式距离反而小的话，损失值会变大，这也正好符合我们的要求。</p><hr><h2 id="References"><a href="#References" class="headerlink" title="References"></a><em>References</em></h2><p>[1] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity metric discriminatively, with application to face verification. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 539–546. IEEE, 2005. </p><p>[2]Hadsell, R., Chopra, S., LeCun, Y.. Dimensionality Reduction by Learning an Invariant Mapping[P]. Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on,2006.</p><p>[3] <a href="https://blog.csdn.net/qq_15192373/article/details/78404761" target="_blank" rel="noopener">siamese(孪生) 网络 ——CSDN</a></p><p>[4]<a href="https://blog.csdn.net/autocyz/article/details/53149760" target="_blank" rel="noopener">Contrastive Loss（损失函数）——CSDN</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Keras 中的损失函数</title>
      <link href="2019/07/21/Keras-%E4%B8%AD%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
      <url>2019/07/21/Keras-%E4%B8%AD%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="Keras-中的损失函数"><a href="#Keras-中的损失函数" class="headerlink" title="Keras 中的损失函数"></a>Keras 中的损失函数</h1><p><a href="https://zhuanlan.zhihu.com/p/34667893" target="_blank" rel="noopener">Keras 中的损失函数</a></p><p>损失函数是模型优化的目标，所以又叫目标函数、优化评分函数，在keras中，模型编译的参数loss指定了损失函数的类别，有两种指定方法：</p><pre><code class="python">model.compile(loss=&#39;mean_squared_error&#39;, optimizer=&#39;sgd&#39;)</code></pre><p>或者</p><pre><code class="python">from keras import lossesmodel.compile(loss=losses.mean_squared_error, optimizer=&#39;sgd&#39;)</code></pre><p>你可以传递一个现有的损失函数名，或者一个TensorFlow/Theano符号函数。 该符号函数为每个数据点返回一个标量，有以下两个参数:</p><ul><li>y_true: 真实标签. TensorFlow/Theano张量</li><li>y_pred: 预测值. TensorFlow/Theano张量，其shape与y_true相同</li></ul><p>实际的优化目标是所有数据点的输出数组的平均值。</p><h2 id="mean-squared-error：均方误差"><a href="#mean-squared-error：均方误差" class="headerlink" title="mean_squared_error：均方误差"></a><strong>mean_squared_error：均方误差</strong></h2><pre><code class="python">mean_squared_error(y_true, y_pred)</code></pre><p>源码：</p><pre><code class="python">def mean_squared_error(y_true, y_pred):    return K.mean(K.square(y_pred - y_true), axis=-1)</code></pre><p>说明：</p><p>MSE: <img src="https://www.zhihu.com/equation?tex=L%3D+%5Cfrac+%7B1%7D%7Bn%7D+%5Csum%5En_%7Bi%3D1%7D+%28y_%7Btrue%7D%5E%7B%28i%29%7D+-+y_%7Bpred%7D%5E%7B%28i%29%7D%29%5E2" alt="[公式]"></p><h2 id="mean-absolute-error"><a href="#mean-absolute-error" class="headerlink" title="mean_absolute_error"></a><strong>mean_absolute_error</strong></h2><pre><code class="python">mean_absolute_error(y_true, y_pred)</code></pre><p>源码：</p><pre><code class="python">def mean_absolute_error(y_true, y_pred):    return K.mean(K.abs(y_pred - y_true), axis=-1)</code></pre><p>说明：</p><p>MAE： <img src="https://www.zhihu.com/equation?tex=L%3D+%5Cfrac+%7B1%7D%7Bn%7D+%5Csum%5En_%7Bi%3D1%7D+%7C%28y_%7Btrue%7D%5E%7B%28i%29%7D+-+y_%7Bpred%7D%5E%7B%28i%29%7D%29%7C" alt="[公式]"></p><h2 id="mean-absolute-percentage-error"><a href="#mean-absolute-percentage-error" class="headerlink" title="mean_absolute_percentage_error"></a><strong>mean_absolute_percentage_error</strong></h2><pre><code class="python">mean_absolute_percentage_error(y_true, y_pred)</code></pre><p>源码：</p><pre><code class="python">def mean_absolute_percentage_error(y_true, y_pred):    diff = K.abs((y_true - y_pred) / K.clip(K.abs(y_true),                                            K.epsilon(),                                            None))    return 100. * K.mean(diff, axis=-1)</code></pre><p>说明：</p><p>MAPE： <img src="https://www.zhihu.com/equation?tex=L%3D+%5Cfrac+%7B1%7D%7Bn%7D+%5Csum%5En_%7Bi%3D1%7D+%7C%5Cfrac+%7By_%7Btrue%7D%5E%7B%28i%29%7D+-+y_%7Bpred%7D%5E%7B%28i%29%7D%7D%7By_%7Btrue%7D%5E%7B%28i%29%7D%7D%7C+%5Ccdot+100" alt="[公式]"></p><h2 id="mean-squared-logarithmic-error"><a href="#mean-squared-logarithmic-error" class="headerlink" title="mean_squared_logarithmic_error"></a><strong>mean_squared_logarithmic_error</strong></h2><pre><code class="python">mean_squared_logarithmic_error(y_true, y_pred)</code></pre><p>源码：</p><pre><code class="python">def mean_squared_logarithmic_error(y_true, y_pred):    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)    return K.mean(K.square(first_log - second_log), axis=-1)</code></pre><p>说明：</p><p>MAPE： <img src="https://www.zhihu.com/equation?tex=L%3D+%5Cfrac+%7B1%7D%7Bn%7D+%5Csum%5En_%7Bi%3D1%7D+%28log%28y_%7Btrue%7D%5E%7B%28i%29%7D+%2B1%29+-+log%28+y_%7Bpred%7D%5E%7B%28i%29%7D%2B1%29%29%5E2" alt="[公式]"></p><h2 id="squared-hinge"><a href="#squared-hinge" class="headerlink" title="squared_hinge"></a><strong>squared_hinge</strong></h2><pre><code class="python">squared_hinge(y_true, y_pred)</code></pre><p>源码：</p><pre><code class="python">def squared_hinge(y_true, y_pred):    return K.mean(K.square(K.maximum(1. - y_true * y_pred, 0.)), axis=-1)</code></pre><p>说明：</p><p><img src="https://www.zhihu.com/equation?tex=L%3D+%5Cfrac+%7B1%7D%7Bn%7D+%5Csum%5En_%7Bi%3D1%7D+%28max%280%2C1-y_%7Bpred%7D%5E%7B%28i%29%7D+%5Ccdot+y_%7Btrue%7D%5E%7B%28i%29%7D%29%29%5E2" alt="[公式]"></p><h2 id="hinge"><a href="#hinge" class="headerlink" title="hinge"></a><strong>hinge</strong></h2><pre><code class="python">hinge(y_true, y_pred)</code></pre><p>源码：</p><pre><code class="python">def hinge(y_true, y_pred):    return K.mean(K.maximum(1. - y_true * y_pred, 0.), axis=-1)</code></pre><p>说明：</p><p><img src="https://www.zhihu.com/equation?tex=L%3D+%5Cfrac+%7B1%7D%7Bn%7D+%5Csum%5En_%7Bi%3D1%7D+max%280%2C1-y_%7Bpred%7D%5E%7B%28i%29%7D+%5Ccdot+y_%7Btrue%7D%5E%7B%28i%29%7D%29" alt="[公式]"></p><h2 id="categorical-hinge"><a href="#categorical-hinge" class="headerlink" title="categorical_hinge"></a><strong>categorical_hinge</strong></h2><pre><code class="python">categorical_hinge(y_true, y_pred)</code></pre><p>源码：</p><pre><code class="python">def categorical_hinge(y_true, y_pred):    pos = K.sum(y_true * y_pred, axis=-1)    neg = K.max((1. - y_true) * y_pred, axis=-1)    return K.maximum(0., neg - pos + 1.)</code></pre><h2 id="logcosh"><a href="#logcosh" class="headerlink" title="logcosh"></a><strong>logcosh</strong></h2><pre><code class="python">logcosh(y_true, y_pred)</code></pre><p>源码：</p><pre><code class="python">def logcosh(y_true, y_pred):    &quot;&quot;&quot;Logarithm of the hyperbolic cosine of the prediction error.    `log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small `x` and    to `abs(x) - log(2)` for large `x`. This means that &#39;logcosh&#39; works mostly    like the mean squared error, but will not be so strongly affected by the    occasional wildly incorrect prediction.    # Arguments        y_true: tensor of true targets.        y_pred: tensor of predicted targets.    # Returns        Tensor with one scalar loss entry per sample.    &quot;&quot;&quot;    def _logcosh(x):        return x + K.softplus(-2. * x) - K.log(2.)    return K.mean(_logcosh(y_pred - y_true), axis=-1)</code></pre><h2 id="categorical-crossentropy"><a href="#categorical-crossentropy" class="headerlink" title="categorical_crossentropy"></a><strong>categorical_crossentropy</strong></h2><pre><code class="python">categorical_crossentropy(y_true, y_pred)</code></pre><p>源码：</p><pre><code class="python">def categorical_crossentropy(y_true, y_pred):    return K.categorical_crossentropy(y_true, y_pred)</code></pre><p>亦称作多类的对数损失，注意使用该目标函数时，需要将标签转化为形如(nb_samples, nb_classes)的二值序列</p><p>注意: 当使用”categorical_crossentropy”作为目标函数时,标签应该为多类模式,即one-hot编码的向量,而不是单个数值. (即，如果你有10个类，每个样本的目标值应该是一个10维的向量，这个向量除了表示类别的那个索引为1，其他均为0)。可以使用工具中的to_categorical函数完成该转换.示例如下:</p><pre><code class="python">from keras.utils.np_utils import to_categoricalcategorical_labels = to_categorical(int_labels, num_classes=None)</code></pre><h2 id="sparse-categorical-crossentropy"><a href="#sparse-categorical-crossentropy" class="headerlink" title="sparse_categorical_crossentropy"></a><strong>sparse_categorical_crossentropy</strong></h2><p>如上，但接受稀疏标签。注意，使用该函数时仍然需要你的标签与输出值的维度相同，你可能需要在标签数据上增加一个维度：np.expand_dims(y,-1)</p><pre><code class="python">sparse_categorical_crossentropy(y_true, y_pred)</code></pre><p>源码：</p><pre><code class="python">def sparse_categorical_crossentropy(y_true, y_pred):    return K.sparse_categorical_crossentropy(y_true, y_pred)def sparse_categorical_crossentropy(target, output, from_logits=False):    &quot;&quot;&quot;Categorical crossentropy with integer targets.    # Arguments        target: An integer tensor.        output: A tensor resulting from a softmax            (unless `from_logits` is True, in which            case `output` is expected to be the logits).        from_logits: Boolean, whether `output` is the            result of a softmax, or is a tensor of logits.    # Returns        Output tensor.    &quot;&quot;&quot;    # Note: tf.nn.sparse_softmax_cross_entropy_with_logits    # expects logits, Keras expects probabilities.    if not from_logits:        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)        output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)        output = tf.log(output)    output_shape = output.get_shape()    targets = cast(flatten(target), &#39;int64&#39;)    logits = tf.reshape(output, [-1, int(output_shape[-1])])    res = tf.nn.sparse_softmax_cross_entropy_with_logits(        labels=targets,        logits=logits)    if len(output_shape) &gt;= 3:        # if our output includes timestep dimension        # or spatial dimensions we need to reshape        return tf.reshape(res, tf.shape(output)[:-1])    else:        return res</code></pre><h2 id="binary-crossentropy"><a href="#binary-crossentropy" class="headerlink" title="binary_crossentropy"></a><strong>binary_crossentropy</strong></h2><p>（亦称作对数损失，logloss）</p><pre><code class="python">binary_crossentropy(y_true, y_pred)</code></pre><p>源码：</p><pre><code class="python">def binary_crossentropy(y_true, y_pred):    return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)def binary_crossentropy(target, output, from_logits=False):    &quot;&quot;&quot;Binary crossentropy between an output tensor and a target tensor.    # Arguments        target: A tensor with the same shape as `output`.        output: A tensor.        from_logits: Whether `output` is expected to be a logits tensor.            By default, we consider that `output`            encodes a probability distribution.    # Returns        A tensor.    &quot;&quot;&quot;    # Note: tf.nn.sigmoid_cross_entropy_with_logits    # expects logits, Keras expects probabilities.    if not from_logits:        # transform back to logits        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)        output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)        output = tf.log(output / (1 - output))    return tf.nn.sigmoid_cross_entropy_with_logits(labels=target,                                                   logits=output)</code></pre><h2 id="kullback-leibler-divergence"><a href="#kullback-leibler-divergence" class="headerlink" title="kullback_leibler_divergence"></a><strong>kullback_leibler_divergence</strong></h2><p>从预测值概率分布Q到真值概率分布P的信息增益,用以度量两个分布的差异</p><pre><code class="python">kullback_leibler_divergence(y_true, y_pred)</code></pre><p>源码：</p><pre><code class="python">def kullback_leibler_divergence(y_true, y_pred):    y_true = K.clip(y_true, K.epsilon(), 1)    y_pred = K.clip(y_pred, K.epsilon(), 1)    return K.sum(y_true * K.log(y_true / y_pred), axis=-1)</code></pre><h2 id="poisson"><a href="#poisson" class="headerlink" title="poisson"></a><strong>poisson</strong></h2><p>(predictions - targets * log(predictions))的均值</p><pre><code class="python">poisson(y_true, y_pred)</code></pre><p>源码：</p><pre><code class="python">def poisson(y_true, y_pred):    return K.mean(y_pred - y_true * K.log(y_pred + K.epsilon()), axis=-1)</code></pre><p>说明：</p><p><img src="https://www.zhihu.com/equation?tex=L%3D+%5Cfrac+%7B1%7D%7Bn%7D+%5Csum%5En_%7Bi%3D1%7D+%28y_%7Bpred%7D%5E%7B%28i%29%7D+-+y_%7Btrue%7D%5E%7B%28i%29%7D%5Ccdot+log%28y_%7Bpred%7D%5E%7B%28i%29%7D%29%29" alt="[公式]"></p><h2 id="cosine-proximity"><a href="#cosine-proximity" class="headerlink" title="cosine_proximity"></a><strong>cosine_proximity</strong></h2><p>即预测值与真实标签的余弦距离平均值的相反数</p><pre><code class="python">cosine_proximity(y_true, y_pred)</code></pre><p>源码：</p><pre><code class="python">def cosine_proximity(y_true, y_pred):    y_true = K.l2_normalize(y_true, axis=-1)    y_pred = K.l2_normalize(y_pred, axis=-1)    return -K.sum(y_true * y_pred, axis=-1)</code></pre><p>说明：</p><p><img src="https://www.zhihu.com/equation?tex=L%3D+-+%5Cfrac%7B+%5Csum%5En_%7Bi%3D1%7D+y_%7Btrue%7D%5E%7B%28i%29%7D+%5Ccdot+y_%7Bpred%7D%5E%7B%28i%29%7D%7D+%7B%5Csqrt%7B+%5Csum%5En_%7Bi%3D1%7D+%28y_%7Btrue%7D%5E%7B%28i%29%7D%29%5E2%7D+%5Ccdot+%5Csqrt+%7B%5Csum%5En_%7Bi%3D1%7D+%28y_%7Bpred%7D%5E%7B%28i%29%7D%29%5E2%7D%7D" alt="[公式]"></p><p>简写：</p><pre><code class="python">mse = MSE = mean_squared_errormae = MAE = mean_absolute_errormape = MAPE = mean_absolute_percentage_errormsle = MSLE = mean_squared_logarithmic_errorkld = KLD = kullback_leibler_divergencecosine = cosine_proximity</code></pre>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Xrandr displaying “Failed to get size of gamma for output default”</title>
      <link href="2019/07/16/Xrandr-displaying-%E2%80%9CFailed-to-get-size-of-gamma-for-output-default%E2%80%9D/"/>
      <url>2019/07/16/Xrandr-displaying-%E2%80%9CFailed-to-get-size-of-gamma-for-output-default%E2%80%9D/</url>
      
        <content type="html"><![CDATA[<p>Ubuntu 16.04 使用 xrandr 修改分辨率报错：</p><pre><code>Failed to get size of gamma for output default</code></pre><p>解决办法：</p><p>Based on info in this article I’ve found answer.</p><p>You need to add <code>&quot;i915.alpha_support=1&quot;</code> booting parameter to GRUB (Info from here) :</p><p>Log in to the system and start a terminal window (Applications-&gt;Accessories-&gt;Terminal).</p><p>In the terminal window at the $ prompt, enter the command: <code>sudo gedit /etc/default/grub</code></p><p>Enter your password when prompted by [sudo]. (If the file /etc/default/grub appears to be empty or does not exist, see the instructions for earlier releases above).</p><p>In the editor window, use the arrow keys to move the cursor to the line beginning with <code>&quot;GRUB_CMDLINE_LINUX_DEFAULT&quot;</code> then edit that line, adding your parameter(s) to the text inside the double-quotes after the words <code>&quot;quiet splash&quot;</code>. (Be sure to add a SPACE after “splash” before adding your new parameter.)</p><p>it should look like this <code>&quot;quiet splash i915.alpha_support=1&quot;</code></p><p>Click the Save button, then close the editor window.</p><p>In the terminal window at the $ prompt, enter the command: <code>sudo update-grub</code> Restart the system.</p><blockquote><p>Reference: <a href="https://stackoverflow.com/questions/47391669/xrandr-displaying-failed-to-get-size-of-gamma-for-output-default" target="_blank" rel="noopener">stack overflow</a></p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> Ubuntu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>el-button样式</title>
      <link href="2019/07/07/el-button%E6%A0%B7%E5%BC%8F/"/>
      <url>2019/07/07/el-button%E6%A0%B7%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<p>一篇没用的水博客</p><p>做数据库小学期时踩到的小坑</p><p><strong>定义el-button样式，改变focus状态样式</strong></p><p>el-button 采用button实现，在点击之后button处于focus状态。</p><p>修改focus状态样式：</p><p>（需要为el-button设置class名称为btn）</p><pre><code class="css">  .btn:focus {    background: white;    color: #606266;    border-color: #dcdfe6;  }  .btn:active {    background: #edf5ff;    color: #559ffd;    border-color: #559ffd;  }</code></pre><p>需要把这三个颜色属性改成需要的颜色。</p><p>由于focus样式后覆盖active样式，会导致按钮被点击时也会变成上面设置的样式。</p><p>因此需要在focus样式后面在定义active样式，顺序不能颠倒</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>ajax和axios、fetch的区别</title>
      <link href="2019/06/05/ajax%E5%92%8Caxios%E3%80%81fetch%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
      <url>2019/06/05/ajax%E5%92%8Caxios%E3%80%81fetch%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<p><strong>1.jQuery ajax</strong></p><pre><code class="javascript">$.ajax({   type: &#39;POST&#39;,   url: url,   data: data,   dataType: dataType,   success: function () {},   error: function () {}});</code></pre><p>传统 Ajax 指的是 XMLHttpRequest（XHR）， 最早出现的发送后端请求技术，隶属于原始js中，核心使用XMLHttpRequest对象，多个请求之间如果有先后关系的话，就会出现<strong>回调地狱</strong>。<br> JQuery ajax 是对原生XHR的封装，除此以外还增添了对<strong>JSONP</strong>的支持。经过多年的更新维护，真的已经是非常的方便了，优点无需多言；如果是硬要举出几个缺点，那可能只有：<br> 1.本身是针对MVC的编程,不符合现在前端<strong>MVVM</strong>的浪潮<br> 2.基于原生的XHR开发，XHR本身的架构不清晰。<br> 3.JQuery整个项目太大，单纯使用ajax却要引入整个JQuery非常的不合理（采取个性化打包的方案又不能享受CDN服务）<br> 4.不符合关注分离（Separation of Concerns）的原则<br> 5.配置和调用方式非常混乱，而且基于事件的异步模型不友好。<br> <strong>PS:MVVM(Model-View-ViewModel), 源自于经典的 Model–View–Controller（MVC）模式。MVVM 的出现促进了 GUI 前端开发与后端业务逻辑的分离，极大地提高了前端开发效率。MVVM 的核心是 ViewModel 层，它就像是一个中转站（value converter），负责转换 Model 中的数据对象来让数据变得更容易管理和使用，该层向上与视图层进行双向数据绑定，向下与 Model 层通过接口请求进行数据交互，起呈上启下作用。View 层展现的不是 Model 层的数据，而是 ViewModel 的数据，由 ViewModel 负责与 Model 层交互，这就完全解耦了 View 层和 Model 层，这个解耦是至关重要的，它是前后端分离方案实施的最重要一环。</strong></p><p><strong>2.axios</strong></p><pre><code class="javascript">axios({    method: &#39;post&#39;,    url: &#39;/user/12345&#39;,    data: {        firstName: &#39;Fred&#39;,        lastName: &#39;Flintstone&#39;    }}).then(function (response) {    console.log(response);}).catch(function (error) {    console.log(error);});</code></pre><p>Vue2.0之后，尤雨溪推荐大家用axios替换JQuery ajax，想必让axios进入了很多人的目光中。<br> axios 是一个基于Promise 用于浏览器和 nodejs 的 HTTP 客户端，本质上也是对原生XHR的封装，只不过它是Promise的实现版本，符合最新的ES规范，它本身具有以下特征：<br> 1.从浏览器中创建 XMLHttpRequest<br> 2.支持 Promise API<br> 3.客户端支持防止CSRF<br> 4.提供了一些并发请求的接口（重要，方便了很多的操作）<br> 5.从 node.js 创建 http 请求<br> 6.拦截请求和响应<br> 7.转换请求和响应数据<br> 8.取消请求<br> 9.自动转换JSON数据<br> <strong>PS:防止CSRF:就是让你的每个请求都带一个从cookie中拿到的key, 根据浏览器同源策略，假冒的网站是拿不到你cookie中得key的，这样，后台就可以轻松辨别出这个请求是否是用户在假冒网站上的误导输入，从而采取正确的策略。</strong><br> <strong>3.fetch</strong></p><pre><code class="javascript">try {  let response = await fetch(url);  let data = response.json();  console.log(data);} catch(e) {  console.log(&quot;Oops, error&quot;, e);}</code></pre><p>fetch号称是AJAX的替代品，是在ES6出现的，使用了ES6中的promise对象。Fetch是基于promise设计的。Fetch的代码结构比起ajax简单多了，参数有点像jQuery ajax。但是，一定记住<strong>fetch不是ajax的进一步封装，而是原生js，没有使用XMLHttpRequest对象</strong>。<br> fetch的优点：<br> 1.符合关注分离，没有将输入、输出和用事件来跟踪的状态混杂在一个对象里<br> 2.更好更方便的写法<br> 坦白说，上面的理由对我来说完全没有什么说服力，因为不管是Jquery还是Axios都已经帮我们把xhr封装的足够好，使用起来也足够方便，为什么我们还要花费大力气去学习fetch？<br> 我认为fetch的优势主要优势就是：</p><pre><code>1.  语法简洁，更加语义化2.  基于标准 Promise 实现，支持 async/await3.  同构方便，使用 [isomorphic-fetch](https://github.com/matthew-andrews/isomorphic-fetch)4.更加底层，提供的API丰富（request, response）5.脱离了XHR，是ES规范里新的实现方式</code></pre><p>最近在使用fetch的时候，也遇到了不少的问题：<br> fetch是一个低层次的API，你可以把它考虑成原生的XHR，所以使用起来并不是那么舒服，需要进行封装。<br> 例如：</p><pre><code>1）fetch只对网络请求报错，对400，500都当做成功的请求，服务器返回 400，500 错误码时并不会 reject，只有网络错误这些导致请求不能完成时，fetch 才会被 reject。2）fetch默认不会带cookie，需要添加配置项： fetch(url, {credentials: &#39;include&#39;})3）fetch不支持abort，不支持超时控制，使用setTimeout及Promise.reject的实现的超时控制并不能阻止请求过程继续在后台运行，造成了流量的浪费4）fetch没有办法原生监测请求的进度，而XHR可以</code></pre><p><strong>总结：axios既提供了并发的封装，也没有fetch的各种问题，而且体积也较小，当之无愧现在最应该选用的请求的方式。</strong></p>]]></content>
      
      
      
        <tags>
            
            <tag> Web </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go echo后端处理跨域的两种方式</title>
      <link href="2019/05/18/go-echo%E5%90%8E%E7%AB%AF%E5%A4%84%E7%90%86%E8%B7%A8%E5%9F%9F%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F/"/>
      <url>2019/05/18/go-echo%E5%90%8E%E7%AB%AF%E5%A4%84%E7%90%86%E8%B7%A8%E5%9F%9F%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<p>跨域报错：</p><p><img src="http://img.cdn.leonwang.top/image-20190518005108377.png" alt=""></p><blockquote><p>跨域资源共享(CORS)标准新增了一组 HTTP 首部字段，允许服务器声明哪些源站有权限访问哪些资源。另外，规范要求，<code>对那些可能对服务器数据产生副作用的HTTP 请求方法（特别是 GET 以外的 HTTP 请求，或者搭配某些 MIME 类型的 POST 请求）</code>，浏览器必须首先使用 OPTIONS 方法发起一个预检请求（preflight request），从而获知服务端是否允许该跨域请求。服务器确认允许之后，才发起实际的 HTTP 请求。在预检请求的返回中，服务器端也可以通知客户端，是否需要携带身份凭证（包括 Cookies 和 HTTP 认证相关数据）。</p></blockquote><blockquote><p>根据 CORS 的标准，当浏览器发送跨域请求时，如果请求不是GET或者特定POST（Content-Type只能是 application/x-www-form-urlencoded, multipart/form-data 或 text/plain的一种）时，强烈要求浏览器必须先以 OPTIONS 请求方式发送一个预请求(preflight request)，从而获知服务器端对跨源请求所支持 HTTP 方法。</p></blockquote><p>跨域问题一般需要在后台解决会比较好。</p><p>1.第一种方式当然是接受所有的跨域方式：</p><pre><code class="go">func setAccessOriginUrl(c echo.Context) {    c.Response().Header().Set(&quot;Access-Control-Allow-Origin&quot;, &quot;*&quot;)}</code></pre><p>2.第二种接受指定地址的跨域请求：</p><pre><code class="go">func setAccessOriginUrl(c echo.Context) {       c.Response().Header().Set(&quot;Access-Control-Allow-Origin&quot;, Conf.Admin.AccessUrl)}</code></pre><p><a href="https://ningyu1.github.io/site/post/92-cors-ajax/" target="_blank" rel="noopener">跨域踩坑经验总结（内涵：跨域知识科普）</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Fetch API 检测请求是否成功</title>
      <link href="2019/05/08/Fetch-API-%E6%A3%80%E6%B5%8B%E8%AF%B7%E6%B1%82%E6%98%AF%E5%90%A6%E6%88%90%E5%8A%9F/"/>
      <url>2019/05/08/Fetch-API-%E6%A3%80%E6%B5%8B%E8%AF%B7%E6%B1%82%E6%98%AF%E5%90%A6%E6%88%90%E5%8A%9F/</url>
      
        <content type="html"><![CDATA[<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p><a href="https://developer.mozilla.org/zh-CN/docs/Web/API/Fetch_API/Using_Fetch" target="_blank" rel="noopener">MDN Web docs</a>中的解释：</p><blockquote><p>请注意，<code>fetch</code>规范与<code>jQuery.ajax()</code>主要有两种方式的不同，牢记：</p><ul><li>当接收到一个代表错误的 HTTP 状态码时，从 <code>fetch()</code>返回的 Promise <strong>不会被标记为 reject，</strong> 即使该 HTTP 响应的状态码是 404 或 500。相反，它会将 Promise 状态标记为 resolve （但是会将 resolve 的返回值的 <code>ok</code> 属性设置为 false ），仅当网络故障时或请求被阻止时，才会标记为 reject。</li><li>默认情况下，<code>fetch</code> <strong>不会从服务端发送或接收任何 cookies</strong>, 如果站点依赖于用户 session，则会导致未经认证的请求（要发送 cookies，必须设置 <a href="https://developer.mozilla.org/zh-CN/docs/Web/API/GlobalFetch/fetch#参数" target="_blank" rel="noopener">credentials</a> 选项）。自从2017年8月25日后，默认的credentials政策变更为<code>same-origin</code>Firefox也在61.0b13中改变默认值</li></ul></blockquote><p>因为Promise不会被标记为reject，不能使用<code>.catch()</code>来检测请求是否成功（例如响应状态码是404或500）。</p><h1 id="检测请求是否成功的方法"><a href="#检测请求是否成功的方法" class="headerlink" title="检测请求是否成功的方法"></a>检测请求是否成功的方法</h1><p>如果遇到网络故障，<a href="https://developer.mozilla.org/zh-CN/docs/Web/API/GlobalFetch/fetch" target="_blank" rel="noopener"><code>fetch()</code></a> promise 将会 reject，带上一个 <a href="https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/TypeError" target="_blank" rel="noopener"><code>TypeError</code></a> 对象。虽然这个情况经常是遇到了权限问题或类似问题——比如 404 不是一个网络故障。想要精确的判断 <code>fetch()</code> 是否成功，需要包含 promise resolved 的情况，此时再判断 <a href="https://developer.mozilla.org/zh-CN/docs/Web/API/Response/ok" target="_blank" rel="noopener"><code>Response.ok</code></a> 是不是为 true。类似以下代码：</p><pre><code class="js">fetch(&#39;flowers.jpg&#39;).then(function(response) {  if(response.ok) {    return response.blob();  }  throw new Error(&#39;Network response was not ok.&#39;);}).then(function(myBlob) {   var objectURL = URL.createObjectURL(myBlob);   myImage.src = objectURL; }).catch(function(error) {  console.log(&#39;There has been a problem with your fetch operation: &#39;, error.message);});</code></pre>]]></content>
      
      
      <categories>
          
          <category> Archive </category>
          
          <category> Web </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Web </tag>
            
            <tag> JavaScript </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法课笔记</title>
      <link href="2019/03/19/Analysis-and-Design-of-Algorithms/"/>
      <url>2019/03/19/Analysis-and-Design-of-Algorithms/</url>
      
        <content type="html"><![CDATA[<table><thead><tr><th>Date</th><th>Content</th></tr></thead><tbody><tr><td>2019.3.19</td><td>[L1 : L2]</td></tr><tr><td>2019.5.12</td><td>[L3 : L7] (outline)</td></tr><tr><td>2019.6.24</td><td>done</td></tr></tbody></table><p>[TOC]</p><h2 id="L1-绪论"><a href="#L1-绪论" class="headerlink" title="L1 绪论"></a>L1 绪论</h2><p>算法的<strong>定义</strong>：算法是解决问题的一系列明确指令的有穷序列。对于符合一定规范的输入，能够在有限时间内获得要求的输出。</p><p>算法的<strong>定义</strong>2: 算法描述了求解一个可计算问题的计算流程。</p><p>算法的<strong>特点</strong>：输入(&gt;=0个)，输出(&gt;=1个)，确定性，有穷性，可行性</p><blockquote><p>确定性：每一步确定，输入的值域确定</p><p>有穷性：指令数有穷，每条指令的执行次数有穷，每条指令的执行时间有穷</p><p>可行性：能被有效地计算执行，步骤足够简单和基础</p></blockquote><p>关于算法的一些点：</p><ul><li>每一步都是明确的</li><li>一个问题有不同的算法</li><li>一个算法有不同的描述</li><li>不同的算法有不同的思路和执行速度</li></ul><p>本课程中关于<strong>伪代码规范</strong>的说明：</p><ol><li>变量的生命不需要写</li><li>使用缩进表示此类语句的范围，如for、if和while</li><li>使用 &lt;- 表示赋值</li></ol><p>例子：计算最大公约数</p><ol><li><p>欧几里得算法</p><p> <img src="http://img.cdn.leonwang.top/006tKfTcly1g182uv5x15j30js080t9f.jpg" alt=""></p></li><li><p>连续整数检测算法</p><p> 把m，n中小的一个数作为起点t，尝试用两个数去除，如果除不尽，t-1</p><p> 两层if嵌套</p><p> <img src="http://img.cdn.leonwang.top/006tKfTcly1g182vbzz6bj30hs0bbgmc.jpg" alt=""></p></li><li><p>中学时期的计算方法</p><p> 找到m所有的质因数</p><p> 找到n所有的质因数</p><p> 找到所有的公因数，注意如果出现多次，需要重复</p><p> 计算乘积，返回结果</p></li></ol><p><strong>算法 和 程序 的比较</strong>：</p><ul><li><p>相似点：</p><ul><li>指令的有穷序列</li></ul></li><li><p>不同点：</p><ul><li><p>表现形式:</p><p>  算法——自然语言，伪码，流程图</p><p>  程序——用某些特定编程语言编码，可以由一些特定机器执行</p></li><li><p>执行：</p><p>  算法——有限步骤</p><p>  程序——可以无限执行</p><blockquote><p> E.G. 操作系统</p><p>不是算法，而是无限执行的一个程序</p><p>每一个任务都可以根据特定的算法被看成子程序</p></blockquote></li><li><p>确定性</p><p>  算法——逐步解决问题的大纲或流程图</p><p>  程序——基于算法的问题解决方案的编码实现</p></li></ul></li><li><p>算法+数据结构=程序</p></li></ul><p>算法设计与分析的过程：</p><blockquote><p>先设计后分析，分析的是算法的效率等，不是分析问题</p></blockquote><p><img src="http://img.cdn.leonwang.top/image-20190624174737692.png" alt=""></p><p>算法设计策略：</p><p><img src="http://img.cdn.leonwang.top/006tKfTcly1g1832ruixcj30md0fzdhi.jpg" alt=""></p><p><strong>如何分析算法效率</strong>：</p><ul><li>现在的算法有多好？<ul><li>时间效率</li><li>空间效率</li></ul></li><li>是否还有更好的算法？<ul><li>下界</li><li>最优性</li></ul></li></ul><p>重要的问题类型：</p><p><img src="http://img.cdn.leonwang.top/006tKfTcly1g1835v7zm1j30lq0g2ab9.jpg" alt=""></p><h2 id="L2-算法效率分析基础"><a href="#L2-算法效率分析基础" class="headerlink" title="L2 算法效率分析基础"></a>L2 算法效率分析基础</h2><p>本章重点：</p><ul><li>算法的效率包括时间效率与空间效率</li><li>时间效率用输入规模的函数来度量，该函数的计算主要关注算法基本操作的执行次数。</li><li>增长阶数</li><li>最好、最坏、平均效率</li><li>渐进符号表示</li><li>递归与非递归算法的效率分析</li></ul><hr><p>with respect to <strong>input size</strong>, <strong>input type</strong>, and <strong>algorithm function</strong> </p><p>C=F(N, I, A) </p><ul><li><p>时间效率（时间复杂度）</p><p>  Time efficiency T(N, I): </p><p>  how fast an algorithm runs. </p></li><li><p>空间效率（空间复杂度）</p><p>  Space efficiency S(N , I):<br>  the space an algorithm requires.</p></li></ul><h3 id="分析框架"><a href="#分析框架" class="headerlink" title="分析框架"></a>分析框架</h3><p>对于输入规模为n的算法，统计它的<strong>基本操作执行次数</strong>，来对其<strong>时间效率</strong>进行度量；统计它<strong>消耗的额外存储单元</strong>的数量对其<strong>空间效率</strong>进行度量。</p><p>主要关心一个算法基本操作次数的增长次数，并把它作为算法效率的主要指标。</p><ul><li><p>输入规模的度量</p><p>  效率被定义为输入规模的函数</p></li><li><p>运行时间的度量</p><p>  基本操作(Basic Operation)：算法中最重要的操作。通常是算法最内层循环中最费时的操作。<strong>最内层循环中，且每层循环中都会执行的操作。</strong>如if、while等结构中的关键操作是条件部分的操作。</p><p>  <img src="http://img.cdn.leonwang.top/006tKfTcly1g186wwbh0bj30lr08cdgb.jpg" alt=""></p><p>  本身是一个近似结果。</p><p>  对于大规模的输入，忽略乘法常量，仅关注执行次数的增长次数。</p></li><li><p>增长次数</p><p>  <img src="http://img.cdn.leonwang.top/006tKfTcly1g1871x6l1fj30s6098wf5.jpg" alt=""></p><p>  $$<br>  \log _{a} n=\log _{a} b \log _{b} n<br>  $$<br>  由于上面这个公式允许只要在对数部分以外增加一个乘法常量就能实现对数换底，<strong>因此我们忽略对数的底，简写成 log <em>n</em> 。</strong></p><p>  一个需要指数级操作的算法只能用来解决规模非常小的问题。</p></li><li><p>算法的最优、最差和平均效率</p><p>  Tbest(n)   Tworst(n)   Tavg(n)</p></li></ul><p>例子：顺序查找：求平均效率时，自己假设搜索成功的概率是p</p><p>最优/最差 -&gt; 基本操作的执行次数C(n)最小/最大</p><p><strong>平均效率，指的是随机情况，不是最好和最坏的平均</strong><br>$$<br>T_{a v g}(N)=\sum_{I \in D_{N}} P(I) T(N, I)=\sum_{I \in D_{N}} P(I) \sum_{i=1}^{k} t_{i} e_{i}(N, I)<br>$$</p><blockquote><p>I 是指输入的类型，不同的类型有不同的概率</p><p>比如顺序搜索问题中，分为 可查找成功的输入 和 会查找失败的输入</p></blockquote><h3 id="渐进符号和基本效率类型"><a href="#渐进符号和基本效率类型" class="headerlink" title="渐进符号和基本效率类型"></a>渐进符号和基本效率类型</h3><p>为了对增长次数进行比较和归类</p><ul><li><p>渐进复杂度</p><p>  <img src="http://img.cdn.leonwang.top/006tKfTcly1g1882q7xrwj30nw0ev0u3.jpg" alt=""></p></li><li><p>渐进符号</p><p>  <strong>定义</strong>：</p><p>  <img src="http://img.cdn.leonwang.top/006tKfTcly1g18842zv0bj30px0a8gmi.jpg" alt=""><br>$$<br>  \Theta(g(n))=O(g(n)) \cap \Omega(g(n))<br>$$<br>&lt;=和&gt;=去掉等号，渐进符号用对应的小写字母表示：</p></li></ul><p><img src="http://img.cdn.leonwang.top/006tKfTcly1g1886j0kmoj30pd0f2gns.jpg" alt=""></p><p>一些<strong>性质</strong>：</p><p><img src="http://img.cdn.leonwang.top/006tKfTcly1g189socljjj30nm0gf765.jpg" alt=""></p><p>定理：</p><p><img src="http://img.cdn.leonwang.top/006tKfTcly1g189wfq3irj30k302x74f.jpg" alt=""></p><ul><li><p>利用<strong>极限</strong>比较增长次数：</p><p>  <img src="http://img.cdn.leonwang.top/006tKfTcly1g18aenp8gkj30rf09igmi.jpg" alt=""></p><p>  可以利用洛必达法则和史特林公式计算极限：</p><p>  <img src="http://img.cdn.leonwang.top/006tKfTcly1g18alg6z57j31340ac3zh.jpg" alt=""></p><p>  比较关系：<br>  $$<br>  1 &lt; logn &lt; n &lt; nlogn &lt; n^a &lt; a^n &lt;n! &lt; n^n<br>  $$<br>  求基本操作的增长次数的方法总结：</p><ol><li>计算极限（注意运用洛必达和史特林）</li><li>利用性质</li><li>利用$O, \Omega, \theta$ 的定义</li></ol></li></ul><h3 id="非递归算法的数学分析"><a href="#非递归算法的数学分析" class="headerlink" title="非递归算法的数学分析"></a>非递归算法的数学分析</h3><p>通用方案：</p><ol><li>决定用哪个（哪些）参数表示<strong>输入规模</strong>。</li><li>找出算法的<strong>基本操作</strong>（作为一个规律，它总是位于算法的最内层循环中）。</li><li><strong>检查</strong>基本操作的执行次数<strong>是否</strong>只依赖于输入规模。如果它还依赖于一些其他的特性，则最差效率、平均效率以及最优效率（如有必要）<strong>需要分别研究</strong>。</li><li>建立一个算法基本操作执行次数C(n)的<strong>求和表达式</strong>。</li><li>利用求和运算的标准公式和法则来建立一个操作次数的闭合公式，或者至少确定它的<strong>增长次数</strong>。</li></ol><p><em>（附录A：求和公式和法则）</em></p><p>例：</p><ol><li><p>求最大元素</p><p> O(n)</p></li><li><p>元素唯一性问题</p><ul><li>两重循环算法：$C_{\text {worst}}(n)\in\Theta\left(n^{2}\right)$</li><li>先排序后比较算法：$\Theta(n \log n)$</li></ul></li><li><p>n阶矩阵乘法</p><p> O(n^3)</p></li><li><p>求十进制整数在二进制表示中二进制数字的个数</p><p> 约是 $\log _{2} n$</p><blockquote><p>循环变量以不同的方式变化（循环变量在循环体内改变），换一种研究方式</p></blockquote></li></ol><h3 id="递归算法的数学分析"><a href="#递归算法的数学分析" class="headerlink" title="递归算法的数学分析"></a>递归算法的数学分析</h3><p>通用方案：</p><ol><li>决定用哪个（哪些）参数表示<strong>输入规模</strong>。</li><li>找出算法的<strong>基本操作</strong>（作为一个规律，它总是位于算法的最内层循环中）。</li><li><strong>检查</strong>基本操作的执行次数是否只依赖于输入规模。如果它还依赖于一些其他的特性，则最差效率、平均效率以及最优效率（如有必要）需要分别研究。</li><li>对于算法基本操作的执行次数，建立一个<strong>递推关系</strong>以及相应的<strong>初始条件</strong>。</li><li>解这个递推式，或者至少确定它的<strong>解的增长次数</strong>。</li></ol><p><em>（附录B：求解递推关系的方法。前向替代、反向替代、二阶常系数线性递推式）</em></p><p>例：</p><ol><li><p>阶乘 theta (n)</p></li><li><p>汉诺塔 theta (2^n)</p></li><li><p>求十进制整数在二进制表示中二进制数字的个数 theta (logn)</p><p> 技巧：let $n=2^{k}$</p><p> <img src="http://img.cdn.leonwang.top/image-20190624204726135.png" alt=""></p></li><li><p>斐波那契数列</p></li></ol><h2 id="L3-蛮力法"><a href="#L3-蛮力法" class="headerlink" title="L3 蛮力法"></a>L3 蛮力法</h2><h3 id="选择排序"><a href="#选择排序" class="headerlink" title="选择排序"></a>选择排序</h3><p><img src="http://img.cdn.leonwang.top/image-20190624213812371.png" alt=""></p><p><img src="http://img.cdn.leonwang.top/image-20190624213828340.png" alt=""></p><h3 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h3><p>!<img src="http://img.cdn.leonwang.top/image-20190624214000246.png" alt=""></p><p><img src="http://img.cdn.leonwang.top/image-20190624214017677.png" alt=""></p><h3 id="蛮力字符串匹配"><a href="#蛮力字符串匹配" class="headerlink" title="蛮力字符串匹配"></a><del>蛮力字符串匹配</del></h3><h3 id="最近对"><a href="#最近对" class="headerlink" title="最近对"></a><del>最近对</del></h3><h3 id="蛮力多项式求值"><a href="#蛮力多项式求值" class="headerlink" title="蛮力多项式求值"></a><del>蛮力多项式求值</del></h3><h3 id="穷举查找"><a href="#穷举查找" class="headerlink" title="穷举查找"></a>穷举查找</h3><p>通常涉及到组合对象，如组合、排列、子集问题</p><p>许多问题是最优化问题，如路径长度、分配成本</p><p>方法：</p><ol><li>系统地列出所有潜在解决方案</li><li>逐个评估解决方案</li><li>最终得到目标解</li></ol><p>例子：</p><ol><li><p>旅行商问题</p><p> 确定一个出发城市，旅行一遍所有城市，最后回到出发城市的最短路线</p><p> 思路：加权连通图，寻找最短的哈密顿回路</p><p> 本质：寻找最优化的<strong>排列</strong>问题（排列中一共有n+1个元素，但首尾两个元素是确定的）</p><p> 分析： 排列总数：$(n-1)!$</p></li><li><p>0-1背包问题</p><p> 给定了总空间，每个物品的体积和价值</p><p> 本质：寻找最优化的子集问题（所有物品的子集）</p><p> 分析： n个元素的子集数 $2^n$</p><blockquote><p>背包问题和旅行商问题都是 NP困难 问题</p></blockquote></li><li><p>分配问题</p><p> n个工作分配给n个人，一一对应，最小化成本</p><p> 每一个一个可行方案是一个排列，其中第i个元素表示分给第i个人的工作</p><p> 分析： 排列总数 $n!$</p><p> 对于问题解空间随着实例规模呈指数增长的问题，没有已知的多项式时间解法</p></li></ol><h3 id="深度优先搜索（图）"><a href="#深度优先搜索（图）" class="headerlink" title="深度优先搜索（图）"></a>深度优先搜索（图）</h3><p>使用<strong>栈</strong>来记录</p><p>构造深度优先搜索森林</p><p>算法：（递归）</p><p><img src="http://img.cdn.leonwang.top/image-20190625092820854.png" alt=""></p><p>应用：</p><ul><li><p>检查图形的连接性</p></li><li><p>检查图的非循环性</p></li><li><p>找出有向图的强连通分量（极大强连通子图）</p></li><li><p>找出无向图的关节点</p><blockquote><p>删除顶点V以及V相关的边后，图的一个连通分量分割为两个或两个以上的连通分量</p></blockquote></li><li><p>有向图拓扑排序</p></li><li><p>边的分类</p></li><li><p>验证无向图是否连通</p></li></ul><h3 id="广度优先搜索（图）"><a href="#广度优先搜索（图）" class="headerlink" title="广度优先搜索（图）"></a>广度优先搜索（图）</h3><p><strong>队列</strong></p><p>广度优先搜索森林</p><p>交叉边连接BFS树相同或相邻级别上的顶点</p><p>算法（非递归）：两层循环，while + foreach</p><p><img src="http://img.cdn.leonwang.top/image-20190625094327936.png" alt=""></p><p>应用：</p><ul><li>检查图形的连接性</li><li>检查图的非循环性</li><li>在两个给定顶点之间找到一条边数最少的路径</li></ul><p>BFS 和 DFS 的效率：</p><p>两者效率相同</p><p>邻接矩阵：$\Theta\left(|V|^{2}\right)$</p><p>邻接链表：$\Theta(|V|+|E|)$</p><h3 id="总结：蛮力法的优点和缺点"><a href="#总结：蛮力法的优点和缺点" class="headerlink" title="总结：蛮力法的优点和缺点"></a>总结：蛮力法的优点和缺点</h3><p><img src="http://img.cdn.leonwang.top/006tNc79ly1g2ybjaucsdj30wc0lqjyl.jpg" alt=""></p><p>简单、适用范围广、对一些重要问题有合理的算法</p><p>效率低、有些问题没有有效的算法、从设计角度缺少建设性</p><h2 id="L4-递归算法"><a href="#L4-递归算法" class="headerlink" title="L4 递归算法"></a>L4 递归算法</h2><p>递归：方法自己调用自己。</p><p>循环都可以改写成递归，递归未必能改写成循环</p><h3 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h3><ol><li><p>阶乘</p><p> 初始条件：n=0 -&gt; f(0) = 1 -&gt; C(0)=0<br> $$<br> n !=\left\{\begin{array}{cc}{1} &amp; {n=0} \ {n(n-1) !} &amp; {n&gt;0}\end{array}\right.<br> $$</p><p> $$<br> \begin{array}{l}{C(0)=0} \ {C(n)=C(n-1)+1}\end{array}<br> $$</p><p> $$<br> \begin{aligned} C(n) &amp;=C(n-1)+1 \ &amp;=[C(n-2)+1]+1=C(n-2)+2 \ &amp;=[C(n-3)+1]+2=C(n-3)+3 \ &amp; \ldots \ldots \ &amp;=[C(n-n)+1]+n-1=n \end{aligned}<br> $$</p></li><li><p>斐波那契数列<br> $$<br> F(n)=\left\{\begin{array}{rl}{1} &amp; {n=0} \ {1} &amp; {n=1} \ {F(n-1)+F(n-2)} &amp; {n&gt;1}\end{array}\right.\\<br> \begin{array}{c}{F(n)=\frac{1}{\sqrt{5}}\left(\frac{1+\sqrt{5}}{2}\right)^{n}-\frac{1}{\sqrt{5}}\left(\frac{1-\sqrt{5}}{2}\right)^{n}} \ {C(n) \in \Theta\left(\phi^{n}\right)}\end{array}<br> $$</p></li><li><p>Ackerman 函数<br> $$<br> \left\{\begin{array}{cc}{A(1,0)=2} \ {A(0, m)=1} &amp; {m \geq 0} \ {A(n, 0)=n+2} &amp; {n \geq 2} \ {A(n, m)=A(A(n-1, m), m-1)} &amp; {n, m \geq 1}\end{array}\right.<br> $$</p><p> $$<br> \begin{aligned} \boldsymbol{E} \boldsymbol{g} . &amp; A(0, y)=y+1 \ &amp; A(1, y)=y+2 \ A(2, y) &amp;=2 y+3 \ A(3, y) &amp;=2^{y+3}-3 \ A(4, y) &amp;=\frac{2^{2}}{y+3}-3 \end{aligned}<br> $$</p></li><li><p><strong>汉诺塔</strong></p><p> <img src="http://img.cdn.leonwang.top/image-20190625101110798.png" alt=""></p><p> 基本操作：moving one disk<br> $$<br> \begin{array}{l}{C(1)=1} \ {C(n)=2 C(n-1)+1=2^{n}-1} \ {C(n) \in \Theta\left(2^{n}\right)}\end{array}<br> $$<br> <img src="http://img.cdn.leonwang.top/image-20190625101332727.png" alt=""></p></li><li><p>排列问题</p></li></ol><h3 id="重要的递推类型（附录B-3）"><a href="#重要的递推类型（附录B-3）" class="headerlink" title="重要的递推类型（附录B.3）"></a>重要的递推类型（附录B.3）</h3><ol><li><p>减一法（属于减治法）<br> $$<br> T(n)=T(n-1)+f(n)<br> $$<br> $f(n)$是把问题规模减一后花费的时间</p></li><li><p>减常因子（也是减治法）<br> $$<br> T(n)=T(n / b)+f(n)<br> $$</p><p> ​            例：折半查找、用平方求幂、俄式乘法、假币问题</p><p> 推广：（分治法的递推式）    </p><p> $$<br> T(n)=a T(n / b)+f(n)<br> $$<br> 上式适合<strong>主定理</strong></p><p> <img src="http://img.cdn.leonwang.top/image-20190625102641764.png" alt=""></p></li></ol><h2 id="L5-分治法"><a href="#L5-分治法" class="headerlink" title="L5 分治法"></a>L5 分治法</h2><p>分析时，可以直接用主定理</p><p>也可以用递归算法中递推公式求C(n)，但是需要首先另 n=b^k，然后再换回来</p><p><img src="http://img.cdn.leonwang.top/006tNc79ly1g2yay5whe5j319w0r2grz.jpg" alt=""></p><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><ol><li><p>概念</p><ul><li>子问题需要满足的要求：<ul><li>相同类型</li><li>独立</li><li>规模均匀</li></ul></li></ul></li><li><p><strong>步骤</strong></p><ol><li>Divide</li><li>Conquer</li><li><p>Combine</p><blockquote><p>具体到下面的四个问题：具体的三个步骤</p></blockquote></li></ol></li><li><p>算法</p><p> <img src="http://img.cdn.leonwang.top/006tNc79ly1g2ybv20ac2j30my09c794.jpg" alt=""></p></li><li><p>算法分析</p><p> $$<br> T(n)=\left\{\begin{array}{cc}{O(1)} &amp; {n=1} \ {a T(n / b)+f(n)} &amp; {n&gt;1}\end{array}\right.<br> $$<br> 要求：$n=b^{k}$</p><blockquote><p>如果不满足这个要求，需要用平滑定理，不考</p></blockquote></li></ol><blockquote><p>注意主定理的使用</p></blockquote><h3 id="大整数乘法"><a href="#大整数乘法" class="headerlink" title="大整数乘法"></a>大整数乘法</h3><p>蛮力：$n^2$</p><ul><li><p>第一种分治算法</p></li><li><p><img src="http://img.cdn.leonwang.top/image-20190625115557551.png" alt=""></p><p>  基本操作：一位乘<br>  $$<br>  T(n)=\left\{\begin{array}{cl}{O(1)} &amp; {n=1} \ {4 T(n / 2)+O(n)} &amp; {n&gt;1}\end{array}\right.<br>  $$</p><p>  $$<br>  T(n)=O\left(n^{2}\right)<br>  $$</p><p>  相比于蛮力，没有提升</p><p>  求C(n)的方法：1. 递推公式 2. 直接用主定理</p></li><li><p>第二种分治算法（相当于对第一种分治进一步优化）</p><p>  上面的方法需要4次$(n/2)$规模的的乘法，可以再减到3次</p><p>  把中间的 (AD+BC)拆开<br>  $$<br>  (A D+B C)=(A+B) <em>(C+D)-A^{</em>} C-B^{*} D<br>  $$</p><p>  $$<br>  T(n)=\left\{\begin{array}{cc}{O(1)} &amp; {n=1} \ {3 T(n / 2)+O(n)} &amp; {n&gt;1}\end{array}\right.<br>  $$</p><p>  $$<br>  T(n)=O\left(n^{\log 2^{3}}\right) \approx O\left(n^{1.585}\right)<br>  $$</p><p>  一般认为$O\left(n^{\log ^{3}}\right) $即可，比蛮力好</p><blockquote><p>最终的，这个思想导致了快速傅利叶变换(Fast Fourier Transform)的产生。 该方法也可以看作是一个复杂的分治算法。</p></blockquote></li></ul><h3 id="Strassen-矩阵乘法"><a href="#Strassen-矩阵乘法" class="headerlink" title="Strassen 矩阵乘法"></a>Strassen 矩阵乘法</h3><p>蛮力：n^3</p><ul><li><p>第一种分治<br>  $$<br>  \left[\begin{array}{ll}{C_{11}} &amp; {C_{12}} \ {C_{21}} &amp; {C_{22}}\end{array}\right]=\left[\begin{array}{ll}{A_{11}} &amp; {A_{12}} \ {A_{21}} &amp; {A_{22}}\end{array}\right]\left[\begin{array}{ll}{B_{11}} &amp; {B_{12}} \ {B_{21}} &amp; {B_{22}}\end{array}\right]\\<br>  =\left[\begin{array}{ll}{A_{11} B_{11}+A_{12} B_{21}} &amp; {A_{11} B_{12}+A_{12} B_{22}} \ {A_{21} B_{11}+A_{22} B_{21}} &amp; {A_{21} B_{12}+A_{22} B_{22}}\end{array}\right]<br>  $$</p><p>  $$<br>  T(n)=\left\{\begin{array}{cc}{O(1)} &amp; {n=1} \ {8 T(n / 2)+O\left(n^{2}\right)} &amp; {n&gt;1}\end{array}\right.<br>  $$</p><p>  $$<br>  T(n)=O\left(n^{3}\right)<br>  $$</p><p>  相比蛮力，没有提升</p></li><li><p>第二种分治（对第一种分治的优化）</p><p>  减少$(n/2)$规模矩阵乘法的次数，8次变成7次</p><p>  <img src="http://img.cdn.leonwang.top/image-20190625121722808.png" alt=""><br>  $$<br>  \begin{array}{l}{T(n)=7 T(n / 2)} \ {T(1)=1}\end{array}<br>  $$</p><p>  $$<br>  T(n)=n^{\log _{2} 7} \approx n^{2.807}<br>  $$</p><p>  比蛮力好</p></li></ul><h3 id="二分查找（折半查找）"><a href="#二分查找（折半查找）" class="headerlink" title="二分查找（折半查找）"></a>二分查找（折半查找）</h3><p>首先要求有序</p><p>迭代形式：</p><p><img src="http://img.cdn.leonwang.top/image-20190625132118248.png" alt=""></p><p>递归形式：</p><p><img src="http://img.cdn.leonwang.top/image-20190625132156766.png" alt=""></p><p>找到/找不到 -&gt; 最好、最坏、平均</p><p>最坏：<br>$$<br>\begin{array}{l}{C_{w}(n)=C_{w}(\lfloor n / 2\rfloor)+1} \ {C_{w}(1)=1}\end{array}<br>$$</p><p>$$<br>C_{w}(n)=\Theta(\log n)<br>$$</p><p>最好：<br>$$<br>c_{b}(n)=1<br>$$<br>平均：</p><p>以二叉搜索树的角度分析，k层，一共$n=2^{k}-1$个元素<br>$$<br>A(n)=\frac{1}{n} \sum_{i=1}^{k} i 2^{i-1} \approx \log (n+1)-1<br>$$<br>按层计算，第 i 层元素需要的比较次数时 i 次，第 i 层一共$2^(i-1)$个元素，最后求和平均</p><p>思考：变三分？变四分？是否越多越好？</p><h3 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h3><p>先分后合</p><p>分到每个数组只有一个元素</p><p>合并时需要三个指针<br>$$<br>C(n)=\Theta(n \log n)<br>$$</p><h3 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h3><p>划分位pivot的选取：</p><ol><li>随机选取</li><li>最简单的策略：选择第一个元素</li></ol><p>方法：</p><ol><li><p>单向扫描</p></li><li><p>双向扫描</p><p> 双向扫描最后是把pivot元素与j指针指向的元素交换位置</p><p> 最后i，j可能交叉了，可能在一个位置</p></li></ol><p>划分位选的好不好 -&gt; 最优、最坏</p><p>最好的情况$\Theta(n \log n)$：是pivot把问题分成了两个规模相同的子问题<br>$$<br>\begin{array}{l}{c_{b}(n)=2 C_{b}(n / 2)+\Theta(n)} \ {c_{b}(1)=0}\end{array}<br>$$<br>最坏的情况$\theta\left(n^{2}\right)$：递归从减常因子退化成了减一<br>$$<br>C_{m}(n)=C_{w}(n-1)+\Theta(n)<br>$$<br>平均：$O(n \log n)$</p><p>改进：pivot采用随机选取</p><h3 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h3><h2 id="L6-减治法"><a href="#L6-减治法" class="headerlink" title="L6 减治法"></a>L6 减治法</h2><h3 id="三种变体"><a href="#三种变体" class="headerlink" title="三种变体"></a>三种变体</h3><ol><li><p>减常量（一般是1）</p><p> 例：求阶乘</p><p> 与蛮力法不是对立的</p></li><li><p>减常因子</p><p> 例：用平方求幂</p><p> 与分治法不是完全对立的</p></li><li><p>减变量</p><p> 例：欧几里得算法求最大公因数</p></li></ol><h3 id="减常量法"><a href="#减常量法" class="headerlink" title="减常量法"></a>减常量法</h3><p>本章主要对第一类减治法，即减常量法/减一法，进行了讲解</p><h4 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a>插入排序</h4><p>三种方式：</p><ol><li>从左向右扫描，找到位置插入</li><li>从右向左扫描，找到位置插入</li><li>进行折半查找，找到位置插入</li></ol><p>迭代实现（从右向左扫描插入）：</p><p><img src="http://img.cdn.leonwang.top/image-20190625143750806.png" alt=""></p><p>最坏：$C_{\text {worst}}(n)=\sum_{i=1}^{n-1} \sum_{j=0}^{i-1} 1=\sum_{i=1}^{n-1} i=\frac{(n-1) n}{2} \in \theta\left(n^{2}\right)$</p><p>最好：$C_{b e s t}(n)=\sum_{i=1}^{n-1} 1=n-1 \in \theta(n)$</p><p>平均：$C_{a v g}(n) \approx \frac{n^{2}}{4} \in \theta\left(n^{2}\right)$</p><h4 id="图的遍历"><a href="#图的遍历" class="headerlink" title="图的遍历"></a>图的遍历</h4><ol><li>深度优先搜索<ul><li>回溯法</li></ul></li><li>广度优先搜索<ul><li>分支限界法</li></ul></li></ol><p>算法：</p><ol><li>拓扑排序</li><li>回溯法</li><li>分治限界法</li></ol><h5 id="拓扑排序"><a href="#拓扑排序" class="headerlink" title="拓扑排序"></a>拓扑排序</h5><p>方法：</p><ol><li>基于DFS算法</li><li>移除源节点法</li></ol><h4 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h4><ul><li><p>搜索算法</p></li><li><p>搜索空间的三种表示</p></li><li><p>搜索效率的思考：随即搜索</p></li><li><p><strong>回溯法</strong></p><ul><li><p>系统性+跳跃性</p></li><li><p>方法概述</p></li><li><p>解空间和解空间树</p></li><li><p>基本思想</p></li><li><p><strong>基本步骤</strong></p><p>  定义解空间树后深搜，及时剪枝，直至完成全部搜索</p><p>  <img src="http://img.cdn.leonwang.top/image-20190625155332358.png" alt=""></p></li><li><p>常见解空间树</p><ol><li>子集树 0-1背包问题</li><li>排列树 TSP问题</li></ol></li><li><p>举例：</p><ul><li><p>背包问题</p><ul><li>约束函数即使剪枝</li><li>限界函数可以减去右子树</li></ul></li><li><p>TSP问题</p><p>  最坏情况下 $O(n!)$</p><blockquote><p>上述两个问题中，边表示选择，树中节点表示的都是某个状态，而不是某个物品或某个城市</p></blockquote></li></ul></li><li><p>剪枝函数</p><ol><li>约束函数 -&gt; 减去不满足约束的子树</li><li>限界函数 -&gt; 减去得不到最优解的子树</li></ol></li><li><p>时空效率分析</p></li></ul></li><li><p>分支限界法</p><ul><li><p>基本思想</p><p>  广度优先或最小耗费/最大效益优先的方式</p></li><li><p>与回溯法的区别</p><p>  <img src="http://img.cdn.leonwang.top/image-20190625152306167.png" alt=""></p></li><li><p><strong>求解步骤</strong></p><p>  定义解空间树后，维护队列直至找到最优解</p><p>  <img src="http://img.cdn.leonwang.top/image-20190625155252201.png" alt=""></p></li><li><p>常见的分支限界法：</p><ul><li><p>队列式（广度优先方式）</p><p>  队列式分支限界法<strong>类似于广度优先</strong>的遍历算法，但<strong>不搜索</strong>以<strong>不可行</strong>结点为根的子树</p></li><li><p>优先队列（最优优先方式）</p><p>  最大堆/最小堆</p></li></ul></li><li><p>举例：背包问题（有上界函数），TSP问题</p></li></ul></li></ul><h2 id="L7-变治法"><a href="#L7-变治法" class="headerlink" title="L7 变治法"></a>L7 变治法</h2><h3 id="三种类型"><a href="#三种类型" class="headerlink" title="三种类型"></a>三种类型</h3><ol><li>实例化简 -&gt; 变成同一个问题的更简单的实例 -&gt; 变简单</li><li>改变表现 -&gt; 变成同一实例的不同表现形式 -&gt; 变形式</li><li>问题转化 -&gt; 变成另一个问题的实例 -&gt; 变问题</li></ol><h3 id="实例化简"><a href="#实例化简" class="headerlink" title="实例化简"></a>实例化简</h3><h4 id="预排序"><a href="#预排序" class="headerlink" title="预排序"></a>预排序</h4><p>许多问题，先对其进行排序，能够让问题化简</p><p>适用于：</p><ol><li>搜索问题</li><li>求中位数/出现最多的数/…（选择问题）</li><li>元素唯一性</li></ol><h4 id="高斯消去法"><a href="#高斯消去法" class="headerlink" title="高斯消去法"></a>高斯消去法</h4><p>问题：求解有任意系数矩阵的一个n元n个线性方程组成的方程组</p><p>步骤</p><ol><li><p>通过初等变换系数矩阵变换成上三角形式 $\Theta\left(n^{3}\right)$</p><p> <img src="http://img.cdn.leonwang.top/image-20190625164946095.png" alt=""></p><p> 初等变换：交换位置/乘倍数/加减</p></li><li><p>从下向上进行替换，把变量逐个求解出来 $\Theta\left(n^{2}\right)$</p></li></ol><p>避免两个问题：</p><ol><li>避免主元是0</li><li>避免主元太小（趋向于0）</li></ol><p>应用：</p><ul><li>LU 矩阵分解</li><li>计算矩阵的逆</li><li>计算行列式</li></ul><h4 id="堆和堆排序"><a href="#堆和堆排序" class="headerlink" title="堆和堆排序"></a>堆和堆排序</h4><p>堆适合实现<strong>优先级</strong>队列</p><p>每个元素有一个优先级</p><p>数据结构：<strong>完全二叉树</strong></p><p>上下层节点之间数据大小有要求，同层的左右节点的大小没有要求</p><p>堆的性质：…</p><p>区别层数和高度</p><p><img src="http://img.cdn.leonwang.top/image-20190625170849743.png" alt=""></p><p>构造堆：</p><ol><li><p>自下而上</p><p> 建立完全二叉树，插入，堆化</p><p> 递归的伪代码 分析（基本操作，复杂度）<br> $$<br> C_{\text {worst}}(n)=\sum_{i=0}^{h-1} \sum_{\text { nodest leveli }} 2(h-i)=\sum_{i=0}^{h-1} 2(h-i) 2^{i}=2\left(n-\log _{2}(n+1)\right)<br> $$<br> 属于 O(n)</p></li><li><p>自上而下</p><p> 插入，堆化。此方法插入<strong>一个</strong>结点的效率是<br> $$<br> O\left(\log _{2} n\right)<br> $$</p></li></ol><p>删除根节点$<br>\Theta(\log n)<br>$：不是真的删除，而是与最右叶子节点交换，然后堆化</p><p>堆排序：1. 自下而上构造堆 2. 进行删除根节点操作 </p><p>构造堆是O(n)，删除n-1次根节点需要O(nlogn)，所以总的复杂度是$O\left(n\log _{2} n\right)$</p><p>堆排序比快排慢，和归并排序相当</p><h3 id="改变表现"><a href="#改变表现" class="headerlink" title="改变表现"></a>改变表现</h3><h4 id="霍纳法则"><a href="#霍纳法则" class="headerlink" title="霍纳法则"></a>霍纳法则</h4><p>计算多项式的值</p><p>蛮力方法：大到小/小到大</p><p>霍纳方法：拆开</p><p>$$<br>p(x)=\left(\ldots\left(a_{n} x+a_{n-1}\right) x+\ldots\right) x+a_{0}<br>$$</p><h3 id="问题转化"><a href="#问题转化" class="headerlink" title="问题转化"></a>问题转化</h3><p>思路：把问题变成另一个问题</p><h4 id="求最小公倍数"><a href="#求最小公倍数" class="headerlink" title="求最小公倍数"></a>求最小公倍数</h4><p>num1 * num2 / 最大公约数</p><h4 id="三点共线"><a href="#三点共线" class="headerlink" title="三点共线"></a>三点共线</h4><p>几何问题 &lt;=&gt; 判断行列式的符号问题<br>$$<br>\operatorname{det}\left|\begin{array}{lll}{x_{1}} &amp; {y_{1}} &amp; {1} \ {x_{2}} &amp; {y_{2}} &amp; {1} \ {x_{3}} &amp; {y_{3}} &amp; {1}\end{array}\right|=x_{1} y_{2}+x_{2} y_{3}+x_{3} y_{1}-x_{3} y_{2}-x_{2} y_{1}-x_{1} y_{3}<br>$$<br>值是正数 &lt;=&gt; 点3在直线12的左侧</p><h4 id="线性规划"><a href="#线性规划" class="headerlink" title="线性规划"></a>线性规划</h4><p>最优决策问题 &lt;=&gt; 线性规划问题</p><ul><li>方法：单纯形法…</li><li>问题<ul><li>投资问题</li><li>背包问题 连续/离散</li></ul></li></ul><h4 id="化简成图的问题（时空状态图）"><a href="#化简成图的问题（时空状态图）" class="headerlink" title="化简成图的问题（时空状态图）"></a>化简成图的问题（时空状态图）</h4><p>问题的解决方案 &lt;=&gt; 时空状态图中从初始状态结点到目标状态结点的路径</p><ul><li>过河问题（人狼羊菜）</li></ul><h2 id="L8-动态规划"><a href="#L8-动态规划" class="headerlink" title="L8 动态规划"></a>L8 动态规划</h2><h3 id="概念-1"><a href="#概念-1" class="headerlink" title="概念"></a>概念</h3><p>求解以<strong>时间</strong>划分<strong>阶段</strong>的动态过程的优过程的<strong>优化</strong>问题</p><p>阶段之间是<strong>相互联系</strong>的</p><p>一些与时间无关的静态规划(如线性规划、非线性规划)，可以人为地引进时间因素，把它视为多阶段决策过程，也可 以用动态规划方法方便地求解。</p><p>最优性原理：求解问题的一个<strong>最优策略序列</strong>的<strong>子策略序列</strong>总是<strong>最优</strong>的，则称该问题满足最优性原理。</p><p>动态规划和分治的比较：</p><p>​        适合不同类型的问题</p><p>​        - 分治法：子问题独立</p><p>​        - 动态规划：子问题重叠</p><p><img src="http://img.cdn.leonwang.top/image-20190625193156028.png" alt=""></p><p>步骤：</p><p><img src="http://img.cdn.leonwang.top/image-20190625193653679.png" alt=""></p><blockquote><p>如果问题只需要最优值，不需要最优解，则第四步可以省略</p></blockquote><p>适合动态规划法的<strong>问题的性质</strong>：</p><ol><li><p>最优子结构（能描述清楚）</p><p> 问题的最优解由子问题的最优解来构造</p><p> 证明问题：反证法</p></li><li><p>重叠子问题</p><p> 子问题之间重叠。如果用递归自顶向下求解，会导致问题重复求解。</p></li></ol><p>Fibonacci问题的形式：</p><ol><li>自顶向下（ 备忘录memory[] ）</li><li>自底向上</li><li>进一步压缩空间</li></ol><p>应用：</p><p>​    计算二项式系数</p><p>​    计算最长公共子序列</p><p>​    计算最短公共超序列</p><p>​    传递闭包的Warshall算法</p><p>​    所有对最短路径的Floyd算法</p><p>​    一些困难的离散优化问题实例：如背包</p><h3 id="求二项式系数"><a href="#求二项式系数" class="headerlink" title="求二项式系数"></a>求二项式系数</h3><p>用二维表<br>$$<br>\begin{aligned} C(n, k) &amp;=C(n-1, k-1)+C(n-1, k), \text { for } n&gt;k&gt;0 \ C(n, 0) &amp;=C(n, n)=1 \end{aligned}<br>$$</p><p>$$<br>A(n, k)\in \theta(n k)<br>$$</p><h3 id="矩阵链相乘"><a href="#矩阵链相乘" class="headerlink" title="矩阵链相乘"></a>矩阵链相乘</h3><p>二维表</p><h3 id="0-1背包问题"><a href="#0-1背包问题" class="headerlink" title="0-1背包问题"></a>0-1背包问题</h3><p>递推表达形式 填表 算法分析 倒过来分析  <del>算法改进不讲</del></p><p>V[n, W]表示对于n个物品，容量W的0-1背包问题的最优解的值<br>$$<br>\begin{array}{c}{V[0, j]=0 \quad \text { for } j \geq 0 ; \qquad V[i, 0]=0 \text { for } i \geq 0} \ {\qquad V(i, j)=\left\{\begin{array}{cc}{\max \left\{V(i-1, j), V\left(i-1, j-w_{i}\right)+v_{i}\right\}} &amp; {j \geq w_{i}} \ {V(i-1, j)} &amp; {0 \leq j&lt;w_{i}}\end{array}\right.}\end{array}<br>$$</p><h3 id="多阶段决策问题"><a href="#多阶段决策问题" class="headerlink" title="多阶段决策问题"></a>多阶段决策问题</h3><p>向前 代价的递推关系式和初始关系式 结果 解</p><p>向后 </p><p>求最优决策序列</p><p>多段图问题：求s到t的最小成本路径</p><h3 id="最短路径问题"><a href="#最短路径问题" class="headerlink" title="最短路径问题"></a>最短路径问题</h3><p>warshall算法 邻接矩阵 <strong>传递闭包（写中间过程）</strong> <strong>能写算法</strong></p><p>floyd算法 权重/距离矩阵 和上个算法相似 也要写求解过程</p><h2 id="L9-贪心"><a href="#L9-贪心" class="headerlink" title="L9 贪心"></a>L9 贪心</h2><p>找零问题</p><p>局部最优-&gt; 全局最优</p><p>每一步的选择：</p><ol><li>可行性</li><li>局部最优解</li><li>不可撤回</li></ol><p>可以通过贪心获得<strong>最优</strong>解的问题：</p><ol><li>一部分找零问题</li><li>最小生成树问题</li><li>单源最短路径问题</li><li>哈夫曼编码</li></ol><p>可以通过贪心获得<strong>近似</strong>解的问题：</p><ol><li>TSP问题</li><li>背包问题</li><li>其他最优化问题</li></ol><p>活动安排（了解）</p><p>贪心算法的基本要素：</p><ul><li>贪心选择性质——证明每一步的贪心选择最终导致整体最优解</li><li>最优子结构性质</li></ul><p>动态规划和贪心的异同</p><p><img src="http://img.cdn.leonwang.top/image-20190625222440809.png" alt=""></p><p><img src="http://img.cdn.leonwang.top/image-20190625222727500.png" alt=""></p><p>贪心问题的特征：解是<strong>子集</strong></p><p>贪心方法：要根据某种度量标准进行排序</p><p>贪心也要最优子结构</p><ul><li><p>背包</p><p>  贪心不能求0-1背包的最优解</p><p>  效率：主要是排序，上界是$nlogn$</p></li><li><p>单源最短路径</p><ul><li>Dijkstra <strong>填表</strong></li></ul></li><li><p>最小生成树</p><ul><li>prim 填表 画树<ul><li>和Dijkstra的比较</li></ul></li><li>kruskal 填表 画树</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 算法与数据结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>UML和模式应用 Note</title>
      <link href="2019/03/16/UML%E5%92%8C%E6%A8%A1%E5%BC%8F%E5%BA%94%E7%94%A8-Note/"/>
      <url>2019/03/16/UML%E5%92%8C%E6%A8%A1%E5%BC%8F%E5%BA%94%E7%94%A8-Note/</url>
      
        <content type="html"><![CDATA[<table><thead><tr><th>Date</th><th>Content</th></tr></thead><tbody><tr><td>2019-3-16</td><td>[Chapter 1 : Chapter 5]</td></tr></tbody></table><p>本书的内容是在OOA/D（面向对象的分析和设计）和相关需求分析的语境下，对UML应用、模型和迭代过程的全景揭示。</p><h2 id="一、绪论"><a href="#一、绪论" class="headerlink" title="一、绪论"></a>一、绪论</h2><h3 id="面向对象分析和设计"><a href="#面向对象分析和设计" class="headerlink" title="面向对象分析和设计"></a>面向对象分析和设计</h3><p><img src="http://img.cdn.leonwang.top/006tKfTcly1g14f3dbeupj30p101xdg1.jpg" alt=""></p><p>在<strong>需求分析</strong>中进行用例的编写。</p><p>在OO开发中，至关重要的能力是熟练地为软件<strong>对象</strong>分配<strong>职责</strong>。</p><p>GRASP在本书中指对象设计恶化指责分配的9项基本原则。</p><p>分析强调的是对问题和需求的调查研究，而不是解决方案；设计强调的是满足需求的概念上的解决方案，而不是其实现。</p><p>步骤：定义用例-&gt;定义领域模型-&gt;定义交互图-&gt;定义设计类图</p><p>领域模型表示的是真实世界的类，设计类图表示的是软件类。</p><p>UML的核心是可视化、图形化表示。</p><p>应用UML的三种透视图：</p><p><img src="http://img.cdn.leonwang.top/006tKfTcly1g14iqe8drpj30qa0cvjsz.jpg" alt="image-20190316122316601"></p><p>概念透视图表示领域模型，后两种都表示设计类图。</p><p>UML中的矩形框被称为类，在不同的模型中类表示不同的含义。概念类、软件类、实现类。</p><h3 id="迭代、进化和建模"><a href="#迭代、进化和建模" class="headerlink" title="迭代、进化和建模"></a>迭代、进化和建模</h3><p><strong>瀑布模型</strong>提倡在编程之前预先完成需求和设计步骤，项目失败率高；而<strong>迭代和进化式开发</strong>对部分系统及早地引入编程和测试，并重复这一循环，项目失败率低。</p><p>软件开发过程：构造、部署以及维护软件的方式。</p><p>统一过程（UP）：一种流行的<strong>面向对象</strong>系统的<strong>迭代</strong>软件开发过程。</p><p><img src="http://img.cdn.leonwang.top/006tKfTcly1g14jsfzaqxj30of04ygmg.jpg" alt=""></p><p><img src="http://img.cdn.leonwang.top/006tKfTcly1g14k3t8fzrj30nr0aeq3o.jpg" alt=""></p><p><img src="http://img.cdn.leonwang.top/006tKfTcly1g14k41sxezj30or07d3zk.jpg" alt=""></p><p>一次迭代的持续时间是2～6周。小步骤、快速反馈和调整。</p><p>迭代的一个关键思想是时间定量。不能拖延。如果无法完成，应从本次迭代中除去部分任务和需求，分配在将来的迭代中，而不是推迟完成日期。</p><p>不要让瀑布思维侵蚀迭代或UP项目。</p><p>UP提倡风险驱动和客户驱动相结合的迭代计划。意味着早期的迭代目标要能识别和降低最高风险，并能构造客户最关心的可视化特性。</p><p>敏捷开发（agile development）：<img src="http://img.cdn.leonwang.top/006tKfTcly1g14kn3an9mj30np022wep.jpg" alt=""></p><p>UP的关键实践：</p><p><img src="http://img.cdn.leonwang.top/006tKfTcly1g14kzfyy6bj30or09a0ts.jpg" alt=""></p><p>UP的四个阶段：</p><p><img src="http://img.cdn.leonwang.top/006tKfTcly1g14kztzmhvj30p0041wey.jpg" alt=""></p><p>科目：discipline。一个主题域中的一组活动及相关制品。</p><p>制品：artifact。所有工作产品。</p><p>UP的科目：</p><p><img src="http://img.cdn.leonwang.top/006tKfTcly1g14l5p9erzj30pu0ek3zz.jpg" alt=""></p><p>注意UP的阶段和科目的关系：每个阶段都会设计几乎所有的科目，只是科目的工作量不同。</p><h3 id="案例研究"><a href="#案例研究" class="headerlink" title="案例研究"></a>案例研究</h3><h2 id="二、初始阶段"><a href="#二、初始阶段" class="headerlink" title="二、初始阶段"></a>二、初始阶段</h2><h3 id="初始不是需求阶段"><a href="#初始不是需求阶段" class="headerlink" title="初始不是需求阶段"></a>初始不是需求阶段</h3><p>初始阶段的目标不是定义所有需求</p><p>初始阶段的制品：</p><p><img src="http://img.cdn.leonwang.top/006tKfTcly1g14lsahp3oj30o209xwfy.jpg" alt=""></p><h3 id="进化式需求"><a href="#进化式需求" class="headerlink" title="进化式需求"></a>进化式需求</h3><h3 id="用例"><a href="#用例" class="headerlink" title="用例"></a>用例</h3><h3 id="其他需求"><a href="#其他需求" class="headerlink" title="其他需求"></a>其他需求</h3>]]></content>
      
      
      <categories>
          
          <category> Archive </category>
          
          <category> UML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> UML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>React Note</title>
      <link href="2019/03/16/React-Note/"/>
      <url>2019/03/16/React-Note/</url>
      
        <content type="html"><![CDATA[<p>修改历史：</p><table><thead><tr><th>Date</th><th>Content</th></tr></thead><tbody><tr><td>2019-3-14</td><td>[:this的绑定]</td></tr></tbody></table><p>React 是一个使用 JavaScript 和 XML 技术（可选）构建可组合用户界面的引擎。</p><p>JSX语法扩展，可以直接在代码中书写XML（以生成HTML）。</p><hr><p>单页应用程序</p><p>概念：</p><p>只有一张Web页面的应用，是加载单个HTML 页面并在用户与应用程序交互时动态更新该页面的Web应用程序。避免即使仅有一点不同还要发送一整页新的页面的情况。</p><p>方式：</p><ol><li>传统的数据绑定技术。在可维护性、可扩展性和性能上都有一些缺陷。</li><li>响应式渲染。用一种声明的方式定义组件的外观和行为。概念上重新渲染整个页面，实际上使用了一种内存中的虚拟DOM。</li></ol><hr><p>面向组件开发</p><p>纯JavaScript</p><hr><p>一个React组件就是一个带有render方法，并且返回组件UI描述的JavaScript类。</p><hr><pre><code class="html">&lt;script src=&quot;https://cdn.staticfile.org/react/16.4.0/umd/react.development.js&quot;&gt;&lt;/script&gt;&lt;script src=&quot;https://cdn.staticfile.org/react-dom/16.4.0/umd/react-dom.development.js&quot;&gt;&lt;/script&gt;&lt;!-- 生产环境中不建议使用 --&gt;&lt;script src=&quot;https://cdn.staticfile.org/babel-standalone/6.26.0/babel.min.js&quot;&gt;&lt;/script&gt;</code></pre><ul><li><strong>react.min.js</strong> - React 的核心库</li><li><strong>react-dom.min.js</strong> - 提供与 DOM 相关的功能</li><li><strong>babel.min.js</strong> - Babel 可以将 ES6 代码转为 ES5 代码，这样我们就能在目前不支持 ES6 浏览器上执行 React 代码。Babel 内嵌了对 JSX 的支持。通过将 Babel 和 babel-sublime 包（package）一同使用可以让源码的语法渲染上升到一个全新的水平。</li></ul><hr><p>组件可以用函数定义，也可以用类定义（es6）</p><hr><p>对于事件处理，需要注意的一点就是this的绑定，其他跟普通Dom绑定监听事件一样，this的绑定有以下几种方式：</p><ol><li>在构造函数中使用bind绑定this</li><li>在调用的时候使用bind绑定this</li><li>在调用的时候使用箭头函数绑定this</li><li>使用属性初始化器语法绑定this</li></ol><hr><p><img src="http://img.cdn.leonwang.top/react.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Archive </category>
          
          <category> Web </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Web </tag>
            
            <tag> React </tag>
            
            <tag> 前端 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ARCore踩坑——无法放置3D模型</title>
      <link href="2019/02/14/ARCore%E8%B8%A9%E5%9D%91%E2%80%94%E2%80%94%E6%97%A0%E6%B3%95%E6%94%BE%E7%BD%AE3D%E6%A8%A1%E5%9E%8B/"/>
      <url>2019/02/14/ARCore%E8%B8%A9%E5%9D%91%E2%80%94%E2%80%94%E6%97%A0%E6%B3%95%E6%94%BE%E7%BD%AE3D%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h1><p>按照 Google ARCore 官方文档实践 Sceneform 项目，实现 hellosceneform 示例项目的效果。</p><p>要在项目中使用 Sceneform 和 ARCore，需要执行以下操作：</p><ol><li><a href="https://developers.google.com/ar/develop/java/sceneform/?hl=zh-cn#configure-project" target="_blank" rel="noopener">配置您项目的 <code>build.gradle</code> 文件</a></li><li><a href="https://developers.google.com/ar/develop/java/sceneform/?hl=zh-cn#manifest" target="_blank" rel="noopener">更新您的 <code>AndroidManifest.xml</code></a></li><li><a href="https://developers.google.com/ar/develop/java/sceneform/?hl=zh-cn#scene-view" target="_blank" rel="noopener">执行运行时检查并创建场景图</a></li><li><a href="https://developers.google.com/ar/develop/java/sceneform/?hl=zh-cn#renderables" target="_blank" rel="noopener">创建可渲染对象</a></li><li><a href="https://developers.google.com/ar/develop/java/sceneform/?hl=zh-cn#scene" target="_blank" rel="noopener">构建场景</a></li></ol><p>按照文档说明进行操作。在 Android Studio 中安装 <code>Google Sceneform Tools (Beta)</code> 插件，手动修改 <code>build.gradle</code> 和<code>AndroidManifest.xml</code>相关配置，导入3D asset，java和xml布局直接复制自 hellosceneform 示例。</p><h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>运行发现点击屏幕并不能在相应的位置放置3D模型，除此之外其他功能均正常。</p><h1 id="调试"><a href="#调试" class="headerlink" title="调试"></a>调试</h1><p>首先在负责响应点击事件的代码部分添加 Log 代码。</p><p> <img src="http://img.cdn.leonwang.top/006tKfTcly1g05vipar3nj319a0luada.jpg" alt=""></p><p>其中第一个参数 TAG 定义为<code>private static final String TAG = MainActivity.class.getSimpleName();</code>，即类的名字<code>MainActivity</code>。</p><p>重新运行，发现在点击屏幕后有以下 log 信息（用MainActivity进行过滤）：</p><p><img src="http://img.cdn.leonwang.top/006tKfTcly1g05visza9aj324u0eu7b2.jpg" alt=""></p><p>可以知道 ModelRenderable 对象为 null。</p><p>以 Debug 模式运行，查看 console 输出：</p><p><img src="http://img.cdn.leonwang.top/006tKfTcly1g05vivpwtrj325o0ssnb2.jpg" alt=""></p><p>其中关键语句：<code>Rendercore bundle (.rcb) version not supported, max version supported is 0.52.X. Version requested for loading is 0.54.1</code></p><p>基本可以确定问题是模型版本过高，渲染依赖不支持。</p><p>通过Google该问题得到以下信息：</p><p><img src="http://img.cdn.leonwang.top/006tKfTcly1g05vizf44gj30u01qdqbr.jpg" alt=""></p><p>是官方github项目仓库中的一个issue，已经有人汇报过这个问题并得到了解决。</p><p>进一步确定了该问题的原因：3D asset是通过最新的<code>Google Sceneform Tools (Beta)</code> 插件产生，而在<code>build.gradle</code>中添加的依赖<code>implementation &#39;com.google.ar.sceneform.ux:sceneform-ux:1.5.0&#39;</code>版本较低，无法渲染版本较高的3D asset。</p><h1 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h1><p><img src="http://img.cdn.leonwang.top/006tKfTcly1g05vj3420sj30v802awem.jpg" alt=""></p><p>把<code>build.gradle</code>中依赖的版本由1.5.0改为最新的1.6.0</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>依赖信息是按照 Google 的官方文档添加，Google ARCore 相关文档的更新没有及时跟上 sdk 的更新速度。</p>]]></content>
      
      
      <categories>
          
          <category> Archive </category>
          
          <category> ARCore </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ARCore </tag>
            
            <tag> AR </tag>
            
            <tag> Android </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git本地分支与远程分支</title>
      <link href="2019/02/02/Git%E6%9C%AC%E5%9C%B0%E5%88%86%E6%94%AF%E4%B8%8E%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF/"/>
      <url>2019/02/02/Git%E6%9C%AC%E5%9C%B0%E5%88%86%E6%94%AF%E4%B8%8E%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF/</url>
      
        <content type="html"><![CDATA[<p><strong>把远程仓库中已经有的dev分支同步到本地：</strong></p><p>git checkout -b dev 新建并切换到本地dev分支</p><p>git pull origin dev 本地分支与远程分支相关联</p><p><strong>在本地新建dev分支并推送到远程：</strong></p><p>git checkout -b dev</p><p>git push origin dev:dev   这样远程仓库中也就创建了一个dev分支。冒号左边是本地分支，右边是远程分支。</p><hr><h2 id="分支相关操作"><a href="#分支相关操作" class="headerlink" title="分支相关操作"></a>分支相关操作</h2><ol><li>克隆代码</li></ol><pre><code class="bash">git clone https://github.com/master-dev.git  # 这个git路径是无效的，示例而已</code></pre><ol start="2"><li>查看所有分支</li></ol><pre><code class="bash">git branch --all  # 默认只有master分支，所以会看到如下两个分支# master[本地主分支] origin/master[远程主分支]# 新克隆下来的代码默认master和origin/master是关联的，也就是他们的代码保持同步</code></pre><ol start="3"><li>创建本地新的dev分支</li></ol><pre><code class="bash">git branch dev  # 创建本地分支git branch  # 查看分支# 这是会看到master和dev，而且master上会有一个星号# 这个时候dev是一个本地分支，远程仓库不知道它的存在# 本地分支可以不同步到远程仓库，我们可以在dev开发，然后merge到master，使用master同步代码，当然也可以同步</code></pre><ol start="4"><li>发布dev分支</li></ol><p>发布dev分支指的是同步dev分支的代码到远程服务器</p><pre><code class="bash">git push origin dev:dev  # 这样远程仓库也有一个dev分支了</code></pre><ol start="5"><li>在dev分支开发代码</li></ol><pre><code class="bash">git checkout dev  # 切换到dev分支进行开发# 开发代码之后，我们有两个选择# 第一个：如果功能开发完成了，可以合并主分支git checkout master  # 切换到主分支git merge dev  # 把dev分支的更改和master合并git push  # 提交主分支代码远程git checkout dev  # 切换到dev远程分支git push  # 提交dev分支到远程# 第二个：如果功能没有完成，可以直接推送git push  # 提交到dev远程分支# 注意：在分支切换之前最好先commit全部的改变，除非你真的知道自己在做什么</code></pre><ol start="6"><li>删除分支</li></ol><p>删除远程dev分支，危险命令哦</p><pre><code class="bash">git push origin :dev</code></pre><p>删除本地dev分支</p><pre><code class="bash">git checkout master  # 切换到master分支git branch -d dev  # 删除本地dev分</code></pre>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
            <tag> GitHub </tag>
            
            <tag> 分支 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ViewPager与ViewPagerTransforms</title>
      <link href="2019/01/27/ViewPager%E4%B8%8EViewPagerTransforms/"/>
      <url>2019/01/27/ViewPager%E4%B8%8EViewPagerTransforms/</url>
      
        <content type="html"><![CDATA[<p>ViewPager 是 Android 官方支持的一个类，可以实现视图的切换，用于实现页面导航、图片轮播等功能，并支持自定义 ViewPager 的切换动画，自行实现翻页动画效果。</p><p>ViewPagerTransforms 是 github 上一个实现了 Viewpager 多种切换动画的开源库，提供了一些现成的PageTransformer 来实现一些动画，同样支持在此基础上进行自定义动画。</p><h2 id="ViewPager"><a href="#ViewPager" class="headerlink" title="ViewPager"></a>ViewPager</h2><p><a href="https://developer.android.com/reference/android/support/v4/view/ViewPager#setpagetransformer" target="_blank" rel="noopener">官方文档</a></p><p><a href="https://www.runoob.com/w3cnote/android-tutorial-viewpager.html" target="_blank" rel="noopener">简介</a></p><p><a href="https://www.jianshu.com/p/e5abbda4a71c" target="_blank" rel="noopener">优秀文章</a></p><h3 id="ViewPager的简介和作用"><a href="#ViewPager的简介和作用" class="headerlink" title="ViewPager的简介和作用"></a>ViewPager的简介和作用</h3><p> ViewPager是android扩展包v4包中的类，这个类可以让用户左右切换当前的view<br> 1）ViewPager类直接继承了ViewGroup类，所有它是一个容器类，可以在其中添加其他的view类。<br> 2）ViewPager类需要一个PagerAdapter适配器类给它提供数据。<br> 3）ViewPager经常和Fragment一起使用，并且提供了专门的FragmentPagerAdapter和FragmentStatePagerAdapter类供Fragment中的ViewPager使用。</p><h3 id="ViewPager的适配器"><a href="#ViewPager的适配器" class="headerlink" title="ViewPager的适配器"></a>ViewPager的适配器</h3><p> 简介中提到了PagerAdapter，和ListView等控件使用一样,需要ViewPager设置PagerAdapter来完成页面和数据的绑定，这个PagerAdapter是一个基类适配器，我们经常用它来实现app引导图，它的子类有FragmentPagerAdapter和FragmentStatePagerAdapter,这两个子类适配器用于和Fragment一起使用，在安卓应用中它们就像listview一样出现的频繁。</p><h3 id="ViewPager的翻页动画"><a href="#ViewPager的翻页动画" class="headerlink" title="ViewPager的翻页动画"></a>ViewPager的翻页动画</h3><p> 为ViewPager设置适配器后，就可以正常使用了，接下来我们为ViewPager增加翻页动画，毕竟人的审美会疲劳，加上一些动画交互会提高不少逼格~~，ViewPager提供了<strong>PageTransformer</strong>接口用于实现翻页动画。<br> 官方提供了PageTransformer的实现例子。</p><pre><code class="java">public class DepthPageTransformer implements ViewPager.PageTransformer {  private static final float MIN_SCALE = 0.75f;  public void transformPage(View view, float position) {      Log.d(&quot;DepthPageTransformer&quot;, view.getTag() + &quot; , &quot; + position + &quot;&quot;);      int pageWidth = view.getWidth();      if (position &lt; -1) { // [-Infinity,-1)          // This page is way off-screen to the left.          view.setAlpha(0);      } else if (position &lt;= 0) { // [-1,0]          // Use the default slide transition when moving to the left page          view.setAlpha(1);          view.setTranslationX(0);          view.setScaleX(1);          view.setScaleY(1);      } else if (position &lt;= 1) { // (0,1]          // Fade the page out.          view.setAlpha(1 - position);          // Counteract the default slide transition          view.setTranslationX(pageWidth * -position);          // Scale the page down (between MIN_SCALE and 1)          float scaleFactor = MIN_SCALE                  + (1 - MIN_SCALE) * (1 - Math.abs(position));          view.setScaleX(scaleFactor);          view.setScaleY(scaleFactor);      } else { // (1,+Infinity]          // This page is way off-screen to the right.          view.setAlpha(0);      }  }}</code></pre><pre><code class="java">public class ZoomOutPageTransformer implements ViewPager.PageTransformer {  private static final float MIN_SCALE = 0.85f;  private static final float MIN_ALPHA = 0.5f;  @SuppressLint(&quot;NewApi&quot;)  public void transformPage(View view, float position) {      int pageWidth = view.getWidth();      int pageHeight = view.getHeight();      Log.e(&quot;TAG&quot;, view + &quot; , &quot; + position + &quot;&quot;);      if (position &lt; -1) { // [-Infinity,-1)          // This page is way off-screen to the left.          view.setAlpha(0);      } else if (position &lt;= 1)       { // [-1,1]          // Modify the default slide transition to shrink the page as well          float scaleFactor = Math.max(MIN_SCALE, 1 - Math.abs(position));          float vertMargin = pageHeight * (1 - scaleFactor) / 2;          float horzMargin = pageWidth * (1 - scaleFactor) / 2;          if (position &lt; 0) {              view.setTranslationX(horzMargin - vertMargin / 2);          } else {              view.setTranslationX(-horzMargin + vertMargin / 2);          }          // Scale the page down (between MIN_SCALE and 1)          view.setScaleX(scaleFactor);          view.setScaleY(scaleFactor);          // Fade the page relative to its size.          view.setAlpha(MIN_ALPHA + (scaleFactor - MIN_SCALE)                  / (1 - MIN_SCALE) * (1 - MIN_ALPHA));      } else { // (1,+Infinity]          // This page is way off-screen to the right.          view.setAlpha(0);      }  }}</code></pre><p>实现翻页动画的关键就是重写transformPage方法，方法里有两个参数view和position,理解这两个参数非常重要。假设有三个页面view1，view2，view3从左至右在viewPager中显示</p><ul><li>往左滑动时：view1，view2，view3的position都是不断变小的。<br>   view1的position: 0 → -1 → 负无穷大<br>   view2的position: 1 → 0 → -1<br>   view3的position: 1 → 0</li><li>往右滑动时：view1，view2，view3的position都是不断变大的。<br>   view1的position: -1 → 0<br>   view2的position: -1 → 0 → 1<br>   view3的position: 0 → 1→ 正无穷大<br>   当position是正负无穷大时view就离开屏幕视野了。因此最核心的控制逻辑是在[-1,0]和(0,1]这两个区间，通过设置透明度，平移，旋转，缩放等动画组合可以实现各式各样的页面变化效果。</li></ul><h2 id="ViewPagerTransforms"><a href="#ViewPagerTransforms" class="headerlink" title="ViewPagerTransforms"></a>ViewPagerTransforms</h2><h3 id="GitHub"><a href="#GitHub" class="headerlink" title="GitHub"></a>GitHub</h3><p>Github: <a href="https://github.com/ToxicBakery/ViewPagerTransforms" target="_blank" rel="noopener">https://github.com/ToxicBakery/ViewPagerTransforms</a></p><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p><strong>ViewPagerTransforms</strong>库的代码结构很简单，干干净净的只有各种PageTransformer：</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fzkxqmbey0j30pm0w813a.jpg" alt="image-20190127103046569"></p><p>有了<strong>ViewPagerTransforms</strong>提供的各种例子，即使不满足它的这些效果想自己实现更多更复杂的效果也会容易一些。</p><p>不过<strong>ViewPagerTransforms</strong>因为使用了view的属性设置方法，所以并不支持2.x低版本，但是这个问题可以使用nineoldandroids解决。参见：<a href="http://jcodecraeer.com/a/anzhuokaifa/androidkaifa/2014/0815/1651.html" target="_blank" rel="noopener">如何在低api中使用View的属性设置方法如setAlpha等</a></p><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><ul><li><p>添加依赖</p><p>  在内部 build.gradle 的 dependencies 中添加</p><pre><code>  implementation &#39;com.ToxicBakery.viewpager.transforms:view-pager-transforms:1.2.32@aar&#39;</code></pre></li><li><p>使用</p><p>  引入 transforms 包：</p><pre><code class="java">  import com.ToxicBakery.viewpager.transforms.*;</code></pre><p>  设置 transform 效果：</p><pre><code class="java">  viewPager.setPageTransformer(true, new ZoomOutTranformer());</code></pre><p>  完整 Activity 示例：</p><p>  <code>`</code>java<br>  package se.tongji.transformtest;</p><p>  import android.support.v7.app.AppCompatActivity;<br>  import android.os.Bundle;<br>  import java.util.ArrayList;<br>  import java.util.List;<br>  import android.support.v4.view.PagerAdapter;<br>  import android.support.v4.view.ViewPager;<br>  import android.view.LayoutInflater;<br>  import android.view.View;<br>  import android.view.ViewGroup;</p><p>  import com.ToxicBakery.viewpager.transforms.*;</p><p>  public class MainActivity extends AppCompatActivity {</p><pre><code>  private View view1, view2, view3;  private ViewPager viewPager;  //对应的viewPager  private List&lt;View&gt; viewList;//view数组</code></pre></li></ul><pre><code>    @Override    protected void onCreate(Bundle savedInstanceState) {        super.onCreate(savedInstanceState);        setContentView(R.layout.activity_main);        // 使用layout填充View        viewPager = (ViewPager) findViewById(R.id.viewpager);        LayoutInflater inflater=getLayoutInflater();        view1 = inflater.inflate(R.layout.layout1, null);        view2 = inflater.inflate(R.layout.layout2,null);        view3 = inflater.inflate(R.layout.layout3, null);        viewList = new ArrayList&lt;View&gt;();// 将要分页显示的View装入数组中        viewList.add(view1);        viewList.add(view2);        viewList.add(view3);        PagerAdapter pagerAdapter = new PagerAdapter() {            @Override            public boolean isViewFromObject(View arg0, Object arg1) {                // TODO Auto-generated method stub                return arg0 == arg1;            }            @Override            public int getCount() {                // TODO Auto-generated method stub                return viewList.size();            }            @Override            public void destroyItem(ViewGroup container, int position,                                    Object object) {                // TODO Auto-generated method stub                container.removeView(viewList.get(position));            }            @Override            public Object instantiateItem(ViewGroup container, int position) {                // TODO Auto-generated method stub                container.addView(viewList.get(position));                return viewList.get(position);            }        };        viewPager.setAdapter(pagerAdapter);        viewPager.setPageTransformer(true, new ZoomOutTranformer());    }}```</code></pre><h3 id="Custom"><a href="#Custom" class="headerlink" title="Custom"></a>Custom</h3><h4 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h4><p>All ViewPagerTransform implementations extend <a href="https://github.com/ToxicBakery/ViewPagerTransforms/blob/master/library/src/main/java/com/ToxicBakery/viewpager/transforms/ABaseTransformer.java" target="_blank" rel="noopener">ABaseTransformer</a> providing useful hooks improving readability of animations and basic functionality important when switching between animations. <a href="https://github.com/ToxicBakery/ViewPagerTransforms/blob/master/library/src/main/java/com/ToxicBakery/viewpager/transforms/ABaseTransformer.java" target="_blank" rel="noopener">ABaseTransformer</a> provides three lifecycle hooks and two flags for default handling of hiding offscreen fragments and mimicking the default paging functionality of the ViewPager.</p><ul><li><a href="https://github.com/ToxicBakery/ViewPagerTransforms/blob/master/library/src/main/java/com/ToxicBakery/viewpager/transforms/ABaseTransformer.java#L85" target="_blank" rel="noopener">onPreTransform(View view, float position)</a></li><li>Default implementation resets the animation state of the fragment to defaults that will place it on the screen if its position permits.</li><li><a href="https://github.com/ToxicBakery/ViewPagerTransforms/blob/master/library/src/main/java/com/ToxicBakery/viewpager/transforms/ABaseTransformer.java#L33" target="_blank" rel="noopener">onTransform(View view, float position)</a></li><li>Animations should perform all or most of their work inside this callback.</li><li><a href="https://github.com/ToxicBakery/ViewPagerTransforms/blob/master/library/src/main/java/com/ToxicBakery/viewpager/transforms/ABaseTransformer.java#L116" target="_blank" rel="noopener">onPostTransform(View view, float position)</a></li><li>Default implementation does nothing. This provides a logical location for any additional work to be done that is not directly related to the animation.</li></ul><h4 id="example"><a href="#example" class="headerlink" title="example"></a>example</h4><p>extends TransformAdapter to custom view animations,for example:</p><pre><code class="java">public class ZoomBothTransform extends TransformAdapter {    @Override    public void onRightScorlling(View view, float position) {        view.setScaleX(1 - position / 2);        view.setScaleY(1 - position / 2);    }    @Override    public void onLeftScorlling(View view, float position) {        view.setScaleX(1 + position / 2);        view.setScaleY(1 + position / 2);    }}</code></pre><p>TransformAdapter has 4 can override method</p><ul><li>onRightScorlling</li></ul><pre><code class="java">/**    * @param view     right view    * @param position right to center 1-&gt;0    *                 center to right 0-&gt;1    */   public void onRightScorlling(View view, float position) {   }</code></pre><ul><li>onLeftScorlling</li></ul><pre><code class="java">/**     * @param view     left view     * @param position left to center  -1-&gt;0     *                 center to left  0-&gt;-1     */    public void onLeftScorlling(View view, float position) {    }</code></pre><ul><li>onCenterIdle</li></ul><pre><code class="java">public void onCenterIdle(View view) {    }</code></pre><ul><li>onTransform</li></ul><pre><code class="java">/**     *     * @param view left and right view both callback     * @param position [-1,1]     */    public void onTransform(View view, float position) {    }</code></pre>]]></content>
      
      
      <categories>
          
          <category> Archive </category>
          
          <category> Android </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Android </tag>
            
            <tag> ViewPager </tag>
            
            <tag> ViewPagerTransforms </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Awesome Solutions</title>
      <link href="2018/12/14/Awesome-Solutions/"/>
      <url>2018/12/14/Awesome-Solutions/</url>
      
        <content type="html"><![CDATA[<h1 id="Awesome-Solutions"><a href="#Awesome-Solutions" class="headerlink" title="Awesome Solutions"></a>Awesome Solutions</h1><ul><li><del><a href="https://www.jianshu.com/p/40cee8ab3d0f" target="_blank" rel="noopener">解决MacBook外接2k显示器出现的字小和字虚的问题</a></del></li><li><a href="https://sspai.com/post/57549" target="_blank" rel="noopener">为 macOS 10.15 开启 HiDPI，让 2K 显示器更舒适</a></li><li><a href="https://github.com/xzhih/one-key-hidpi/blob/master/README-zh.md" target="_blank" rel="noopener">MacOS10.15外接2K显示器开启hidpi调整分辨率</a></li><li><a href="https://www.cnblogs.com/wcwnina/p/8044353.html" target="_blank" rel="noopener">Windows下Apache的安装</a></li><li><a href="https://www.cnblogs.com/wanxudong/p/5846907.html" target="_blank" rel="noopener">Mac下配置Apache服务器</a></li><li><a href="https://blog.csdn.net/Zereao/article/details/102840924" target="_blank" rel="noopener">解决 Mac Catalina 活动监视器中数据不见了，只能看到进程名的问题</a></li><li><a href="https://www.jianshu.com/p/56d37a0002d7" target="_blank" rel="noopener">Mac 移动硬盘未挂载-解决办法</a></li><li><a href="https://leay.net/2019/12/25/hexo/" target="_blank" rel="noopener">Hexo 引用本地图片</a></li><li><a href="https://blog.csdn.net/fake_hydra/article/details/82414965" target="_blank" rel="noopener">GitHub Pages自定义域名后每次hexo d都会失效解决</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> solutions </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Web开发笔记</title>
      <link href="2018/12/14/Web%E5%BC%80%E5%8F%91%E7%AC%94%E8%AE%B0/"/>
      <url>2018/12/14/Web%E5%BC%80%E5%8F%91%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="Web开发笔记"><a href="#Web开发笔记" class="headerlink" title="Web开发笔记"></a>Web开发笔记</h1><h2 id="Tools"><a href="#Tools" class="headerlink" title="Tools"></a>Tools</h2><ul><li><p>色彩网站：<a href="https://flatuicolors.com/palette/gb" target="_blank" rel="noopener">https://flatuicolors.com/palette/gb</a></p></li><li><p>Font awesome：<a href="https://fontawesome.com/" target="_blank" rel="noopener">https://fontawesome.com/</a></p></li></ul><blockquote><p>SVG 与 Webfont中相同图标的显示效果可能不同</p></blockquote><h2 id="Projects"><a href="#Projects" class="headerlink" title="Projects"></a>Projects</h2><ol><li>Search Box (using only html &amp; css)</li></ol>]]></content>
      
      
      <categories>
          
          <category> Archive </category>
          
          <category> Web </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Web </tag>
            
            <tag> 前端 </tag>
            
            <tag> html </tag>
            
            <tag> css </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ns-3使用及计网实验总结</title>
      <link href="2018/12/07/ns-3%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/"/>
      <url>2018/12/07/ns-3%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h1 id="ns-3使用及计网实验总结"><a href="#ns-3使用及计网实验总结" class="headerlink" title="ns-3使用及计网实验总结"></a>ns-3使用及计网实验总结</h1><h2 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h2><ol><li><p>由于ubuntu18.04中删去了可视化工具pyViz依赖的几个python包，所以在ubuntu18.04下无法实现pyViz的可视化。</p><p>计网设计实验采用的环境：Ubuntu14.04.5 + ns-3.25</p></li><li><p>具体搭建过程参照官网installation instruction的wiki即可。</p></li></ol><h2 id="可视化方式"><a href="#可视化方式" class="headerlink" title="可视化方式"></a>可视化方式</h2><ol><li><p>ns-3提供了两种可视化工具：</p><ul><li><p>PyViz：功能较简单，运行时实时显示node和数据传输状态。使用方便，但功能相对单一。使用时需在脚本文件的<code>main()</code>中加入以下两行代码，并在terminal中运行时加入<code>--vis</code> 选项。</p><pre><code class="c++">CommandLine cmd;cmd.Parse(argc,argv);</code></pre><p>效果如下（鼠标悬停在结点处可显示结点详细信息）：</p><p><img src="http://img.cdn.leonwang.top/006tNbRwgy1fxymojcdawj30vd0u0b29.jpg" alt=""></p></li><li><p>NetAnim：一个独立的、基于Qt4的离线动画演示工具，在ns-3仿真过程中生成xml格式的trace文件，仿真结束后NetAnim读取该文件显示网络拓扑和结点间数据分组流等动画过程。</p><p>NetAnim需单独编译。</p><p>使用时需在脚本文件中加入生成xml文件的代码，如：</p><pre><code class="c++">AnimationInterface anim (&quot;animation.xml&quot;);</code></pre><p>使用NetAnim加载后效果如下（双击结点可显示结点详细信息）：</p><p><img src="http://img.cdn.leonwang.top/006tNbRwgy1fxymtq1zsbj30ve0u0ts0.jpg" alt=""></p></li></ul></li></ol><h2 id="学习过程"><a href="#学习过程" class="headerlink" title="学习过程"></a>学习过程</h2><ol><li>首先了解基本ns-3基本知识，可以阅读《NS-3网络模拟器基础与应用》。</li><li>阅读 example/tutorials 中的代码，从first.cc开始，逐步加深。多阅读官方案例代码，官方提供了丰富的案例，除 example 文件夹外，在 src/module_name/example 中也有每个模块的样例代码。</li></ol><h2 id="实验项目题目选择"><a href="#实验项目题目选择" class="headerlink" title="实验项目题目选择"></a>实验项目题目选择</h2><ol><li><p>用ns-3实现组网、组播、无线网络等都相对比较简单，没有难度和创新性。</p></li><li><p>用ns-3比较难实现选择实验中的物理（相对于使用ns-3模拟）防火墙实验项目，因为防火墙需要配置NAT，而ns-3并没有提供NAT的相关模块，需要自行编写。</p><p>ns-3虽然没有提供官方的NAT模块，但官网有收录了一个<a href="https://www.nsnam.org/wiki/GSOC2012NetworkAddressTranslation" target="_blank" rel="noopener">NAT项目</a>（收录于<a href="https://www.nsnam.org/wiki/GSOC2012AcceptedProjects" target="_blank" rel="noopener">GSOC2012AcceptedProjects</a>）。尝试对 final review 中的内容进行安装（参考<a href="https://blog.csdn.net/barcodegun/article/details/6898193" target="_blank" rel="noopener">如何在ns-3中添加模块</a>），需要安装patch1和patch3中的内容，但添加模块有难度。因为该NAT模块不仅编写了自己的文件，还对官方的internet模块内已有的文件进行了改动，然而几年来官方也对这几个文件进行了更新，可以理解为git上不同分支产生了冲突。</p><p>于是，放弃了实现NAT和基于NAT的防火墙的想法…但是如果有兴趣，应该也可以自己编写实验需要的静态NAT协议，不需要写得像官方收录的NAT项目那么复杂。</p></li><li><p>最终选择题目：使用Fat-Tree拓扑结构的数据中心网络(DCN, data center network)</p><p>使用NetAnim进行演示。</p></li></ol><h2 id="学习资源"><a href="#学习资源" class="headerlink" title="学习资源"></a>学习资源</h2><ul><li>官网</li><li>《NS-3网络模拟器基础与应用》</li><li>阅读样例代码</li><li>google group讨论区</li><li>github</li><li>有问题多google</li></ul>]]></content>
      
      
      <categories>
          
          <category> Archive </category>
          
          <category> 计算机网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机网络 </tag>
            
            <tag> ns-3 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ns-3入门</title>
      <link href="2018/12/02/ns-3%E5%85%A5%E9%97%A8/"/>
      <url>2018/12/02/ns-3%E5%85%A5%E9%97%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="ns-3入门"><a href="#ns-3入门" class="headerlink" title="ns-3入门"></a>ns-3入门</h1><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><a href="https://www.nsnam.org/wiki/Installation" target="_blank" rel="noopener">官方Installtion</a></p><p><a href="https://blog.csdn.net/u010394419/article/details/80441518" target="_blank" rel="noopener">硬盘安装Ubuntu18.04+NS3.27过程总结-CSDN</a></p><h2 id="教程"><a href="#教程" class="headerlink" title="教程"></a>教程</h2><p><a href="https://wenku.baidu.com/view/0b46126fee06eff9aef807dd.html" target="_blank" rel="noopener">中文教程-百度文库</a></p><p><a href="https://wenku.baidu.com/view/a9bc68620b1c59eef8c7b496.html?sxts=1543720223233" target="_blank" rel="noopener">Ns3网络仿真系统软件介绍-百度文库</a></p><h2 id="example"><a href="#example" class="headerlink" title="example"></a>example</h2><h3 id="hello-simulator-cc"><a href="#hello-simulator-cc" class="headerlink" title="hello-simulator.cc"></a>hello-simulator.cc</h3><pre><code class="c++">/* -*- Mode:C++; c-file-style:&quot;gnu&quot;; indent-tabs-mode:nil; -*- *//* * This program is free software; you can redistribute it and/or modify * it under the terms of the GNU General Public License version 2 as * published by the Free Software Foundation; * * This program is distributed in the hope that it will be useful, * but WITHOUT ANY WARRANTY; without even the implied warranty of * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the * GNU General Public License for more details. * * You should have received a copy of the GNU General Public License * along with this program; if not, write to the Free Software * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA */#include &quot;ns3/core-module.h&quot;using namespace ns3;NS_LOG_COMPONENT_DEFINE (&quot;HelloSimulator&quot;);intmain (int argc, char *argv[]){  NS_LOG_UNCOND (&quot;Hello Simulator&quot;);}</code></pre><p><img src="http://img.cdn.leonwang.top/006tNbRwgy1fxrq06w8uej313z05utb5.jpg" alt=""></p><h3 id="first-cc"><a href="#first-cc" class="headerlink" title="first.cc"></a>first.cc</h3><pre><code class="c++">/* -*- Mode:C++; c-file-style:&quot;gnu&quot;; indent-tabs-mode:nil; -*- *//* * This program is free software; you can redistribute it and/or modify * it under the terms of the GNU General Public License version 2 as * published by the Free Software Foundation; * * This program is distributed in the hope that it will be useful, * but WITHOUT ANY WARRANTY; without even the implied warranty of * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the * GNU General Public License for more details. * * You should have received a copy of the GNU General Public License * along with this program; if not, write to the Free Software * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA */#include &quot;ns3/core-module.h&quot;#include &quot;ns3/network-module.h&quot;#include &quot;ns3/internet-module.h&quot;#include &quot;ns3/point-to-point-module.h&quot;#include &quot;ns3/applications-module.h&quot;using namespace ns3;//这一行声明了一个叫FirstScriptExample的日志构件，通过引用FirstScriptExample这个名字的操作，可以实现打开或者关闭控制台日志的输出。NS_LOG_COMPONENT_DEFINE (&quot;FirstScriptExample&quot;);intmain (int argc, char *argv[]){  CommandLine cmd;  cmd.Parse (argc, argv);  Time::SetResolution (Time::NS);    //下面两行脚本是用来使两个日志组件生效的。它们被内建在Echo Client 和Echo Server 应用中  LogComponentEnable (&quot;UdpEchoClientApplication&quot;, LOG_LEVEL_INFO);  LogComponentEnable (&quot;UdpEchoServerApplication&quot;, LOG_LEVEL_INFO);  NodeContainer nodes;  nodes.Create (2);  PointToPointHelper pointToPoint;  pointToPoint.SetDeviceAttribute (&quot;DataRate&quot;, StringValue (&quot;5Mbps&quot;));  pointToPoint.SetChannelAttribute (&quot;Delay&quot;, StringValue (&quot;2ms&quot;));  NetDeviceContainer devices;  devices = pointToPoint.Install (nodes);  InternetStackHelper stack;  stack.Install (nodes);  Ipv4AddressHelper address;  address.SetBase (&quot;10.1.1.0&quot;, &quot;255.255.255.0&quot;);  Ipv4InterfaceContainer interfaces = address.Assign (devices);  UdpEchoServerHelper echoServer (9);  ApplicationContainer serverApps = echoServer.Install (nodes.Get (1));  serverApps.Start (Seconds (1.0));  serverApps.Stop (Seconds (10.0));  UdpEchoClientHelper echoClient (interfaces.GetAddress (1), 9);  echoClient.SetAttribute (&quot;MaxPackets&quot;, UintegerValue (1));  echoClient.SetAttribute (&quot;Interval&quot;, TimeValue (Seconds (1.0)));  echoClient.SetAttribute (&quot;PacketSize&quot;, UintegerValue (1024));  ApplicationContainer clientApps = echoClient.Install (nodes.Get (0));  clientApps.Start (Seconds (2.0));  clientApps.Stop (Seconds (10.0));  Simulator::Run ();  Simulator::Destroy ();  return 0;}</code></pre><p>代码分析：</p><p><img src="http://img.cdn.leonwang.top/006tNbRwgy1fxynrtbgtvj31a50u0hdz.jpg" alt=""></p><p>脚本运行：</p><p><img src="http://img.cdn.leonwang.top/006tNbRwgy1fxrq0i6ettj313608sdk7.jpg" alt=""></p><p>后加 <code>--vis</code>实现可视化</p><p><img src="http://img.cdn.leonwang.top/006tNbRwly1fxs9lr4fj4j319n0u07av.jpg" alt=""></p><h2 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h2><p>OpenFlow，一种网上通信协议，属于数据链路层，能够控制网上交换器或路由器的转发平面（forwarding plane），借此改变网上数据包所走的网上路径。</p><p>Berkeley套接字（也作BSD套接字应用程序接口）包括了一个用C语言写成的应用程序开发库，主要用于实现进程间通讯，在计算机网络通讯方面被广泛使用。</p><h2 id="踩坑"><a href="#踩坑" class="headerlink" title="踩坑"></a>踩坑</h2><p>编译时路径内不能有中文</p><p>ubuntu18版本内缺少可视化组件需要的python包，最终实现可视化的环境是ubuntu + ns-3.25</p>]]></content>
      
      
      <categories>
          
          <category> Archive </category>
          
          <category> 计算机网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机网络 </tag>
            
            <tag> ns-3 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ARCore 3D模型</title>
      <link href="2018/11/22/ARCore-3D%E6%A8%A1%E5%9E%8B/"/>
      <url>2018/11/22/ARCore-3D%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="本文档解决的问题"><a href="#本文档解决的问题" class="headerlink" title="本文档解决的问题"></a>本文档解决的问题</h1><ul><li style="list-style: none"><input type="checkbox" checked> 模型格式</li><li style="list-style: none"><input type="checkbox" checked> 支持动画的模型类型</li><li style="list-style: none"><input type="checkbox"> 是否需要人工打光</li></ul><hr><h1 id="官方sample模型格式"><a href="#官方sample模型格式" class="headerlink" title="官方sample模型格式"></a>官方sample模型格式</h1><p>官方SDK的sample中均使用<strong>.obj</strong>格式文件。</p><p><img src="http://img.cdn.leonwang.top/006tNbRwgy1fxh59jhswaj30si0iyn0a.jpg" alt=""></p><p><img src="http://img.cdn.leonwang.top/006tNbRwgy1fxh59e3nerj30qi0j0n08.jpg" alt=""></p><p><strong>obj文件</strong>：obj文件是3D模型文件格式。由Alias|Wavefront公司为3D建模和动画软件”Advanced Visualizer”开发的一种标准，适合用于3D软件模型之间的互导，也可以通过Maya读写。</p><p><strong>文件特点</strong>：OBJ3.0文件格式支持直线(Line)、多边形(Polygon)、表面(Surface)和自由形态曲线(Free-form Curve)。直线和多边形通过它们的点来描述，曲线和表面则根据它们的控制点和依附于曲线类型的额外信息来定义，这些信息支持规则和不规则的曲线，包括那些基于贝塞尔曲线(Bezier)、B样条(B-spline)、基数(Cardinal/Catmull-Rom)和泰勒方程(Taylor equations)的曲线。其他特点如下：</p><ol><li>OBJ文件是一种3D模型文件。<strong>不包含</strong>动画、材质特性、贴图路径、动力学、粒子等信息。</li><li>OBJ文件主要支持多边形(Polygons)模型。虽然也支持曲线(Curves)、表面(Surfaces)、点组材质(Point Group Materials)，但Maya导出的OBJ文件并不包括这些信息。</li><li>OBJ文件支持三个点以上的面，这一点很有用。很多其它的模型文件格式只支持三个点的面，所以导入Maya的模型经常被三角化了，这对于我们对模型进行再加工甚为不利。</li><li>OBJ文件<strong>支持法线和贴图坐标</strong>。在其它软件中调整好贴图后，贴图坐标信息可以存入OBJ文件中，这样文件导入Maya后只需指定一下贴图文件路径就行了，不需要再调整贴图坐标。</li></ol><p><strong><em><u>！！！不支持动画</u></em></strong></p><hr><h1 id="ARCore-Sceneform支持的三种3D模型类型"><a href="#ARCore-Sceneform支持的三种3D模型类型" class="headerlink" title="ARCore Sceneform支持的三种3D模型类型"></a>ARCore Sceneform支持的三种3D模型类型</h1><h2 id="Sceneform-概览"><a href="#Sceneform-概览" class="headerlink" title="Sceneform 概览"></a>Sceneform 概览</h2><p><a href="https://developers.google.cn/ar/develop/java/sceneform/" target="_blank" rel="noopener">Sceneform</a> 让 Android 开发者不必学习 3D 图形和 OpenGL 就能使用 ARCore。 它包括一个高级场景图 API，仿真基于物理的渲染器，一个用于导入、查看和构建 3D 资产的 Android Studio 插件，并且可以轻松地集成到 ARCore 内进行简单的 AR 应用构建。</p><h2 id="模型类型支持"><a href="#模型类型支持" class="headerlink" title="模型类型支持"></a>模型类型支持</h2><p>阅读官方文档，ARCore支持 OBJ、FBX 和 glTF 格式的 3D 资产，并提供了相关工具和插件。</p><p><img src="http://img.cdn.leonwang.top/006tNbRwgy1fxh7svmrf2j31cq0u047i.jpg" alt=""></p><p><a href="https://developers.google.cn/ar/develop/java/sceneform/create-renderables" target="_blank" rel="noopener">官方文档：创建可渲染对象</a></p><p><a href="https://developers.google.cn/ar/develop/java/sceneform/import-assets" target="_blank" rel="noopener">官方文档：导入和预览3D资产</a></p><h2 id="三种模型比较"><a href="#三种模型比较" class="headerlink" title="三种模型比较"></a>三种模型比较</h2><h4 id="obj"><a href="#obj" class="headerlink" title=".obj"></a>.obj</h4><p>OBJ文件是Alias|Wavefront公司为它的一套基于工作站的3D建模和动画软件”Advanced Visualizer”开发的一种标准3D模型文件格式，很适合用于3D软件模型之间的互导。目前几乎所有知名的3D软件都支持OBJ文件的读写。OBJ文件是一种文本文件，可以直接用写字板打开进行查看和编辑修改。</p><p>. obj 格式， 静态多边形模型 - 附带 UV 信息及材质路径！<strong>不包含动画</strong>、材质特性、贴图路径、动力学、粒子等信息。主要支持多边形(Polygons)模型。是最受欢迎的格式。</p><h4 id="glTF"><a href="#glTF" class="headerlink" title=".glTF"></a>.glTF</h4><p>glTF是一种可以减少3D格式中与渲染无关的冗余数据并且在更加适合OpenGL簇加载的一种3D文件格式。glTF的提出是源自于3D工业和媒体发展的过程中，对3D格式统一化的急迫需求。如果用一句话来描述：glTF 就是三维文件的 JPEG ，三维格式的 MP3。在没有glTF的时候，大家都要花很长的的时间来处理模型的载入。很多的游戏引擎或者工控渲染引擎，都使用的是插件的方式来载入各种格式的模型。可是，各种格式的模型都包含了很多无关的信息。就glTF格式而言，虽然以前有很多3D格式，但是各种3D模型渲染程序都要处理很多种的格式。对于那些对载入格式不是那么重要的软件，可以显著减少代码量，所以也有人说，最大的受益者是那些对程序大小敏感的3D Web渲染引擎，只需要很少的代码就可以顺利地载入各种模型了。此外，glTF是对近二十年来各种3D格式的总结，使用最优的数据结构，来保证最大的兼容性以及可伸缩性。这就好比是本世纪初xml的提出。glTF使用json格式进行描述，也可以编译成二进制的内容：bglTF。<strong>glTF可以包括场景、摄像机、动画等，也可以包括网格、材质、纹理，甚至包括了渲染技术（technique）、着色器以及着色器程序。</strong>同时由于json格式的特点，它支持预留一般以及特定供应商的扩展。</p><p>.glTF 格式，<strong>支持动画</strong>等！.gITF 2.0 格式逐步的完成了 WebGL 的布局，也成为了这个领域的专用格式，随着发展游戏领域的应用也会越来越广泛。官网细节：<a href="https://www.khronos.org/gltf/" target="_blank" rel="noopener">gltf</a></p><h4 id="fbx"><a href="#fbx" class="headerlink" title=".fbx"></a>.fbx</h4><p>FBX 是 FilmBoX 这套软件所使用的格式，后改称 Motionbuilder。因为Motionbuilder扮演的是动作制作的平台，所以在前端的modeling和后端的rendering也都有赖于其它软件的配合，所以Motionbuilder在档案的转换上自然下了一番功夫。FBX最大的用途是用在诸如在 Max、Maya、Softimage 等软件间进行模型、材质、动作和摄影机信息的互导，这样就可以发挥 Max 和 Maya 等软件的优势。可以说，FBX 方案是非常好的互导方案。</p><p>. fbx 格式，Autodesk 家族格式 ，<strong>支持动画</strong>！这是一个商业的格式，兼容最好的当属 Autodesk 家族的软件了。fbx 也开放给了第三方软件，但总是感觉除了他自己的软件之外或多或少的都有解决不完的问题。 毋庸置疑，FBX 现在是最受欢迎的格式。</p><hr><h1 id="Google-Poly（3D对象库）"><a href="#Google-Poly（3D对象库）" class="headerlink" title="Google Poly（3D对象库）"></a>Google Poly（3D对象库）</h1><p>网址：<a href="https://poly.google.com/" target="_blank" rel="noopener">Google Poly</a></p><p>Google对外推出了Poly，这是一个用于增强和虚拟现实应用程序的对象库。3D对象可以免费下载，并且可以兼容ARR/VR平台，比如Google的ARCore和苹果的ARKit。</p><p><em>经过简单浏览，主要是VR模型。</em></p><hr><h1 id="其他参考资料"><a href="#其他参考资料" class="headerlink" title="其他参考资料"></a>其他参考资料</h1><ul><li><a href="http://www.zwqxin.com/archives/opengl/obj-model-format-import-and-render-1.html" target="_blank" rel="noopener">OBJ模型文件的结构、导入与渲染</a></li><li><p><a href="http://www.zwqxin.com/archives/opengl/model-fbx-dae-format-import-animation.html" target="_blank" rel="noopener">FBX、DAE模型的格式、导入与骨骼动画</a></p></li><li><p><a href="http://www.bgteach.com/article/132" target="_blank" rel="noopener">三维文件知多少</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Archive </category>
          
          <category> ARCore </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ARCore </tag>
            
            <tag> AR </tag>
            
            <tag> Android </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ARCore SDK sample 代码分析</title>
      <link href="2018/11/22/ARCore-SDK-sample-%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/"/>
      <url>2018/11/22/ARCore-SDK-sample-%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>本文档将分析两个官方 SDK 中各一个 sample 案例。</p><ul><li>arcore-android-sdk-1.5.0 中的 hello_ar_java，本案例未使用 Sceneform 。</li><li>sceneform-android-sdk-1.5.1 中的 hellosceneform，本案例使用 Sceneform 进行构建。</li></ul><hr><h1 id="hello-ar-java-项目"><a href="#hello-ar-java-项目" class="headerlink" title="hello_ar_java 项目"></a>hello_ar_java 项目</h1><p>该项目未使用Google提供的 Sceneform</p><h2 id="obj模型的导入"><a href="#obj模型的导入" class="headerlink" title="obj模型的导入"></a>obj模型的导入</h2><p>jar包：<a href="https://mvnrepository.com/artifact/de.javagl/obj/0.2.1" target="_blank" rel="noopener">obj-0.2.1-sources.jar</a></p><p><a href="https://github.com/javagl/Obj" target="_blank" rel="noopener">Github page</a></p><p>A simple Wavefront OBJ file loader and writer.</p><p>该sample使用此jar包进行obj模型的导入。</p><p><img src="http://img.cdn.leonwang.top/006tNbRwgy1fxh7lqzfizj31ko0k8agy.jpg" alt=""></p><p><img src="http://img.cdn.leonwang.top/006tNbRwgy1fxh7nq7lf3j317q0nydmb.jpg" alt=""></p><hr><h1 id="hellosceneform-项目"><a href="#hellosceneform-项目" class="headerlink" title="hellosceneform 项目"></a>hellosceneform 项目</h1><p>本项目使用 <a href="https://developers.google.cn/ar/develop/java/sceneform/" target="_blank" rel="noopener">Sceneform</a> 构建,不必学习 3D 图形和 OpenGL 就能使用 ARCore。 </p><h2 id="HelloSceneformActivity-源码"><a href="#HelloSceneformActivity-源码" class="headerlink" title="HelloSceneformActivity 源码"></a>HelloSceneformActivity 源码</h2><pre><code class="java">/* * Copyright 2018 Google LLC. All Rights Reserved. * * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); * you may not use this file except in compliance with the License. * You may obtain a copy of the License at * *      http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an &quot;AS IS&quot; BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */package com.google.ar.sceneform.samples.hellosceneform;import android.app.Activity;import android.app.ActivityManager;import android.content.Context;import android.os.Build;import android.os.Build.VERSION_CODES;import android.os.Bundle;import android.support.v7.app.AppCompatActivity;import android.util.Log;import android.view.Gravity;import android.view.MotionEvent;import android.widget.Toast;import com.google.ar.core.Anchor;import com.google.ar.core.HitResult;import com.google.ar.core.Plane;import com.google.ar.sceneform.AnchorNode;import com.google.ar.sceneform.rendering.ModelRenderable;import com.google.ar.sceneform.ux.ArFragment;import com.google.ar.sceneform.ux.TransformableNode;/** * This is an example activity that uses the Sceneform UX package to make common AR tasks easier. */public class HelloSceneformActivity extends AppCompatActivity {  private static final String TAG = HelloSceneformActivity.class.getSimpleName();  private static final double MIN_OPENGL_VERSION = 3.0;  private ArFragment arFragment;  private ModelRenderable andyRenderable;  @Override  @SuppressWarnings({&quot;AndroidApiChecker&quot;, &quot;FutureReturnValueIgnored&quot;})  // CompletableFuture requires api level 24  // FutureReturnValueIgnored is not valid  protected void onCreate(Bundle savedInstanceState) {    super.onCreate(savedInstanceState);    if (!checkIsSupportedDeviceOrFinish(this)) {      return;    }    setContentView(R.layout.activity_ux);    arFragment = (ArFragment) getSupportFragmentManager().findFragmentById(R.id.ux_fragment);    // When you build a Renderable, Sceneform loads its resources in the background while returning    // a CompletableFuture. Call thenAccept(), handle(), or check isDone() before calling get().    ModelRenderable.builder()        .setSource(this, R.raw.andy)        .build()        .thenAccept(renderable -&gt; andyRenderable = renderable)        .exceptionally(            throwable -&gt; {              Toast toast =                  Toast.makeText(this, &quot;Unable to load andy renderable&quot;, Toast.LENGTH_LONG);              toast.setGravity(Gravity.CENTER, 0, 0);              toast.show();              return null;            });    arFragment.setOnTapArPlaneListener(        (HitResult hitResult, Plane plane, MotionEvent motionEvent) -&gt; {          if (andyRenderable == null) {            return;          }          // Create the Anchor.          Anchor anchor = hitResult.createAnchor();          AnchorNode anchorNode = new AnchorNode(anchor);          anchorNode.setParent(arFragment.getArSceneView().getScene());          // Create the transformable andy and add it to the anchor.          TransformableNode andy = new TransformableNode(arFragment.getTransformationSystem());          andy.setParent(anchorNode);          andy.setRenderable(andyRenderable);          andy.select();        });  }  /**   * Returns false and displays an error message if Sceneform can not run, true if Sceneform can run   * on this device.   *   * &lt;p&gt;Sceneform requires Android N on the device as well as OpenGL 3.0 capabilities.   *   * &lt;p&gt;Finishes the activity if Sceneform can not run   */  public static boolean checkIsSupportedDeviceOrFinish(final Activity activity) {    if (Build.VERSION.SDK_INT &lt; VERSION_CODES.N) {      Log.e(TAG, &quot;Sceneform requires Android N or later&quot;);      Toast.makeText(activity, &quot;Sceneform requires Android N or later&quot;, Toast.LENGTH_LONG).show();      activity.finish();      return false;    }    String openGlVersionString =        ((ActivityManager) activity.getSystemService(Context.ACTIVITY_SERVICE))            .getDeviceConfigurationInfo()            .getGlEsVersion();    if (Double.parseDouble(openGlVersionString) &lt; MIN_OPENGL_VERSION) {      Log.e(TAG, &quot;Sceneform requires OpenGL ES 3.0 later&quot;);      Toast.makeText(activity, &quot;Sceneform requires OpenGL ES 3.0 or later&quot;, Toast.LENGTH_LONG)          .show();      activity.finish();      return false;    }    return true;  }}</code></pre><h2 id="模型的导入"><a href="#模型的导入" class="headerlink" title="模型的导入"></a>模型的导入</h2><p>onCreate() 中对模型资源位置进行了设置。</p><p>Sceneform中3D资源的导入方法（官方文档）：<a href="https://developers.google.cn/ar/develop/java/sceneform/import-assets" target="_blank" rel="noopener">导入和预览 3D 资产</a></p>]]></content>
      
      
      <categories>
          
          <category> Archive </category>
          
          <category> ARCore </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ARCore </tag>
            
            <tag> AR </tag>
            
            <tag> Android </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
